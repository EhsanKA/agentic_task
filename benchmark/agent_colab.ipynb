{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
    "\n",
    "This notebook sets up **Gemini 3 Pro Preview** as an autonomous agent to solve the Research Paper Entity Extraction and Citation Analysis benchmark.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (for native Gemini access via `google.colab.ai`)\n",
    "\n",
    "**Model Used:**\n",
    "- `google/gemini-3-pro-preview` - Gemini 3 Pro Preview model\n",
    "\n",
    "**Implementation:**\n",
    "- Uses `google.colab.ai` module for native Colab Pro AI integration\n",
    "- No external API keys required - uses Colab Pro's built-in AI capabilities\n",
    "- Self-contained dataset generation (no file uploads needed)\n",
    "\n",
    "**Note:** This notebook runs end-to-end without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q pandas networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import ai\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import unittest\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# List available models in Colab Pro\n",
    "print(\"Available AI models in Colab Pro:\")\n",
    "available_models = ai.list_models()\n",
    "for model in available_models:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "Select and configure Gemini-3-Pro from available Colab Pro models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent model initialized: gemini-3-pro\n"
     ]
    }
   ],
   "source": [
    "# Select the model for agentic tasks\n",
    "# Using Gemini 3 Pro Preview - the most advanced reasoning model available\n",
    "MODEL_NAME = \"google/gemini-3-pro-preview\"\n",
    "\n",
    "# Verify the model is available\n",
    "if MODEL_NAME in available_models:\n",
    "    print(f\"Model '{MODEL_NAME}' is available - SELECTED\")\n",
    "else:\n",
    "    print(f\"Warning: '{MODEL_NAME}' not found. Available models: {available_models}\")\n",
    "    # Fallback to other Pro/capable models\n",
    "    fallback_order = [\"google/gemini-2.5-pro\", \"google/gemini-2.0-flash\", \"google/gemini-2.5-flash\"]\n",
    "    for fallback in fallback_order:\n",
    "        if fallback in available_models:\n",
    "            MODEL_NAME = fallback\n",
    "            print(f\"Using fallback model: {MODEL_NAME}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nAgent model selected: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "Generate the synthetic benchmark dataset. This ensures the notebook is fully self-contained and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED DATASET GENERATION - With Headroom Challenges\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "import csv\n",
    "from datetime import timedelta\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Authors - including ambiguous ones (auth_011, auth_012)\n",
    "CANONICAL_AUTHORS = {\n",
    "    \"auth_001\": {\"canonical_name\": \"John Smith\", \"variations\": [\"J. Smith\", \"John A. Smith\"], \"typos\": [\"Jonh Smith\"], \"institution\": \"inst_001\"},\n",
    "    \"auth_002\": {\"canonical_name\": \"Maria Garcia\", \"variations\": [\"M. Garcia\", \"Maria L. Garcia\"], \"typos\": [\"Maria Gracia\"], \"institution\": \"inst_002\"},\n",
    "    \"auth_003\": {\"canonical_name\": \"Wei Zhang\", \"variations\": [\"W. Zhang\", \"Zhang, Wei\"], \"typos\": [], \"institution\": \"inst_003\"},\n",
    "    \"auth_004\": {\"canonical_name\": \"Emily Johnson\", \"variations\": [\"E. Johnson\"], \"typos\": [], \"institution\": \"inst_001\"},\n",
    "    \"auth_005\": {\"canonical_name\": \"Ahmed Hassan\", \"variations\": [\"A. Hassan\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    \"auth_006\": {\"canonical_name\": \"Sarah Williams\", \"variations\": [\"S. Williams\"], \"typos\": [], \"institution\": \"inst_002\"},\n",
    "    \"auth_007\": {\"canonical_name\": \"Yuki Tanaka\", \"variations\": [\"Y. Tanaka\"], \"typos\": [], \"institution\": \"inst_005\"},\n",
    "    \"auth_008\": {\"canonical_name\": \"Michael Brown\", \"variations\": [\"M. Brown\"], \"typos\": [], \"institution\": \"inst_003\"},\n",
    "    \"auth_009\": {\"canonical_name\": \"Lisa Chen\", \"variations\": [\"L. Chen\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    \"auth_010\": {\"canonical_name\": \"David Miller\", \"variations\": [\"D. Miller\"], \"typos\": [], \"institution\": \"inst_005\"},\n",
    "    # TRAP: Different person with same initials!\n",
    "    \"auth_011\": {\"canonical_name\": \"James Smith\", \"variations\": [\"J. Smith\", \"J. B. Smith\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    \"auth_012\": {\"canonical_name\": \"Wei Zhang\", \"variations\": [\"W. Zhang\", \"W. X. Zhang\"], \"typos\": [], \"institution\": \"inst_002\"},\n",
    "}\n",
    "\n",
    "CANONICAL_INSTITUTIONS = {\n",
    "    \"inst_001\": {\"canonical_name\": \"Massachusetts Institute of Technology\", \"variations\": [\"MIT\", \"M.I.T.\"], \"typos\": [\"Massachusets Institute of Technology\"], \"country\": \"USA\"},\n",
    "    \"inst_002\": {\"canonical_name\": \"Stanford University\", \"variations\": [\"Stanford\", \"Stanford Univ.\"], \"typos\": [\"Standford University\"], \"country\": \"USA\"},\n",
    "    \"inst_003\": {\"canonical_name\": \"Tsinghua University\", \"variations\": [\"Tsinghua\", \"THU\"], \"typos\": [], \"country\": \"China\"},\n",
    "    \"inst_004\": {\"canonical_name\": \"University of Oxford\", \"variations\": [\"Oxford\", \"Oxford Univ.\"], \"typos\": [], \"country\": \"UK\"},\n",
    "    \"inst_005\": {\"canonical_name\": \"University of Tokyo\", \"variations\": [\"Tokyo Univ.\", \"UTokyo\"], \"typos\": [], \"country\": \"Japan\"},\n",
    "}\n",
    "\n",
    "VENUES = {\n",
    "    \"neurips\": {\"canonical\": \"NeurIPS\", \"variations\": [\"NeurIPS\", \"NIPS\", \"Neural Information Processing Systems\"]},\n",
    "    \"icml\": {\"canonical\": \"ICML\", \"variations\": [\"ICML\", \"International Conference on Machine Learning\"]},\n",
    "    \"cvpr\": {\"canonical\": \"CVPR\", \"variations\": [\"CVPR\", \"IEEE/CVF CVPR\"]},\n",
    "    \"acl\": {\"canonical\": \"ACL\", \"variations\": [\"ACL\", \"Annual Meeting of the ACL\"]},\n",
    "}\n",
    "\n",
    "RESEARCH_TOPICS = [\"machine learning\", \"deep learning\", \"neural networks\", \"natural language processing\",\n",
    "    \"computer vision\", \"reinforcement learning\", \"transformer models\", \"attention mechanisms\"]\n",
    "\n",
    "CITATION_RING_PAPERS = [\"paper_0030\", \"paper_0031\", \"paper_0032\", \"paper_0033\", \"paper_0034\"]\n",
    "TEMPORAL_ANOMALY_PAPERS = [\"paper_0050\", \"paper_0051\"]\n",
    "\n",
    "def generate_papers(num_papers=100):\n",
    "    papers = []\n",
    "    author_ids = list(CANONICAL_AUTHORS.keys())\n",
    "    base_date = datetime(2020, 1, 1)\n",
    "    \n",
    "    for i in range(num_papers):\n",
    "        paper_id = f\"paper_{i:04d}\"\n",
    "        num_authors = random.randint(1, 3)\n",
    "        selected_ids = random.sample(author_ids, num_authors)\n",
    "        \n",
    "        authors = []\n",
    "        for aid in selected_ids:\n",
    "            auth = CANONICAL_AUTHORS[aid]\n",
    "            if random.random() > 0.5 and auth[\"variations\"]:\n",
    "                authors.append(random.choice(auth[\"variations\"]))\n",
    "            else:\n",
    "                authors.append(auth[\"canonical_name\"])\n",
    "        \n",
    "        inst_id = CANONICAL_AUTHORS[selected_ids[0]][\"institution\"]\n",
    "        inst = CANONICAL_INSTITUTIONS[inst_id]\n",
    "        institution = random.choice(inst[\"variations\"]) if random.random() > 0.5 else inst[\"canonical_name\"]\n",
    "        \n",
    "        venue_key = random.choice(list(VENUES.keys()))\n",
    "        venue = random.choice(VENUES[venue_key][\"variations\"])\n",
    "        \n",
    "        pub_date = base_date + timedelta(days=random.randint(0, 1500))\n",
    "        \n",
    "        paper = {\n",
    "            \"paper_id\": paper_id, \"title\": f\"Research on {random.choice(RESEARCH_TOPICS).title()}\",\n",
    "            \"authors\": authors, \"institution\": institution,\n",
    "            \"abstract\": f\"This paper presents research on {random.choice(RESEARCH_TOPICS)}.\",\n",
    "            \"keywords\": random.sample(RESEARCH_TOPICS, 2),\n",
    "            \"venue\": venue, \"year\": pub_date.year,\n",
    "            \"publication_date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "        \n",
    "        # Basic edge cases\n",
    "        if i == 5: paper[\"abstract\"] = \"\"\n",
    "        if i == 12: paper[\"keywords\"] = []\n",
    "        if i == 45: paper[\"institution\"] = None\n",
    "        \n",
    "        # HEADROOM: Ambiguous J. Smith\n",
    "        if i == 8:\n",
    "            paper[\"authors\"] = [\"J. Smith\", \"Maria Garcia\"]\n",
    "            paper[\"institution\"] = \"MIT\"\n",
    "        if i == 9:\n",
    "            paper[\"authors\"] = [\"J. Smith\", \"Ahmed Hassan\"]\n",
    "            paper[\"institution\"] = \"Oxford\"\n",
    "            \n",
    "        # HEADROOM: Typos\n",
    "        if i == 35:\n",
    "            paper[\"authors\"] = [\"Jonh Smith\", \"Maria Gracia\"]\n",
    "            paper[\"institution\"] = \"Massachusets Institute of Technology\"\n",
    "        \n",
    "        # Citation ring papers\n",
    "        if paper_id in CITATION_RING_PAPERS:\n",
    "            paper[\"year\"] = 2022\n",
    "            paper[\"publication_date\"] = \"2022-06-15\"\n",
    "        \n",
    "        # Temporal anomaly targets\n",
    "        if paper_id in TEMPORAL_ANOMALY_PAPERS:\n",
    "            paper[\"year\"] = 2023\n",
    "            paper[\"publication_date\"] = \"2023-01-15\"\n",
    "        \n",
    "        if i == 40:\n",
    "            paper[\"year\"] = 2021\n",
    "            paper[\"publication_date\"] = \"2021-03-01\"\n",
    "        \n",
    "        # Venue disambiguation\n",
    "        if i == 55: paper[\"venue\"] = \"NIPS\"\n",
    "        if i == 56: paper[\"venue\"] = \"NeurIPS\"\n",
    "        \n",
    "        # Conflicting affiliation\n",
    "        if i == 25:\n",
    "            paper[\"authors\"] = [\"Maria Garcia\"]\n",
    "            paper[\"institution\"] = \"MIT\"\n",
    "        \n",
    "        papers.append(paper)\n",
    "    return papers\n",
    "\n",
    "def generate_citations(papers):\n",
    "    citations = []\n",
    "    paper_years = {p[\"paper_id\"]: p[\"year\"] for p in papers}\n",
    "    paper_ids = [p[\"paper_id\"] for p in papers]\n",
    "    \n",
    "    for citing in paper_ids:\n",
    "        citable = [p for p in paper_ids if paper_years[p] <= paper_years[citing] and p != citing]\n",
    "        if citable:\n",
    "            for cited in random.sample(citable, min(3, len(citable))):\n",
    "                citations.append({\"citing_paper\": citing, \"cited_paper\": cited})\n",
    "    \n",
    "    # Orphan and self-citation\n",
    "    citations.append({\"citing_paper\": \"paper_0010\", \"cited_paper\": \"paper_9999\"})\n",
    "    citations.append({\"citing_paper\": \"paper_0015\", \"cited_paper\": \"paper_0015\"})\n",
    "    \n",
    "    # Citation ring\n",
    "    ring = CITATION_RING_PAPERS\n",
    "    for i in range(len(ring)):\n",
    "        citations.append({\"citing_paper\": ring[i], \"cited_paper\": ring[(i+1) % len(ring)]})\n",
    "    citations.append({\"citing_paper\": ring[0], \"cited_paper\": ring[2]})\n",
    "    citations.append({\"citing_paper\": ring[1], \"cited_paper\": ring[3]})\n",
    "    \n",
    "    # Temporal anomalies\n",
    "    for future in TEMPORAL_ANOMALY_PAPERS:\n",
    "        citations.append({\"citing_paper\": \"paper_0040\", \"cited_paper\": future})\n",
    "    \n",
    "    return citations\n",
    "\n",
    "def generate_affiliations():\n",
    "    affiliations = {\"authors\": {}, \"institutions\": {}, \"disambiguation_notes\": [], \"venue_notes\": []}\n",
    "    for aid, auth in CANONICAL_AUTHORS.items():\n",
    "        affiliations[\"authors\"][aid] = {\n",
    "            \"canonical_name\": auth[\"canonical_name\"],\n",
    "            \"known_variations\": auth[\"variations\"],\n",
    "            \"primary_institution\": auth[\"institution\"]\n",
    "        }\n",
    "    for iid, inst in CANONICAL_INSTITUTIONS.items():\n",
    "        affiliations[\"institutions\"][iid] = {\n",
    "            \"canonical_name\": inst[\"canonical_name\"],\n",
    "            \"known_variations\": inst[\"variations\"],\n",
    "            \"country\": inst[\"country\"]\n",
    "        }\n",
    "    affiliations[\"disambiguation_notes\"].append({\"warning\": \"J. Smith at MIT (auth_001) is DIFFERENT from J. Smith at Oxford (auth_011)\"})\n",
    "    affiliations[\"venue_notes\"].append(\"NIPS was renamed to NeurIPS in 2018\")\n",
    "    return affiliations\n",
    "\n",
    "# Generate\n",
    "print(\"Generating ENHANCED dataset with headroom challenges...\")\n",
    "papers_list = generate_papers(100)\n",
    "citations_list = generate_citations(papers_list)\n",
    "affiliations_data_gen = generate_affiliations()\n",
    "\n",
    "papers_raw = papers_list\n",
    "citations_raw = pd.DataFrame(citations_list)\n",
    "affiliations_raw = affiliations_data_gen\n",
    "\n",
    "print(f\"\\n✓ Dataset generated:\")\n",
    "print(f\"  Papers: {len(papers_raw)}, Citations: {len(citations_raw)}\")\n",
    "print(f\"\\n⚠️ HEADROOM CHALLENGES INCLUDED:\")\n",
    "print(f\"  - Ambiguous authors (J. Smith at MIT vs J. Smith at Oxford)\")\n",
    "print(f\"  - Typos (Jonh Smith, Maria Gracia)\")\n",
    "print(f\"  - Citation ring ({len(CITATION_RING_PAPERS)} papers)\")\n",
    "print(f\"  - Temporal anomalies (2021 paper cites 2023 papers)\")\n",
    "print(f\"  - Venue variations (NIPS vs NeurIPS)\")\n",
    "print(f\"  - Conflicting affiliations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Prompt\n",
    "\n",
    "The task specification for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_PROMPT = \"\"\"\n",
    "# Research Paper Entity Extraction and Citation Analysis Benchmark\n",
    "## (ENHANCED VERSION - Headroom Testing for Gemini 3 Pro)\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are a data scientist tasked with building an **advanced automated pipeline** for analyzing research paper metadata. Your goal is to extract structured information from a collection of research papers, resolve entity ambiguities (including challenging edge cases), detect anomalies in the citation network, and produce a comprehensive analytical report.\n",
    "\n",
    "**You must decide for yourself how to decompose the task**, which intermediate computations to perform, and in what order. **Do not simply follow a fixed step-by-step structure.**\n",
    "\n",
    "**This task contains deliberately challenging edge cases** that require careful reasoning to solve correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Context\n",
    "\n",
    "You have access to three data sources (already loaded in memory):\n",
    "\n",
    "### Input Data Structures\n",
    "\n",
    "**`papers_raw`** (`list[dict]`): List of ~100 paper records. Each paper dict has this schema:\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": str,           # e.g., \"paper_0001\"\n",
    "    \"title\": str,\n",
    "    \"authors\": list[str],      # e.g., [\"J. Smith\", \"Maria Garcia\"]\n",
    "    \"institution\": str | None, # e.g., \"MIT\" or \"Stanford University\"\n",
    "    \"abstract\": str,           # May be empty string \"\"\n",
    "    \"keywords\": list[str],     # e.g., [\"machine learning\", \"neural networks\"], may be []\n",
    "    \"venue\": str,              # e.g., \"NeurIPS\", \"ICML\", \"NIPS\" (venue names may vary!)\n",
    "    \"year\": int,\n",
    "    \"publication_date\": str    # ISO format \"YYYY-MM-DD\"\n",
    "}\n",
    "```\n",
    "\n",
    "**`citations_raw`** (`pd.DataFrame`): Citation relationships with columns:\n",
    "- `citing_paper`: str (paper_id of the paper doing the citing)\n",
    "- `cited_paper`: str (paper_id of the paper being cited)\n",
    "\n",
    "**`affiliations_raw`** (`dict`): Reference data for entity resolution. **Structure is a dict-of-dicts keyed by ID**:\n",
    "```python\n",
    "{\n",
    "    \"authors\": {\n",
    "        \"auth_001\": {\n",
    "            \"canonical_name\": str,       # e.g., \"John Smith\"\n",
    "            \"known_variations\": list[str], # e.g., [\"J. Smith\", \"John A. Smith\"]\n",
    "            \"primary_institution\": str   # Institution ID, e.g., \"inst_001\"\n",
    "        },\n",
    "        # ... more authors (NOTE: Some authors share initials but are DIFFERENT people!)\n",
    "    },\n",
    "    \"institutions\": {\n",
    "        \"inst_001\": {\n",
    "            \"canonical_name\": str,       # e.g., \"Massachusetts Institute of Technology\"\n",
    "            \"known_variations\": list[str], # e.g., [\"MIT\", \"M.I.T.\"]\n",
    "            \"country\": str\n",
    "        },\n",
    "        # ... more institutions\n",
    "    },\n",
    "    \"disambiguation_notes\": list[dict],  # Hints about ambiguous entities\n",
    "    \"venue_notes\": list[str]             # Notes about venue name changes\n",
    "}\n",
    "```\n",
    "\n",
    "### Data Challenges (ENHANCED - Requires Advanced Reasoning)\n",
    "\n",
    "The data contains **challenging edge cases** you must handle:\n",
    "\n",
    "#### Basic Challenges (Standard)\n",
    "- **Author name variations**: Same person appears as \"John Smith\", \"J. Smith\", \"Smith, John\"\n",
    "- **Institution name variations**: Same institution appears as \"MIT\", \"Massachusetts Institute of Technology\"\n",
    "- **Missing fields**: Some papers have empty abstract (`\"\"`) or empty keywords (`[]`)\n",
    "- **Orphan citations**: Some citations reference paper_ids that don't exist in papers_raw\n",
    "- **Self-citations**: Some papers cite themselves\n",
    "\n",
    "#### Advanced Challenges (Headroom Testing)\n",
    "\n",
    "1. **⚠️ AMBIGUOUS AUTHORS**: Some authors share the same initials but are **DIFFERENT PEOPLE** at different institutions.\n",
    "   - Example: \"J. Smith\" at MIT is **NOT** the same person as \"J. Smith\" at Oxford\n",
    "   - You must use **institution context** to disambiguate\n",
    "   - Naive merging will produce INCORRECT results\n",
    "\n",
    "2. **⚠️ TYPOS/OCR ERRORS**: Some author and institution names contain typos.\n",
    "   - Example: \"Jonh Smith\" should map to \"John Smith\"\n",
    "   - Example: \"Massachusets Institute of Technology\" should map to \"MIT\"\n",
    "   - You must use fuzzy matching or edit distance\n",
    "\n",
    "3. **⚠️ CITATION RING DETECTION**: A subset of papers cite each other in a suspicious circular pattern.\n",
    "   - Identify groups where: A→B→C→D→E→A (and cross-citations within)\n",
    "   - These should be flagged as anomalous\n",
    "\n",
    "4. **⚠️ TEMPORAL ANOMALIES**: Some citations violate temporal logic.\n",
    "   - A paper from 2021 cannot cite a paper from 2023\n",
    "   - Identify and flag these impossible citations\n",
    "\n",
    "5. **⚠️ VENUE DISAMBIGUATION**: Venues may appear with different names.\n",
    "   - \"NIPS\" → \"NeurIPS\" (renamed in 2018)\n",
    "   - \"CVPR\" → \"Conference on Computer Vision and Pattern Recognition\"\n",
    "   - These should be normalized to canonical forms\n",
    "\n",
    "6. **⚠️ CONFLICTING AFFILIATIONS**: Some papers list an author with an incorrect institution.\n",
    "   - Cross-reference with affiliations_raw to detect mismatches\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You are free to choose the order and decomposition of the task, but your final implementation must produce all of the following variables.\n",
    "\n",
    "### Required Variables\n",
    "\n",
    "#### Data Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `papers_df` | `pd.DataFrame` | Papers data with columns: paper_id, title, authors, institution, abstract, keywords, venue, year, publication_date |\n",
    "| `citations_df` | `pd.DataFrame` | Citation relationships with columns: citing_paper, cited_paper |\n",
    "| `affiliations_data` | `dict` | Author affiliations reference data |\n",
    "\n",
    "#### Entity Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `extracted_authors` | `list[dict]` | Each dict: `name`, `paper_ids`, `name_variations` |\n",
    "| `extracted_institutions` | `list[dict]` | Each dict: `name`, `paper_ids`, `name_variations` |\n",
    "| `extracted_topics` | `dict[str, int]` | Topic → frequency count |\n",
    "| `methods_from_abstracts` | `list[str]` | Research methods found in abstracts |\n",
    "\n",
    "#### Resolution Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `author_resolution_map` | `dict[str, str]` | Variation → canonical name |\n",
    "| `institution_resolution_map` | `dict[str, str]` | Variation → canonical name |\n",
    "| `resolved_author_count` | `int` | Unique authors after resolution |\n",
    "| `resolved_institution_count` | `int` | Unique institutions after resolution |\n",
    "\n",
    "#### Citation Network Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `citation_graph` | `dict[str, list[str]]` | Adjacency list |\n",
    "| `in_degree` | `dict[str, int]` | Incoming citations per paper |\n",
    "| `out_degree` | `dict[str, int]` | Outgoing citations per paper |\n",
    "| `pagerank_scores` | `dict[str, float]` | PageRank centrality scores |\n",
    "| `top_cited_papers` | `list[str]` | Top 10 most cited paper_ids |\n",
    "| `orphan_citations` | `list[dict]` | Citations to non-existent papers |\n",
    "| `self_citations` | `list[str]` | Paper_ids that cite themselves |\n",
    "\n",
    "#### ⭐ NEW: Anomaly Detection Variables (Headroom)\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `citation_ring_papers` | `list[str]` | Paper_ids involved in suspicious citation rings |\n",
    "| `temporal_anomalies` | `list[dict]` | Citations where citing_paper.year > cited_paper.year. Each dict: `citing_paper`, `cited_paper`, `citing_year`, `cited_year` |\n",
    "| `ambiguous_author_resolutions` | `list[dict]` | Cases where \"J. Smith\" was disambiguated. Each dict: `name_variation`, `resolved_to`, `institution_used`, `reasoning` |\n",
    "| `typo_corrections` | `list[dict]` | Typos that were corrected. Each dict: `original`, `corrected`, `confidence` |\n",
    "| `venue_normalizations` | `dict[str, str]` | Map of venue variations to canonical names |\n",
    "| `affiliation_conflicts` | `list[dict]` | Papers where listed institution doesn't match author's known institution. Each dict: `paper_id`, `author`, `listed_institution`, `expected_institution` |\n",
    "\n",
    "#### Validation Variables\n",
    "\n",
    "```python\n",
    "validation_results: dict[str, bool]\n",
    "```\n",
    "\n",
    "Required keys (ENHANCED):\n",
    "\n",
    "| Key | What to Check |\n",
    "|-----|---------------|\n",
    "| `\"papers_loaded_ok\"` | papers_df has expected columns and >0 rows |\n",
    "| `\"citations_loaded_ok\"` | citations_df has expected columns and >0 rows |\n",
    "| `\"affiliations_loaded_ok\"` | affiliations_data is valid dict |\n",
    "| `\"no_duplicate_paper_ids\"` | All paper_ids unique |\n",
    "| `\"authors_extracted\"` | extracted_authors has >0 entries |\n",
    "| `\"institutions_extracted\"` | extracted_institutions has >0 entries |\n",
    "| `\"resolution_maps_valid\"` | Resolution maps are non-empty |\n",
    "| `\"citation_graph_built\"` | citation_graph is non-empty |\n",
    "| `\"pagerank_computed\"` | pagerank_scores is non-empty with floats |\n",
    "| `\"orphans_identified\"` | Checked for orphan citations |\n",
    "| `\"self_citations_identified\"` | Checked for self-citations |\n",
    "| `\"all_pagerank_finite\"` | All PageRank values are finite |\n",
    "| `\"citation_rings_checked\"` | ⭐ Checked for citation rings |\n",
    "| `\"temporal_anomalies_checked\"` | ⭐ Checked for temporal violations |\n",
    "| `\"ambiguous_authors_handled\"` | ⭐ Used institution context for disambiguation |\n",
    "| `\"typos_handled\"` | ⭐ Applied fuzzy matching for typos |\n",
    "| `\"venues_normalized\"` | ⭐ Normalized venue name variations |\n",
    "\n",
    "#### Summary Statistics\n",
    "\n",
    "`summary_stats: dict[str, Any]` with required keys:\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|------|-------------|\n",
    "| `\"total_papers\"` | `int` | Total papers |\n",
    "| `\"total_citations\"` | `int` | Total citation relationships |\n",
    "| `\"unique_authors_raw\"` | `int` | Before resolution |\n",
    "| `\"unique_authors_resolved\"` | `int` | After resolution |\n",
    "| `\"unique_institutions_raw\"` | `int` | Before resolution |\n",
    "| `\"unique_institutions_resolved\"` | `int` | After resolution |\n",
    "| `\"papers_with_missing_abstract\"` | `int` | Empty/null abstract |\n",
    "| `\"papers_with_missing_keywords\"` | `int` | Empty/null keywords |\n",
    "| `\"orphan_citation_count\"` | `int` | Orphan citations |\n",
    "| `\"self_citation_count\"` | `int` | Self-citations |\n",
    "| `\"avg_citations_per_paper\"` | `float` | Average outgoing citations |\n",
    "| `\"most_common_venue\"` | `str` | Most frequent venue |\n",
    "| `\"year_range\"` | `tuple[int, int]` | (min_year, max_year) |\n",
    "| `\"citation_ring_count\"` | `int` | ⭐ Papers in citation rings |\n",
    "| `\"temporal_anomaly_count\"` | `int` | ⭐ Temporal violations |\n",
    "| `\"typo_correction_count\"` | `int` | ⭐ Typos corrected |\n",
    "| `\"affiliation_conflict_count\"` | `int` | ⭐ Affiliation mismatches |\n",
    "\n",
    "#### Final Report\n",
    "\n",
    "```python\n",
    "final_report: dict\n",
    "```\n",
    "\n",
    "Must have this structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"metadata\": {\n",
    "        \"task\": \"Research Paper Entity Extraction and Citation Analysis\",\n",
    "        \"papers_analyzed\": int,\n",
    "        \"execution_timestamp\": str\n",
    "    },\n",
    "    \"entity_extraction\": {\n",
    "        \"authors\": {\n",
    "            \"total_unique\": int,\n",
    "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]\n",
    "        },\n",
    "        \"institutions\": {\n",
    "            \"total_unique\": int,\n",
    "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]\n",
    "        },\n",
    "        \"topics\": {\n",
    "            \"total_unique\": int,\n",
    "            \"top_10_by_frequency\": [{\"topic\": str, \"count\": int}, ...]\n",
    "        }\n",
    "    },\n",
    "    \"citation_analysis\": {\n",
    "        \"total_citations\": int,\n",
    "        \"top_10_cited_papers\": [{\"paper_id\": str, \"citation_count\": int, \"title\": str}, ...],\n",
    "        \"orphan_citations\": [{\"citing_paper\": str, \"cited_paper\": str}, ...],\n",
    "        \"self_citations\": [str, ...],\n",
    "        \"network_statistics\": {\n",
    "            \"avg_in_degree\": float,\n",
    "            \"avg_out_degree\": float,\n",
    "            \"max_in_degree\": int,\n",
    "            \"max_out_degree\": int\n",
    "        }\n",
    "    },\n",
    "    \"anomaly_detection\": {  # ⭐ NEW SECTION\n",
    "        \"citation_rings\": {\n",
    "            \"detected\": bool,\n",
    "            \"papers_involved\": [str, ...],\n",
    "            \"description\": str\n",
    "        },\n",
    "        \"temporal_anomalies\": {\n",
    "            \"count\": int,\n",
    "            \"examples\": [{\"citing\": str, \"cited\": str, \"issue\": str}, ...]\n",
    "        },\n",
    "        \"ambiguous_resolutions\": [\n",
    "            {\"variation\": str, \"resolved_to\": str, \"method\": str}, ...\n",
    "        ],\n",
    "        \"typo_corrections\": [\n",
    "            {\"original\": str, \"corrected\": str}, ...\n",
    "        ],\n",
    "        \"affiliation_conflicts\": [\n",
    "            {\"paper_id\": str, \"author\": str, \"conflict\": str}, ...\n",
    "        ]\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_abstracts\": int,\n",
    "        \"missing_keywords\": int,\n",
    "        \"missing_institutions\": int,\n",
    "        \"duplicate_author_entries\": int\n",
    "    },\n",
    "    \"validation_summary\": {\n",
    "        \"all_checks_passed\": bool,\n",
    "        \"failed_checks\": [str, ...]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Constraints\n",
    "\n",
    "1. **Do not hardcode specific paper IDs, author names, or institution names**\n",
    "2. **Entity resolution MUST use institution context for disambiguation** - \"J. Smith\" at MIT ≠ \"J. Smith\" at Oxford\n",
    "3. **Typo handling MUST use fuzzy matching** (e.g., Levenshtein distance)\n",
    "4. **PageRank with damping factor 0.85**\n",
    "5. **Citation rings require cycle detection** in the citation graph\n",
    "6. **Temporal anomalies require comparing publication years**\n",
    "7. **All intermediate variables must be inspectable**\n",
    "8. **Handle edge cases gracefully**\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "Your solution is successful if:\n",
    "\n",
    "1. **All validation checks pass** (including new headroom checks)\n",
    "2. **Entity resolution correctly disambiguates** \"J. Smith\" at different institutions as different people\n",
    "3. **Citation rings are detected** (there is at least one ring of 5 papers)\n",
    "4. **Temporal anomalies are detected** (there is at least one)\n",
    "5. **Typos are corrected** with fuzzy matching\n",
    "6. **Venue names are normalized** (NIPS → NeurIPS)\n",
    "7. **Affiliation conflicts are identified**\n",
    "8. **PageRank scores sum to ~1.0**\n",
    "9. **Final report follows the exact schema**\n",
    "10. **All numeric values are finite**\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format\n",
    "\n",
    "```python\n",
    "import json\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(json.dumps(validation_results, indent=2))\n",
    "print(\"\\n=== FINAL REPORT ===\")\n",
    "print(json.dumps(final_report, indent=2, default=str))\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Enhanced benchmark prompt loaded\")\n",
    "print(\"Includes: Ambiguous author disambiguation, citation rings, temporal anomalies, typos, venue normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_task(prompt, data_context):\n",
    "    \"\"\"Run the agent using google.colab.ai to generate code for the task.\"\"\"\n",
    "    \n",
    "    # Prepare context with data samples\n",
    "    context = f\"\"\"\n",
    "You have access to the following data (already loaded in Python):\n",
    "\n",
    "papers_raw: A list of {len(data_context['papers'])} paper dictionaries\n",
    "Sample: {json.dumps(data_context['papers'][0], indent=2)}\n",
    "\n",
    "citations_raw: A pandas DataFrame with {len(data_context['citations'])} rows\n",
    "Columns: {data_context['citations'].columns.tolist()}\n",
    "Sample:\n",
    "{data_context['citations'].head(3).to_string()}\n",
    "\n",
    "affiliations_raw: A dictionary with author and institution reference data\n",
    "Keys: {list(data_context['affiliations'].keys())}\n",
    "Sample author: {json.dumps(list(data_context['affiliations']['authors'].values())[0], indent=2)}\n",
    "Sample institution: {json.dumps(list(data_context['affiliations']['institutions'].values())[0], indent=2)}\n",
    "\n",
    "{prompt}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Sending task to agent...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Use google.colab.ai to generate response\n",
    "    # The ai.generate_text function uses the Colab Pro's native AI capabilities\n",
    "    response = ai.generate_text(\n",
    "        prompt=context,\n",
    "        model_name=MODEL_NAME,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Prepare data context\n",
    "data_context = {\n",
    "    'papers': papers_raw,\n",
    "    'citations': citations_raw,\n",
    "    'affiliations': affiliations_raw\n",
    "}\n",
    "\n",
    "# Run the agent\n",
    "agent_response = run_agent_task(BENCHMARK_PROMPT, data_context)\n",
    "print(\"Agent response received\")\n",
    "print(\"=\"*50)\n",
    "print(agent_response[:2000] + \"...\" if len(agent_response) > 2000 else agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Python code from agent response and execute it\n",
    "def extract_and_execute_code(response_text):\n",
    "    \"\"\"Extract Python code blocks from the response and execute them.\"\"\"\n",
    "    \n",
    "    # Find all code blocks\n",
    "    code_blocks = re.findall(r'```python\\n(.*?)```', response_text, re.DOTALL)\n",
    "    \n",
    "    if not code_blocks:\n",
    "        # Try without language specifier\n",
    "        code_blocks = re.findall(r'```\\n(.*?)```', response_text, re.DOTALL)\n",
    "    \n",
    "    if not code_blocks:\n",
    "        print(\"No code blocks found in response\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all code blocks\n",
    "    full_code = \"\\n\\n\".join(code_blocks)\n",
    "    \n",
    "    print(f\"Extracted {len(code_blocks)} code block(s)\")\n",
    "    print(\"Executing agent code...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Execute the code\n",
    "    exec_globals = {\n",
    "        'papers_raw': papers_raw,\n",
    "        'citations_raw': citations_raw,\n",
    "        'affiliations_raw': affiliations_raw,\n",
    "        'pd': pd,\n",
    "        'np': np,\n",
    "        'json': json,\n",
    "        're': re,\n",
    "        'nx': nx,\n",
    "        'defaultdict': defaultdict,\n",
    "        'Counter': Counter,\n",
    "        'datetime': datetime,\n",
    "        'Dict': Dict,\n",
    "        'List': List,\n",
    "        'Any': Any,\n",
    "        'Tuple': Tuple,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        exec(full_code, exec_globals)\n",
    "        print(\"Code executed successfully!\")\n",
    "        return exec_globals\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing code: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute the agent's code\n",
    "exec_result = extract_and_execute_code(agent_response)\n",
    "\n",
    "# If successful, extract variables to global scope\n",
    "if exec_result:\n",
    "    required_vars = [\n",
    "        'papers_df', 'citations_df', 'affiliations_data',\n",
    "        'extracted_authors', 'extracted_institutions', 'extracted_topics', 'methods_from_abstracts',\n",
    "        'author_resolution_map', 'institution_resolution_map', 'resolved_author_count', 'resolved_institution_count',\n",
    "        'citation_graph', 'in_degree', 'out_degree', 'pagerank_scores', 'top_cited_papers',\n",
    "        'orphan_citations', 'self_citations',\n",
    "        'validation_results', 'summary_stats', 'final_report'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nVariable extraction:\")\n",
    "    for var in required_vars:\n",
    "        if var in exec_result:\n",
    "            globals()[var] = exec_result[var]\n",
    "            print(f\"  ✓ {var}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var} (missing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Output\n",
    "\n",
    "Display the results produced by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the agent's outputs\n",
    "try:\n",
    "    print(\"=== VALIDATION RESULTS ===\")\n",
    "    print(json.dumps(validation_results, indent=2))\n",
    "    print(\"\\n=== FINAL REPORT ===\")\n",
    "    print(json.dumps(final_report, indent=2, default=str))\n",
    "except NameError as e:\n",
    "    print(f\"Variable not defined: {e}\")\n",
    "    print(\"Agent may not have completed the task successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Unit Tests\n",
    "\n",
    "Comprehensive tests to validate the agent's solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataLoading(unittest.TestCase):\n",
    "    def test_papers_df_exists_and_not_empty(self):\n",
    "        self.assertIsInstance(papers_df, pd.DataFrame)\n",
    "        self.assertGreater(len(papers_df), 0)\n",
    "    \n",
    "    def test_papers_df_has_required_columns(self):\n",
    "        required = {'paper_id', 'title', 'authors', 'institution', 'abstract', 'keywords', 'venue', 'year'}\n",
    "        self.assertTrue(required.issubset(set(papers_df.columns)))\n",
    "    \n",
    "    def test_citations_df_exists(self):\n",
    "        self.assertIsInstance(citations_df, pd.DataFrame)\n",
    "        self.assertGreater(len(citations_df), 0)\n",
    "    \n",
    "    def test_affiliations_data_structure(self):\n",
    "        self.assertIsInstance(affiliations_data, dict)\n",
    "        self.assertIn('authors', affiliations_data)\n",
    "\n",
    "class TestEntityExtraction(unittest.TestCase):\n",
    "    def test_extracted_authors_not_empty(self):\n",
    "        self.assertGreater(len(extracted_authors), 0)\n",
    "    \n",
    "    def test_extracted_institutions_not_empty(self):\n",
    "        self.assertGreater(len(extracted_institutions), 0)\n",
    "\n",
    "class TestEntityResolution(unittest.TestCase):\n",
    "    def test_author_resolution_map_not_empty(self):\n",
    "        self.assertGreater(len(author_resolution_map), 0)\n",
    "    \n",
    "    def test_resolved_counts_positive(self):\n",
    "        self.assertGreater(resolved_author_count, 0)\n",
    "\n",
    "class TestCitationNetwork(unittest.TestCase):\n",
    "    def test_citation_graph_not_empty(self):\n",
    "        self.assertGreater(len(citation_graph), 0)\n",
    "    \n",
    "    def test_pagerank_scores_sum_to_one(self):\n",
    "        self.assertAlmostEqual(sum(pagerank_scores.values()), 1.0, delta=0.01)\n",
    "    \n",
    "    def test_orphan_citations_identified(self):\n",
    "        self.assertGreater(len(orphan_citations), 0)\n",
    "    \n",
    "    def test_self_citations_identified(self):\n",
    "        self.assertGreater(len(self_citations), 0)\n",
    "\n",
    "class TestHeadroomChallenges(unittest.TestCase):\n",
    "    \"\"\"Tests for ENHANCED headroom challenges - weaker models should fail these.\"\"\"\n",
    "    \n",
    "    def test_citation_rings_detected(self):\n",
    "        self.assertTrue(hasattr(globals().get('citation_ring_papers', None), '__len__') or \n",
    "                       'citation_ring_papers' in dir(),\n",
    "                       \"Must detect citation rings\")\n",
    "        self.assertGreater(len(citation_ring_papers), 0, \"Should find citation ring papers\")\n",
    "    \n",
    "    def test_temporal_anomalies_detected(self):\n",
    "        self.assertTrue('temporal_anomalies' in dir() or hasattr(globals().get('temporal_anomalies', None), '__len__'),\n",
    "                       \"Must detect temporal anomalies\")\n",
    "        self.assertGreater(len(temporal_anomalies), 0, \"Should find temporal anomalies\")\n",
    "    \n",
    "    def test_typo_corrections_made(self):\n",
    "        self.assertTrue('typo_corrections' in dir() or hasattr(globals().get('typo_corrections', None), '__len__'),\n",
    "                       \"Must correct typos\")\n",
    "    \n",
    "    def test_ambiguous_authors_disambiguated(self):\n",
    "        self.assertTrue('ambiguous_author_resolutions' in dir() or \n",
    "                       hasattr(globals().get('ambiguous_author_resolutions', None), '__len__'),\n",
    "                       \"Must disambiguate authors like J. Smith\")\n",
    "\n",
    "class TestValidationResults(unittest.TestCase):\n",
    "    def test_validation_results_is_dict(self):\n",
    "        self.assertIsInstance(validation_results, dict)\n",
    "    \n",
    "    def test_headroom_validations_present(self):\n",
    "        self.assertIn(\"citation_rings_checked\", validation_results)\n",
    "        self.assertIn(\"temporal_anomalies_checked\", validation_results)\n",
    "\n",
    "class TestFinalReport(unittest.TestCase):\n",
    "    def test_final_report_has_anomaly_detection(self):\n",
    "        self.assertIn('anomaly_detection', final_report, \"Report must include anomaly_detection section\")\n",
    "    \n",
    "    def test_final_report_has_citation_rings(self):\n",
    "        self.assertIn('citation_rings', final_report.get('anomaly_detection', {}))\n",
    "    \n",
    "    def test_all_checks_passed(self):\n",
    "        self.assertTrue(final_report['validation_summary']['all_checks_passed'])\n",
    "\n",
    "class TestSummaryStats(unittest.TestCase):\n",
    "    def test_summary_stats_has_required_keys(self):\n",
    "        required = ['total_papers', 'total_citations', 'unique_authors_raw', \n",
    "                    'unique_authors_resolved', 'orphan_citation_count']\n",
    "        for key in required:\n",
    "            self.assertIn(key, summary_stats)\n",
    "    \n",
    "    def test_summary_stats_headroom_keys(self):\n",
    "        self.assertIn('citation_ring_count', summary_stats)\n",
    "        self.assertIn('temporal_anomaly_count', summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all unit tests\n",
    "def run_tests():\n",
    "    \"\"\"Run all unit tests and report results.\"\"\"\n",
    "    loader = unittest.TestLoader()\n",
    "    suite = unittest.TestSuite()\n",
    "    \n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDataLoading))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestEntityExtraction))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestEntityResolution))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestCitationNetwork))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestHeadroomChallenges))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestValidationResults))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestSummaryStats))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestFinalReport))\n",
    "    \n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    print(f\"Success: {result.wasSuccessful()}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Execute tests\n",
    "try:\n",
    "    test_result = run_tests()\n",
    "except Exception as e:\n",
    "    print(f\"Error running tests: {e}\")\n",
    "    print(\"Some required variables may not be defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nAgent Model: {MODEL_NAME}\")\n",
    "    print(f\"Papers Analyzed: {len(papers_df)}\")\n",
    "    print(f\"Citations Processed: {len(citations_df)}\")\n",
    "    print(f\"\\nEntity Resolution:\")\n",
    "    print(f\"  Authors: {summary_stats.get('unique_authors_raw', 'N/A')} raw -> {resolved_author_count} resolved\")\n",
    "    print(f\"  Institutions: {summary_stats.get('unique_institutions_raw', 'N/A')} raw -> {resolved_institution_count} resolved\")\n",
    "    print(f\"\\nCitation Network:\")\n",
    "    print(f\"  Orphan citations found: {len(orphan_citations)}\")\n",
    "    print(f\"  Self-citations found: {len(self_citations)}\")\n",
    "    print(f\"  PageRank sum: {sum(pagerank_scores.values()):.4f}\")\n",
    "    print(f\"\\nValidation Summary:\")\n",
    "    failed = [k for k, v in validation_results.items() if not v]\n",
    "    if failed:\n",
    "        print(f\"  FAILED checks: {failed}\")\n",
    "    else:\n",
    "        print(\"  ALL CHECKS PASSED ✓\")\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Tests run: {test_result.testsRun}\")\n",
    "    print(f\"  Failures: {len(test_result.failures)}\")\n",
    "    print(f\"  Errors: {len(test_result.errors)}\")\n",
    "    \n",
    "    if test_result.wasSuccessful() and not failed:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ BENCHMARK COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✗ BENCHMARK COMPLETED WITH ISSUES\")\n",
    "        print(\"=\"*60)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError generating summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
