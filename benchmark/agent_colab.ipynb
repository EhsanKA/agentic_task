{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Colab: Model Comparison Benchmark\n",
        "\n",
        "Compares **Gemini 3 Pro Preview** vs **Gemini 2.5 Pro** on an agentic, data-driven reasoning task.\n",
        "\n",
        "Each model is run **N times** on the same task and data. Per-test pass rates are collected to show statistically meaningful differences â€” especially on the \"headroom\" challenges (citation rings, temporal anomalies, typo correction, ambiguous author disambiguation).\n",
        "\n",
        "**Setup:** Google Colab Pro with `google.colab.ai` (no API keys needed)\n",
        "\n",
        "**Agent must:**\n",
        "1. Load data from files (environment interaction)\n",
        "2. Extract entities and resolve ambiguities (multi-step reasoning)\n",
        "3. Analyze citation network and detect anomalies (graph reasoning)\n",
        "4. Save final_report.json to disk (artifact generation)\n",
        "5. Pass all unit tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, sys, os, shutil\n",
        "\n",
        "REPO_URL = \"https://github.com/EhsanKA/agentic_task.git\"\n",
        "REPO_DIR = \"/content/agentic_task\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--force-reinstall\", \"--no-cache-dir\", REPO_DIR], check=True)\n",
        "\n",
        "# Purge ALL cached benchmark submodules so fresh code is loaded\n",
        "for mod_name in list(sys.modules):\n",
        "    if mod_name == \"benchmark\" or mod_name.startswith(\"benchmark.\"):\n",
        "        del sys.modules[mod_name]\n",
        "\n",
        "# Verify correct version loaded\n",
        "from benchmark.evaluation.agent import build_agent_context\n",
        "import inspect\n",
        "print(\"build_agent_context signature:\", inspect.signature(build_agent_context))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import ai\n",
        "import json\n",
        "\n",
        "available_models = ai.list_models()\n",
        "print(\"Available models:\", available_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from benchmark.data.loader import setup_data\n",
        "from benchmark.evaluation.prompt import BENCHMARK_PROMPT\n",
        "from benchmark.evaluation.agent import build_agent_context\n",
        "from benchmark.evaluation.comparison import (\n",
        "    run_comparison, pass_rate_table, summary_table, run_log_table, print_verdict,\n",
        ")\n",
        "\n",
        "# --- Comparison configuration ---\n",
        "MODELS_TO_COMPARE = [\n",
        "    \"google/gemini-3-pro-preview\",\n",
        "    \"google/gemini-2.5-pro\",\n",
        "]\n",
        "NUM_RUNS = 5  # runs per model (increase to 10 for stronger statistical signal)\n",
        "\n",
        "# Filter to models actually available in this Colab session\n",
        "MODELS_TO_COMPARE = [m for m in MODELS_TO_COMPARE if m in available_models]\n",
        "assert len(MODELS_TO_COMPARE) >= 1, f\"None of the target models available. Have: {available_models}\"\n",
        "print(f\"Models to compare: {MODELS_TO_COMPARE}\")\n",
        "print(f\"Runs per model:    {NUM_RUNS}\")\n",
        "\n",
        "# Generate data once (fixed across all runs for fair comparison)\n",
        "_, _, _, DATA_DIR = setup_data()\n",
        "print(f\"Data directory:    {DATA_DIR}\")\n",
        "\n",
        "# Adapter: wrap google.colab.ai into the signature comparison.py expects\n",
        "def generate_fn(prompt, model_name):\n",
        "    return ai.generate_text(prompt=prompt, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Run Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "context = build_agent_context(BENCHMARK_PROMPT, DATA_DIR)\n",
        "\n",
        "all_run_results = run_comparison(\n",
        "    models=MODELS_TO_COMPARE,\n",
        "    num_runs=NUM_RUNS,\n",
        "    context=context,\n",
        "    data_dir=DATA_DIR,\n",
        "    generate_fn=generate_fn,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_rates = pass_rate_table(all_run_results, MODELS_TO_COMPARE)\n",
        "print(\"=\" * 70)\n",
        "print(\"PER-TEST PASS RATES\")\n",
        "print(\"=\" * 70)\n",
        "print(df_rates.to_string())\n",
        "print(\"\\n*** = headroom challenge test (designed to differentiate models)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_summary = summary_table(all_run_results, MODELS_TO_COMPARE)\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(df_summary.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Run Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_runs = run_log_table(all_run_results)\n",
        "print(df_runs.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verdict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print_verdict(all_run_results, MODELS_TO_COMPARE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}