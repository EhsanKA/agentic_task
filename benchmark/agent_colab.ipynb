{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
        "\n",
        "This notebook sets up **Gemini 3 Pro** as an autonomous agent to solve the Research Paper Entity Extraction and Citation Analysis benchmark.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab Pro (for native Gemini access via `google.colab.ai`)\n",
        "- Dataset files uploaded or accessible\n",
        "\n",
        "**Model Used:**\n",
        "- `google/gemini-3-pro` - The most advanced reasoning model available in Colab Pro\n",
        "\n",
        "**Implementation:**\n",
        "- Uses `google.colab.ai` module for native Colab Pro AI integration\n",
        "- No external API keys required - uses Colab Pro's built-in AI capabilities\n",
        "\n",
        "**Note:** This notebook should run end-to-end without manual intervention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q pandas networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available AI models in Colab Pro:\n"
          ]
        },
        {
          "ename": "TimeoutException",
          "evalue": "Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1270679202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# List available models in Colab Pro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available AI models in Colab Pro:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mavailable_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36mlist_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_proxy_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     response = _requests.get(\n\u001b[1;32m    132\u001b[0m         \u001b[0;34mf'{_get_model_proxy_host()}/models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36m_get_model_proxy_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_userdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import re\n",
        "import networkx as nx\n",
        "import warnings\n",
        "import unittest\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# List available models in Colab Pro\n",
        "print(\"Available AI models in Colab Pro:\")\n",
        "available_models = ai.list_models()\n",
        "for model in available_models:\n",
        "    print(f\"  - {model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Configuration\n",
        "\n",
        "Select and configure Gemini-3-Pro from available Colab Pro models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent model initialized: gemini-3-pro\n"
          ]
        }
      ],
      "source": [
        "# Select the most capable model for agentic tasks\n",
        "# Using Gemini 2.5 Pro - the most advanced reasoning model available\n",
        "MODEL_NAME = \"google/gemini-2.5-pro\"\n",
        "\n",
        "# Verify the model is available\n",
        "if MODEL_NAME in available_models:\n",
        "    print(f\"Model '{MODEL_NAME}' is available - SELECTED\")\n",
        "else:\n",
        "    print(f\"Warning: '{MODEL_NAME}' not found. Available models: {available_models}\")\n",
        "    # Fallback to other Pro/capable models\n",
        "    fallback_order = [\"google/gemini-2.5-pro\", \"google/gemini-2.0-flash\", \"google/gemini-2.5-flash\"]\n",
        "    for fallback in fallback_order:\n",
        "        if fallback in available_models:\n",
        "            MODEL_NAME = fallback\n",
        "            print(f\"Using fallback model: {MODEL_NAME}\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nAgent model selected: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Load the benchmark dataset files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'papers_metadata.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2805805738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAPERS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpapers_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'papers_metadata.json'"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "PAPERS_FILE = \"papers_metadata.json\"\n",
        "CITATIONS_FILE = \"citations.csv\"\n",
        "AFFILIATIONS_FILE = \"author_affiliations.json\"\n",
        "\n",
        "# Load the data files\n",
        "with open(PAPERS_FILE, 'r') as f:\n",
        "    papers_raw = json.load(f)\n",
        "\n",
        "citations_raw = pd.read_csv(CITATIONS_FILE)\n",
        "\n",
        "with open(AFFILIATIONS_FILE, 'r') as f:\n",
        "    affiliations_raw = json.load(f)\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"  - Papers: {len(papers_raw)} records\")\n",
        "print(f\"  - Citations: {len(citations_raw)} relationships\")\n",
        "print(f\"  - Affiliations: {len(affiliations_raw.get('authors', {}))} authors, {len(affiliations_raw.get('institutions', {}))} institutions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Prompt\n",
        "\n",
        "The task specification for the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BENCHMARK_PROMPT = \"\"\"\n",
        "# Research Paper Entity Extraction and Citation Analysis Benchmark\n",
        "\n",
        "## Scenario\n",
        "\n",
        "You are a data scientist tasked with building an automated pipeline for analyzing research paper metadata. \n",
        "Your goal is to extract structured information from a collection of research papers, resolve entity ambiguities, \n",
        "construct a citation network, and produce a comprehensive analytical report.\n",
        "\n",
        "You must decide for yourself how to decompose the task, which intermediate computations to perform, and in what order.\n",
        "Do not simply follow a fixed step-by-step structure.\n",
        "\n",
        "## Context\n",
        "\n",
        "You have access to three data files (already loaded):\n",
        "1. papers_raw - List of paper metadata dictionaries\n",
        "2. citations_raw - DataFrame of citation relationships  \n",
        "3. affiliations_raw - Reference data about known authors and institutions\n",
        "\n",
        "The data contains intentional challenges:\n",
        "- Author names appear in different formats (e.g., \"John Smith\" vs \"J. Smith\")\n",
        "- Institution names have variations (e.g., \"MIT\" vs \"Massachusetts Institute of Technology\")\n",
        "- Some papers have missing fields\n",
        "- The citation data may contain anomalies (orphan references, self-citations)\n",
        "\n",
        "## Required Output Variables\n",
        "\n",
        "You must produce these variables:\n",
        "\n",
        "### Core Data Variables\n",
        "- papers_df: pd.DataFrame with columns: paper_id, title, authors, institution, abstract, keywords, venue, year, publication_date\n",
        "- citations_df: pd.DataFrame with columns: citing_paper, cited_paper\n",
        "- affiliations_data: dict with 'authors' and 'institutions' keys\n",
        "\n",
        "### Entity Extraction Variables\n",
        "- extracted_authors: list[dict] with keys: name, paper_ids, name_variations\n",
        "- extracted_institutions: list[dict] with keys: name, paper_ids, name_variations\n",
        "- extracted_topics: dict[str, int] mapping topics to frequency counts\n",
        "- methods_from_abstracts: list[str] of research methods found\n",
        "\n",
        "### Entity Resolution Variables\n",
        "- author_resolution_map: dict[str, str] mapping variations to canonical names\n",
        "- institution_resolution_map: dict[str, str] mapping variations to canonical names\n",
        "- resolved_author_count: int\n",
        "- resolved_institution_count: int\n",
        "\n",
        "### Citation Network Variables\n",
        "- citation_graph: dict[str, list[str]] adjacency list\n",
        "- in_degree: dict[str, int] incoming citations per paper\n",
        "- out_degree: dict[str, int] outgoing citations per paper\n",
        "- pagerank_scores: dict[str, float] PageRank scores\n",
        "- top_cited_papers: list[str] top 10 most cited paper IDs\n",
        "- orphan_citations: list[dict] citations to non-existent papers\n",
        "- self_citations: list[str] papers that cite themselves\n",
        "\n",
        "### Validation Dictionary\n",
        "- validation_results: dict[str, bool] with keys:\n",
        "  - papers_loaded_ok, citations_loaded_ok, affiliations_loaded_ok\n",
        "  - no_duplicate_paper_ids, authors_extracted, institutions_extracted\n",
        "  - resolution_maps_valid, citation_graph_built, pagerank_computed\n",
        "  - orphans_identified, self_citations_identified, all_pagerank_finite\n",
        "\n",
        "### Summary Statistics\n",
        "- summary_stats: dict with keys:\n",
        "  - total_papers, total_citations, unique_authors_raw, unique_authors_resolved\n",
        "  - unique_institutions_raw, unique_institutions_resolved\n",
        "  - papers_with_missing_abstract, papers_with_missing_keywords\n",
        "  - orphan_citation_count, self_citation_count, avg_citations_per_paper\n",
        "  - most_common_venue, year_range\n",
        "\n",
        "### Final Report\n",
        "- final_report: dict with structure:\n",
        "  - metadata: {task, papers_analyzed, execution_timestamp}\n",
        "  - entity_extraction: {authors, institutions, topics}\n",
        "  - citation_analysis: {total_citations, top_10_cited_papers, orphan_citations, self_citations, network_statistics}\n",
        "  - data_quality: {missing_abstracts, missing_keywords, missing_institutions, duplicate_author_entries}\n",
        "  - validation_summary: {all_checks_passed, failed_checks}\n",
        "\n",
        "## Constraints\n",
        "1. Do not hardcode specific paper IDs, author names, or institution names\n",
        "2. Entity resolution must use fuzzy matching or reference data\n",
        "3. PageRank must use damping factor 0.85\n",
        "4. Handle edge cases gracefully\n",
        "\n",
        "## Success Criteria\n",
        "1. All validation checks pass\n",
        "2. Entity resolution reduces author count\n",
        "3. Orphan citations are identified (at least one exists)\n",
        "4. Self-citations are identified (at least one exists)\n",
        "5. PageRank scores sum to approximately 1.0\n",
        "6. Final report follows exact schema\n",
        "7. All numeric values are finite\n",
        "\n",
        "Write complete Python code to solve this task. Store all results in the specified variable names.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Benchmark prompt loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent_task(prompt, data_context):\n",
        "    \"\"\"Run the agent using google.colab.ai to generate code for the task.\"\"\"\n",
        "    \n",
        "    # Prepare context with data samples\n",
        "    context = f\"\"\"\n",
        "You have access to the following data (already loaded in Python):\n",
        "\n",
        "papers_raw: A list of {len(data_context['papers'])} paper dictionaries\n",
        "Sample: {json.dumps(data_context['papers'][0], indent=2)}\n",
        "\n",
        "citations_raw: A pandas DataFrame with {len(data_context['citations'])} rows\n",
        "Columns: {data_context['citations'].columns.tolist()}\n",
        "Sample:\n",
        "{data_context['citations'].head(3).to_string()}\n",
        "\n",
        "affiliations_raw: A dictionary with author and institution reference data\n",
        "Keys: {list(data_context['affiliations'].keys())}\n",
        "Sample author: {json.dumps(list(data_context['affiliations']['authors'].values())[0], indent=2)}\n",
        "Sample institution: {json.dumps(list(data_context['affiliations']['institutions'].values())[0], indent=2)}\n",
        "\n",
        "{prompt}\n",
        "\"\"\"\n",
        "    \n",
        "    print(\"Sending task to agent...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Use google.colab.ai to generate response\n",
        "    # The ai.generate_text function uses the Colab Pro's native AI capabilities\n",
        "    response = ai.generate_text(\n",
        "        prompt=context,\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.1,  # Lower temperature for deterministic outputs\n",
        "    )\n",
        "    \n",
        "    return response\n",
        "\n",
        "\n",
        "# Prepare data context\n",
        "data_context = {\n",
        "    'papers': papers_raw,\n",
        "    'citations': citations_raw,\n",
        "    'affiliations': affiliations_raw\n",
        "}\n",
        "\n",
        "# Run the agent\n",
        "agent_response = run_agent_task(BENCHMARK_PROMPT, data_context)\n",
        "print(\"Agent response received\")\n",
        "print(\"=\"*50)\n",
        "print(agent_response[:2000] + \"...\" if len(agent_response) > 2000 else agent_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Python code from agent response and execute it\n",
        "def extract_and_execute_code(response_text):\n",
        "    \"\"\"Extract Python code blocks from the response and execute them.\"\"\"\n",
        "    \n",
        "    # Find all code blocks\n",
        "    code_blocks = re.findall(r'```python\\n(.*?)```', response_text, re.DOTALL)\n",
        "    \n",
        "    if not code_blocks:\n",
        "        # Try without language specifier\n",
        "        code_blocks = re.findall(r'```\\n(.*?)```', response_text, re.DOTALL)\n",
        "    \n",
        "    if not code_blocks:\n",
        "        print(\"No code blocks found in response\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all code blocks\n",
        "    full_code = \"\\n\\n\".join(code_blocks)\n",
        "    \n",
        "    print(f\"Extracted {len(code_blocks)} code block(s)\")\n",
        "    print(\"Executing agent code...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Execute the code\n",
        "    exec_globals = {\n",
        "        'papers_raw': papers_raw,\n",
        "        'citations_raw': citations_raw,\n",
        "        'affiliations_raw': affiliations_raw,\n",
        "        'pd': pd,\n",
        "        'np': np,\n",
        "        'json': json,\n",
        "        're': re,\n",
        "        'nx': nx,\n",
        "        'defaultdict': defaultdict,\n",
        "        'Counter': Counter,\n",
        "        'datetime': datetime,\n",
        "        'Dict': Dict,\n",
        "        'List': List,\n",
        "        'Any': Any,\n",
        "        'Tuple': Tuple,\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        exec(full_code, exec_globals)\n",
        "        print(\"Code executed successfully!\")\n",
        "        return exec_globals\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing code: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute the agent's code\n",
        "exec_result = extract_and_execute_code(agent_response)\n",
        "\n",
        "# If successful, extract variables to global scope\n",
        "if exec_result:\n",
        "    # Extract all required variables\n",
        "    required_vars = [\n",
        "        'papers_df', 'citations_df', 'affiliations_data',\n",
        "        'extracted_authors', 'extracted_institutions', 'extracted_topics', 'methods_from_abstracts',\n",
        "        'author_resolution_map', 'institution_resolution_map', 'resolved_author_count', 'resolved_institution_count',\n",
        "        'citation_graph', 'in_degree', 'out_degree', 'pagerank_scores', 'top_cited_papers',\n",
        "        'orphan_citations', 'self_citations',\n",
        "        'validation_results', 'summary_stats', 'final_report'\n",
        "    ]\n",
        "    \n",
        "    for var in required_vars:\n",
        "        if var in exec_result:\n",
        "            globals()[var] = exec_result[var]\n",
        "            print(f\"  Loaded: {var}\")\n",
        "        else:\n",
        "            print(f\"  Missing: {var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the agent's outputs\n",
        "try:\n",
        "    print(\"=== VALIDATION RESULTS ===\")\n",
        "    print(json.dumps(validation_results, indent=2))\n",
        "    print(\"\\n=== FINAL REPORT ===\")\n",
        "    print(json.dumps(final_report, indent=2))\n",
        "except NameError as e:\n",
        "    print(f\"Variable not defined: {e}\")\n",
        "    print(\"Agent may not have completed the task successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Unit Tests\n",
        "\n",
        "Comprehensive tests to validate the agent's solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDataLoading(unittest.TestCase):\n",
        "    \"\"\"Tests for data loading functionality.\"\"\"\n",
        "    \n",
        "    def test_papers_df_exists_and_not_empty(self):\n",
        "        self.assertIsInstance(papers_df, pd.DataFrame)\n",
        "        self.assertGreater(len(papers_df), 0)\n",
        "    \n",
        "    def test_papers_df_has_required_columns(self):\n",
        "        required = {'paper_id', 'title', 'authors', 'institution', \n",
        "                   'abstract', 'keywords', 'venue', 'year', 'publication_date'}\n",
        "        self.assertTrue(required.issubset(set(papers_df.columns)))\n",
        "    \n",
        "    def test_citations_df_exists_and_not_empty(self):\n",
        "        self.assertIsInstance(citations_df, pd.DataFrame)\n",
        "        self.assertGreater(len(citations_df), 0)\n",
        "    \n",
        "    def test_citations_df_has_required_columns(self):\n",
        "        required = {'citing_paper', 'cited_paper'}\n",
        "        self.assertTrue(required.issubset(set(citations_df.columns)))\n",
        "    \n",
        "    def test_affiliations_data_structure(self):\n",
        "        self.assertIsInstance(affiliations_data, dict)\n",
        "        self.assertIn('authors', affiliations_data)\n",
        "        self.assertIn('institutions', affiliations_data)\n",
        "    \n",
        "    def test_no_duplicate_paper_ids(self):\n",
        "        self.assertEqual(papers_df['paper_id'].nunique(), len(papers_df))\n",
        "\n",
        "\n",
        "class TestEntityExtraction(unittest.TestCase):\n",
        "    \"\"\"Tests for entity extraction functionality.\"\"\"\n",
        "    \n",
        "    def test_extracted_authors_not_empty(self):\n",
        "        self.assertGreater(len(extracted_authors), 0)\n",
        "    \n",
        "    def test_extracted_authors_structure(self):\n",
        "        for author in extracted_authors:\n",
        "            self.assertIn('name', author)\n",
        "            self.assertIn('paper_ids', author)\n",
        "            self.assertIn('name_variations', author)\n",
        "    \n",
        "    def test_extracted_institutions_not_empty(self):\n",
        "        self.assertGreater(len(extracted_institutions), 0)\n",
        "    \n",
        "    def test_extracted_topics_is_dict(self):\n",
        "        self.assertIsInstance(extracted_topics, dict)\n",
        "    \n",
        "    def test_methods_from_abstracts_is_list(self):\n",
        "        self.assertIsInstance(methods_from_abstracts, list)\n",
        "\n",
        "\n",
        "class TestEntityResolution(unittest.TestCase):\n",
        "    \"\"\"Tests for entity resolution functionality.\"\"\"\n",
        "    \n",
        "    def test_author_resolution_map_not_empty(self):\n",
        "        self.assertGreater(len(author_resolution_map), 0)\n",
        "    \n",
        "    def test_institution_resolution_map_not_empty(self):\n",
        "        self.assertGreater(len(institution_resolution_map), 0)\n",
        "    \n",
        "    def test_resolution_reduces_author_count(self):\n",
        "        raw_count = len(extracted_authors)\n",
        "        self.assertLessEqual(resolved_author_count, raw_count)\n",
        "    \n",
        "    def test_resolved_counts_are_positive(self):\n",
        "        self.assertGreater(resolved_author_count, 0)\n",
        "        self.assertGreater(resolved_institution_count, 0)\n",
        "\n",
        "\n",
        "class TestCitationNetwork(unittest.TestCase):\n",
        "    \"\"\"Tests for citation network functionality.\"\"\"\n",
        "    \n",
        "    def test_citation_graph_not_empty(self):\n",
        "        self.assertGreater(len(citation_graph), 0)\n",
        "    \n",
        "    def test_in_degree_covers_all_papers(self):\n",
        "        self.assertEqual(len(in_degree), len(papers_df))\n",
        "    \n",
        "    def test_out_degree_covers_all_papers(self):\n",
        "        self.assertEqual(len(out_degree), len(papers_df))\n",
        "    \n",
        "    def test_pagerank_scores_not_empty(self):\n",
        "        self.assertGreater(len(pagerank_scores), 0)\n",
        "    \n",
        "    def test_pagerank_scores_sum_to_one(self):\n",
        "        total = sum(pagerank_scores.values())\n",
        "        self.assertAlmostEqual(total, 1.0, delta=0.01)\n",
        "    \n",
        "    def test_pagerank_scores_are_finite(self):\n",
        "        for score in pagerank_scores.values():\n",
        "            self.assertTrue(np.isfinite(score))\n",
        "    \n",
        "    def test_orphan_citations_identified(self):\n",
        "        self.assertIsInstance(orphan_citations, list)\n",
        "        self.assertGreater(len(orphan_citations), 0)\n",
        "    \n",
        "    def test_self_citations_identified(self):\n",
        "        self.assertIsInstance(self_citations, list)\n",
        "        self.assertGreater(len(self_citations), 0)\n",
        "\n",
        "\n",
        "class TestValidationResults(unittest.TestCase):\n",
        "    \"\"\"Tests for validation results.\"\"\"\n",
        "    \n",
        "    def test_validation_results_is_dict(self):\n",
        "        self.assertIsInstance(validation_results, dict)\n",
        "    \n",
        "    def test_validation_results_has_required_keys(self):\n",
        "        required_keys = {\n",
        "            \"papers_loaded_ok\", \"citations_loaded_ok\", \"affiliations_loaded_ok\",\n",
        "            \"no_duplicate_paper_ids\", \"authors_extracted\", \"institutions_extracted\",\n",
        "            \"resolution_maps_valid\", \"citation_graph_built\", \"pagerank_computed\",\n",
        "            \"orphans_identified\", \"self_citations_identified\", \"all_pagerank_finite\"\n",
        "        }\n",
        "        self.assertTrue(required_keys.issubset(set(validation_results.keys())))\n",
        "    \n",
        "    def test_all_validations_pass(self):\n",
        "        failed = [k for k, v in validation_results.items() if not v]\n",
        "        self.assertEqual(len(failed), 0, f\"Failed validations: {failed}\")\n",
        "\n",
        "\n",
        "class TestSummaryStats(unittest.TestCase):\n",
        "    \"\"\"Tests for summary statistics.\"\"\"\n",
        "    \n",
        "    def test_summary_stats_is_dict(self):\n",
        "        self.assertIsInstance(summary_stats, dict)\n",
        "    \n",
        "    def test_summary_stats_has_required_keys(self):\n",
        "        required_keys = {\n",
        "            \"total_papers\", \"total_citations\", \"unique_authors_raw\",\n",
        "            \"unique_authors_resolved\", \"unique_institutions_raw\",\n",
        "            \"unique_institutions_resolved\", \"papers_with_missing_abstract\",\n",
        "            \"papers_with_missing_keywords\", \"orphan_citation_count\",\n",
        "            \"self_citation_count\", \"avg_citations_per_paper\",\n",
        "            \"most_common_venue\", \"year_range\"\n",
        "        }\n",
        "        self.assertTrue(required_keys.issubset(set(summary_stats.keys())))\n",
        "\n",
        "\n",
        "class TestFinalReport(unittest.TestCase):\n",
        "    \"\"\"Tests for final report structure.\"\"\"\n",
        "    \n",
        "    def test_final_report_is_dict(self):\n",
        "        self.assertIsInstance(final_report, dict)\n",
        "    \n",
        "    def test_final_report_has_metadata(self):\n",
        "        self.assertIn('metadata', final_report)\n",
        "    \n",
        "    def test_final_report_has_entity_extraction(self):\n",
        "        self.assertIn('entity_extraction', final_report)\n",
        "    \n",
        "    def test_final_report_has_citation_analysis(self):\n",
        "        self.assertIn('citation_analysis', final_report)\n",
        "    \n",
        "    def test_final_report_has_data_quality(self):\n",
        "        self.assertIn('data_quality', final_report)\n",
        "    \n",
        "    def test_final_report_has_validation_summary(self):\n",
        "        self.assertIn('validation_summary', final_report)\n",
        "    \n",
        "    def test_all_checks_passed(self):\n",
        "        self.assertTrue(final_report['validation_summary']['all_checks_passed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all unit tests\n",
        "def run_tests():\n",
        "    \"\"\"Run all unit tests and report results.\"\"\"\n",
        "    loader = unittest.TestLoader()\n",
        "    suite = unittest.TestSuite()\n",
        "    \n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestDataLoading))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestEntityExtraction))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestEntityResolution))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestCitationNetwork))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestValidationResults))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestSummaryStats))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestFinalReport))\n",
        "    \n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    result = runner.run(suite)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Tests run: {result.testsRun}\")\n",
        "    print(f\"Failures: {len(result.failures)}\")\n",
        "    print(f\"Errors: {len(result.errors)}\")\n",
        "    print(f\"Success: {result.wasSuccessful()}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Execute tests\n",
        "try:\n",
        "    test_result = run_tests()\n",
        "except Exception as e:\n",
        "    print(f\"Error running tests: {e}\")\n",
        "    print(\"Some required variables may not be defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"BENCHMARK EXECUTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    print(f\"\\nAgent Model: {MODEL_NAME}\")\n",
        "    print(f\"Papers Analyzed: {len(papers_df)}\")\n",
        "    print(f\"Citations Processed: {len(citations_df)}\")\n",
        "    print(f\"\\nEntity Resolution:\")\n",
        "    print(f\"  Authors: {len(extracted_authors)} raw -> {resolved_author_count} resolved\")\n",
        "    print(f\"  Institutions: {len(extracted_institutions)} raw -> {resolved_institution_count} resolved\")\n",
        "    print(f\"\\nCitation Network:\")\n",
        "    print(f\"  Orphan citations found: {len(orphan_citations)}\")\n",
        "    print(f\"  Self-citations found: {len(self_citations)}\")\n",
        "    print(f\"  PageRank sum: {sum(pagerank_scores.values()):.4f}\")\n",
        "    print(f\"\\nValidation Summary:\")\n",
        "    failed = [k for k, v in validation_results.items() if not v]\n",
        "    if failed:\n",
        "        print(f\"  FAILED checks: {failed}\")\n",
        "    else:\n",
        "        print(\"  ALL CHECKS PASSED\")\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"  Tests run: {test_result.testsRun}\")\n",
        "    print(f\"  Failures: {len(test_result.failures)}\")\n",
        "    print(f\"  Errors: {len(test_result.errors)}\")\n",
        "    \n",
        "    if test_result.wasSuccessful() and not failed:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"BENCHMARK COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"BENCHMARK COMPLETED WITH ISSUES\")\n",
        "        print(\"=\"*60)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError generating summary: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
