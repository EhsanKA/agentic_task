{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
        "\n",
        "Evaluates **Gemini 3 Pro Preview** on an agentic, data-driven reasoning task.\n",
        "\n",
        "**Setup:** Google Colab Pro with `google.colab.ai` (no API keys needed)\n",
        "\n",
        "**Agent must:**\n",
        "1. Load data from files (environment interaction)\n",
        "2. Extract entities and resolve ambiguities (multi-step reasoning)\n",
        "3. Analyze citation network and detect anomalies (graph reasoning)\n",
        "4. Save final_report.json to disk (artifact generation)\n",
        "5. Pass all unit tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, sys, os, shutil\n",
        "\n",
        "REPO_URL = \"https://github.com/EhsanKA/agentic_task.git\"\n",
        "REPO_DIR = \"/content/agentic_task\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--force-reinstall\", \"--no-cache-dir\", REPO_DIR], check=True)\n",
        "\n",
        "# Purge ALL cached benchmark submodules so fresh code is loaded\n",
        "for mod_name in list(sys.modules):\n",
        "    if mod_name == \"benchmark\" or mod_name.startswith(\"benchmark.\"):\n",
        "        del sys.modules[mod_name]\n",
        "\n",
        "# Verify correct version loaded\n",
        "from benchmark.evaluation.agent import build_agent_context\n",
        "import inspect\n",
        "print(\"build_agent_context signature:\", inspect.signature(build_agent_context))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import ai\n",
        "import json\n",
        "\n",
        "available_models = ai.list_models()\n",
        "print(\"Available models:\", available_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from benchmark.data.loader import setup_data\n",
        "from benchmark.evaluation.prompt import BENCHMARK_PROMPT\n",
        "from benchmark.evaluation.agent import select_model, build_agent_context, execute_agent_code, extract_variables\n",
        "\n",
        "_, _, _, DATA_DIR = setup_data()\n",
        "MODEL_NAME = select_model(available_models)\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Model: {MODEL_NAME}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "context = build_agent_context(BENCHMARK_PROMPT, DATA_DIR)\n",
        "\n",
        "print(\"Sending task to agent...\")\n",
        "agent_response = ai.generate_text(prompt=context, model_name=MODEL_NAME)\n",
        "print(\"Response received.\")\n",
        "print(agent_response[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exec_result = execute_agent_code(agent_response, DATA_DIR)\n",
        "\n",
        "# Retry once if first attempt failed\n",
        "if exec_result and \"__error__\" in exec_result:\n",
        "    print(f\"\\nFirst attempt failed: {exec_result['__error__']}\")\n",
        "    print(\"Retrying with error context...\")\n",
        "    retry_prompt = (\n",
        "        f\"Your previous code produced this error:\\n{exec_result['__traceback__']}\\n\\n\"\n",
        "        f\"Fix the bug and return the corrected complete code.\\n\\n{context}\"\n",
        "    )\n",
        "    agent_response = ai.generate_text(prompt=retry_prompt, model_name=MODEL_NAME)\n",
        "    exec_result = execute_agent_code(agent_response, DATA_DIR)\n",
        "\n",
        "results = extract_variables(exec_result) if exec_result else {}\n",
        "for k, v in results.items():\n",
        "    globals()[k] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    print(json.dumps(validation_results, indent=2))\n",
        "    print(json.dumps(final_report, indent=2, default=str))\n",
        "except NameError as e:\n",
        "    print(f\"Missing: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from benchmark.evaluation.tests import set_context, run_all_tests\n",
        "\n",
        "results[\"_data_dir\"] = DATA_DIR\n",
        "set_context(results)\n",
        "test_result = run_all_tests()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Artifact saved: {os.path.exists(os.path.join(DATA_DIR, 'final_report.json'))}\")\n",
        "print(f\"Tests: {test_result.testsRun} run, {len(test_result.failures)} failures, {len(test_result.errors)} errors\")\n",
        "if test_result.wasSuccessful():\n",
        "    print(\"BENCHMARK PASSED\")\n",
        "else:\n",
        "    print(\"BENCHMARK FAILED\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}