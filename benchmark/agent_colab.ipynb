{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
        "\n",
        "Runs **Gemini 3 Pro Preview** as an autonomous agent on the benchmark task.\n",
        "Uses `google.colab.ai` for native Colab Pro AI integration. No API keys required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q pandas networkx python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q git+https://github.com/EhsanKA/agentic_task.git\n",
        "\n",
        "from google.colab import ai\n",
        "import json\n",
        "\n",
        "available_models = ai.list_models()\n",
        "print(\"Available models:\", available_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.data.loader import setup_data\n",
        "from benchmark.evaluation.prompt import BENCHMARK_PROMPT\n",
        "from benchmark.evaluation.agent import (\n",
        "    select_model, build_agent_context, execute_agent_code, extract_variables\n",
        ")\n",
        "\n",
        "papers_raw, citations_raw, affiliations_raw, DATA_DIR = setup_data()\n",
        "MODEL_NAME = select_model(available_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = build_agent_context(BENCHMARK_PROMPT, papers_raw, citations_raw, affiliations_raw)\n",
        "\n",
        "print(\"Sending task to agent...\")\n",
        "agent_response = ai.generate_text(prompt=context, model_name=MODEL_NAME)\n",
        "print(\"Response received.\")\n",
        "print(agent_response[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exec_result = execute_agent_code(agent_response, papers_raw, citations_raw, affiliations_raw)\n",
        "\n",
        "results = extract_variables(exec_result) if exec_result else {}\n",
        "for k, v in results.items():\n",
        "    globals()[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    print(json.dumps(validation_results, indent=2))\n",
        "    print(json.dumps(final_report, indent=2, default=str))\n",
        "except NameError as e:\n",
        "    print(f\"Missing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.evaluation.tests import set_context, run_all_tests\n",
        "\n",
        "set_context(results)\n",
        "test_result = run_all_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Tests: {test_result.testsRun} run, {len(test_result.failures)} failures, {len(test_result.errors)} errors\")\n",
        "if test_result.wasSuccessful():\n",
        "    print(\"BENCHMARK PASSED\")\n",
        "else:\n",
        "    print(\"BENCHMARK FAILED\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
