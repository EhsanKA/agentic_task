{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
    "\n",
    "This notebook sets up **Gemini 3 Pro Preview** as an autonomous agent to solve the Research Paper Entity Extraction and Citation Analysis benchmark.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (for native Gemini access via `google.colab.ai`)\n",
    "\n",
    "**Model Used:**\n",
    "- `google/gemini-3-pro-preview` - Gemini 3 Pro Preview model\n",
    "\n",
    "**Implementation:**\n",
    "- Uses `google.colab.ai` module for native Colab Pro AI integration\n",
    "- No external API keys required - uses Colab Pro's built-in AI capabilities\n",
    "- Self-contained dataset generation (no file uploads needed)\n",
    "\n",
    "**Note:** This notebook runs end-to-end without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q pandas networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available AI models in Colab Pro:\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1270679202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# List available models in Colab Pro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available AI models in Colab Pro:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mavailable_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36mlist_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_proxy_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     response = _requests.get(\n\u001b[1;32m    132\u001b[0m         \u001b[0;34mf'{_get_model_proxy_host()}/models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36m_get_model_proxy_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_userdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI."
     ]
    }
   ],
   "source": [
    "from google.colab import ai\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import unittest\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# List available models in Colab Pro\n",
    "print(\"Available AI models in Colab Pro:\")\n",
    "available_models = ai.list_models()\n",
    "for model in available_models:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "Select and configure Gemini-3-Pro from available Colab Pro models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent model initialized: gemini-3-pro\n"
     ]
    }
   ],
   "source": [
    "# Select the model for agentic tasks\n",
    "# Using Gemini 3 Pro Preview - the most advanced reasoning model available\n",
    "MODEL_NAME = \"google/gemini-3-pro-preview\"\n",
    "\n",
    "# Verify the model is available\n",
    "if MODEL_NAME in available_models:\n",
    "    print(f\"Model '{MODEL_NAME}' is available - SELECTED\")\n",
    "else:\n",
    "    print(f\"Warning: '{MODEL_NAME}' not found. Available models: {available_models}\")\n",
    "    # Fallback to other Pro/capable models\n",
    "    fallback_order = [\"google/gemini-2.5-pro\", \"google/gemini-2.0-flash\", \"google/gemini-2.5-flash\"]\n",
    "    for fallback in fallback_order:\n",
    "        if fallback in available_models:\n",
    "            MODEL_NAME = fallback\n",
    "            print(f\"Using fallback model: {MODEL_NAME}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nAgent model selected: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "Generate the synthetic benchmark dataset. This ensures the notebook is fully self-contained and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'papers_metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2805805738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAPERS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpapers_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'papers_metadata.json'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATASET GENERATION - Self-contained synthetic data generation\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "import csv\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Canonical authors with their name variations\n",
    "CANONICAL_AUTHORS = {\n",
    "    \"auth_001\": {\n",
    "        \"canonical_name\": \"John Smith\",\n",
    "        \"variations\": [\"J. Smith\", \"John A. Smith\", \"J. A. Smith\", \"Smith, John\"],\n",
    "        \"institution\": \"inst_001\"\n",
    "    },\n",
    "    \"auth_002\": {\n",
    "        \"canonical_name\": \"Maria Garcia\",\n",
    "        \"variations\": [\"M. Garcia\", \"Maria L. Garcia\", \"Garcia, Maria\", \"M. L. Garcia\"],\n",
    "        \"institution\": \"inst_002\"\n",
    "    },\n",
    "    \"auth_003\": {\n",
    "        \"canonical_name\": \"Wei Zhang\",\n",
    "        \"variations\": [\"W. Zhang\", \"Wei W. Zhang\", \"Zhang, Wei\", \"Zhang Wei\"],\n",
    "        \"institution\": \"inst_003\"\n",
    "    },\n",
    "    \"auth_004\": {\n",
    "        \"canonical_name\": \"Emily Johnson\",\n",
    "        \"variations\": [\"E. Johnson\", \"Emily R. Johnson\", \"Johnson, Emily\", \"E. R. Johnson\"],\n",
    "        \"institution\": \"inst_001\"\n",
    "    },\n",
    "    \"auth_005\": {\n",
    "        \"canonical_name\": \"Ahmed Hassan\",\n",
    "        \"variations\": [\"A. Hassan\", \"Ahmed M. Hassan\", \"Hassan, Ahmed\", \"A. M. Hassan\"],\n",
    "        \"institution\": \"inst_004\"\n",
    "    },\n",
    "    \"auth_006\": {\n",
    "        \"canonical_name\": \"Sarah Williams\",\n",
    "        \"variations\": [\"S. Williams\", \"Sarah K. Williams\", \"Williams, Sarah\", \"S. K. Williams\"],\n",
    "        \"institution\": \"inst_002\"\n",
    "    },\n",
    "    \"auth_007\": {\n",
    "        \"canonical_name\": \"Yuki Tanaka\",\n",
    "        \"variations\": [\"Y. Tanaka\", \"Yuki S. Tanaka\", \"Tanaka, Yuki\", \"Tanaka Yuki\"],\n",
    "        \"institution\": \"inst_005\"\n",
    "    },\n",
    "    \"auth_008\": {\n",
    "        \"canonical_name\": \"Michael Brown\",\n",
    "        \"variations\": [\"M. Brown\", \"Michael J. Brown\", \"Brown, Michael\", \"M. J. Brown\"],\n",
    "        \"institution\": \"inst_003\"\n",
    "    },\n",
    "    \"auth_009\": {\n",
    "        \"canonical_name\": \"Lisa Chen\",\n",
    "        \"variations\": [\"L. Chen\", \"Lisa Y. Chen\", \"Chen, Lisa\", \"Chen Lisa\"],\n",
    "        \"institution\": \"inst_004\"\n",
    "    },\n",
    "    \"auth_010\": {\n",
    "        \"canonical_name\": \"David Miller\",\n",
    "        \"variations\": [\"D. Miller\", \"David A. Miller\", \"Miller, David\", \"D. A. Miller\"],\n",
    "        \"institution\": \"inst_005\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Canonical institutions with their variations\n",
    "CANONICAL_INSTITUTIONS = {\n",
    "    \"inst_001\": {\n",
    "        \"canonical_name\": \"Massachusetts Institute of Technology\",\n",
    "        \"variations\": [\"MIT\", \"M.I.T.\", \"Massachusetts Inst. of Technology\", \"Mass. Institute of Technology\"],\n",
    "        \"country\": \"USA\"\n",
    "    },\n",
    "    \"inst_002\": {\n",
    "        \"canonical_name\": \"Stanford University\",\n",
    "        \"variations\": [\"Stanford\", \"Stanford Univ.\", \"Stanford U.\", \"Leland Stanford Junior University\"],\n",
    "        \"country\": \"USA\"\n",
    "    },\n",
    "    \"inst_003\": {\n",
    "        \"canonical_name\": \"Tsinghua University\",\n",
    "        \"variations\": [\"Tsinghua\", \"Tsinghua Univ.\", \"Qinghua University\", \"THU\"],\n",
    "        \"country\": \"China\"\n",
    "    },\n",
    "    \"inst_004\": {\n",
    "        \"canonical_name\": \"University of Oxford\",\n",
    "        \"variations\": [\"Oxford\", \"Oxford Univ.\", \"Oxford University\", \"Univ. of Oxford\"],\n",
    "        \"country\": \"UK\"\n",
    "    },\n",
    "    \"inst_005\": {\n",
    "        \"canonical_name\": \"University of Tokyo\",\n",
    "        \"variations\": [\"Tokyo Univ.\", \"UTokyo\", \"Tokyo University\", \"Univ. of Tokyo\"],\n",
    "        \"country\": \"Japan\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Research topics and methods\n",
    "RESEARCH_TOPICS = [\n",
    "    \"machine learning\", \"deep learning\", \"neural networks\", \"natural language processing\",\n",
    "    \"computer vision\", \"reinforcement learning\", \"transformer models\", \"attention mechanisms\",\n",
    "    \"graph neural networks\", \"federated learning\", \"transfer learning\", \"meta-learning\",\n",
    "    \"generative models\", \"adversarial learning\", \"explainable AI\", \"optimization\",\n",
    "    \"representation learning\", \"self-supervised learning\", \"multi-task learning\", \"few-shot learning\"\n",
    "]\n",
    "\n",
    "RESEARCH_METHODS = [\n",
    "    \"gradient descent\", \"backpropagation\", \"stochastic optimization\", \"cross-validation\",\n",
    "    \"ablation study\", \"hyperparameter tuning\", \"ensemble methods\", \"regularization\",\n",
    "    \"dropout\", \"batch normalization\", \"attention mechanism\", \"skip connections\",\n",
    "    \"data augmentation\", \"pre-training\", \"fine-tuning\", \"knowledge distillation\"\n",
    "]\n",
    "\n",
    "VENUES = [\n",
    "    \"NeurIPS\", \"ICML\", \"ICLR\", \"AAAI\", \"CVPR\", \"ACL\", \"EMNLP\", \"NAACL\",\n",
    "    \"ECCV\", \"ICCV\", \"KDD\", \"WWW\", \"SIGIR\", \"IJCAI\", \"UAI\", \"AISTATS\"\n",
    "]\n",
    "\n",
    "ABSTRACT_TEMPLATES = [\n",
    "    \"We propose {method}, a novel approach to {topic} that achieves state-of-the-art results on {benchmark}. \"\n",
    "    \"Our method leverages {technique} to address the challenge of {challenge}. \"\n",
    "    \"Experiments demonstrate {improvement}% improvement over previous baselines.\",\n",
    "    \n",
    "    \"This paper introduces {method} for {topic}. Unlike prior work that relies on {old_approach}, \"\n",
    "    \"we utilize {technique} to capture {aspect}. Our approach shows significant improvements in {metric}.\",\n",
    "    \n",
    "    \"Recent advances in {topic} have shown promising results using {technique}. \"\n",
    "    \"We address limitations by proposing {method}, which combines {component1} with {component2}. \"\n",
    "    \"Comprehensive experiments demonstrate the effectiveness of our approach.\",\n",
    "]\n",
    "\n",
    "TEMPLATE_FILLS = {\n",
    "    \"method\": [\"DeepNet\", \"TransNet\", \"GraphFormer\", \"AttnNet\", \"MultiScale\", \"HierNet\", \"AdaptNet\"],\n",
    "    \"benchmark\": [\"ImageNet\", \"COCO\", \"GLUE\", \"SQuAD\", \"WMT\", \"Citeseer\", \"PubMed\"],\n",
    "    \"challenge\": [\"scalability\", \"generalization\", \"data efficiency\", \"computational cost\"],\n",
    "    \"technique\": [\"self-attention\", \"graph convolution\", \"contrastive learning\", \"knowledge distillation\"],\n",
    "    \"improvement\": [\"15\", \"23\", \"8\", \"31\", \"12\", \"19\", \"27\"],\n",
    "    \"old_approach\": [\"hand-crafted features\", \"fixed architectures\", \"single-scale processing\"],\n",
    "    \"aspect\": [\"semantic relationships\", \"hierarchical structure\", \"temporal dynamics\"],\n",
    "    \"metric\": [\"accuracy\", \"F1 score\", \"BLEU score\", \"perplexity\"],\n",
    "    \"component1\": [\"local attention\", \"global context\", \"residual connections\"],\n",
    "    \"component2\": [\"positional encoding\", \"gating mechanisms\", \"skip connections\"],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_abstract(topics, methods):\n",
    "    \"\"\"Generate a synthetic abstract.\"\"\"\n",
    "    template = random.choice(ABSTRACT_TEMPLATES)\n",
    "    fills = {key: random.choice(values) for key, values in TEMPLATE_FILLS.items()}\n",
    "    fills[\"topic\"] = random.choice(topics)\n",
    "    abstract = template.format(**fills)\n",
    "    if random.random() > 0.5:\n",
    "        abstract += f\" We employ {random.choice(methods)} in our implementation.\"\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def generate_papers(num_papers=100):\n",
    "    \"\"\"Generate synthetic paper metadata.\"\"\"\n",
    "    papers = []\n",
    "    author_ids = list(CANONICAL_AUTHORS.keys())\n",
    "    base_date = datetime(2020, 1, 1)\n",
    "    \n",
    "    for i in range(num_papers):\n",
    "        paper_id = f\"paper_{i:04d}\"\n",
    "        num_authors = random.randint(1, 4)\n",
    "        selected_author_ids = random.sample(author_ids, num_authors)\n",
    "        \n",
    "        # Use name variations for authors\n",
    "        authors = []\n",
    "        for aid in selected_author_ids:\n",
    "            auth = CANONICAL_AUTHORS[aid]\n",
    "            if random.random() > 0.4:\n",
    "                authors.append(random.choice(auth[\"variations\"]))\n",
    "            else:\n",
    "                authors.append(auth[\"canonical_name\"])\n",
    "        \n",
    "        # Get institution with variations\n",
    "        primary_inst_id = CANONICAL_AUTHORS[selected_author_ids[0]][\"institution\"]\n",
    "        inst = CANONICAL_INSTITUTIONS[primary_inst_id]\n",
    "        institution = random.choice(inst[\"variations\"]) if random.random() > 0.5 else inst[\"canonical_name\"]\n",
    "        \n",
    "        paper_topics = random.sample(RESEARCH_TOPICS, random.randint(2, 4))\n",
    "        paper_methods = random.sample(RESEARCH_METHODS, random.randint(1, 3))\n",
    "        abstract = generate_abstract(paper_topics, paper_methods)\n",
    "        \n",
    "        method_name = random.choice(TEMPLATE_FILLS[\"method\"])\n",
    "        main_topic = paper_topics[0].title()\n",
    "        title_templates = [\n",
    "            f\"{method_name}: A Novel Approach to {main_topic}\",\n",
    "            f\"Improving {main_topic} with {method_name}\",\n",
    "            f\"{method_name} for Efficient {main_topic}\",\n",
    "        ]\n",
    "        title = random.choice(title_templates)\n",
    "        \n",
    "        venue = random.choice(VENUES)\n",
    "        pub_date = base_date + timedelta(days=random.randint(0, 1500))\n",
    "        \n",
    "        paper = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"institution\": institution,\n",
    "            \"abstract\": abstract,\n",
    "            \"keywords\": paper_topics,\n",
    "            \"venue\": venue,\n",
    "            \"year\": pub_date.year,\n",
    "            \"publication_date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "        \n",
    "        # Edge cases\n",
    "        if i == 5: paper[\"abstract\"] = \"\"\n",
    "        if i == 12: paper[\"keywords\"] = []\n",
    "        if i == 45: paper[\"institution\"] = None\n",
    "        if i == 67:\n",
    "            dup_author = selected_author_ids[0]\n",
    "            paper[\"authors\"].append(CANONICAL_AUTHORS[dup_author][\"variations\"][0])\n",
    "        \n",
    "        papers.append(paper)\n",
    "    \n",
    "    return papers\n",
    "\n",
    "\n",
    "def generate_citations(papers, density=0.05):\n",
    "    \"\"\"Generate citation relationships.\"\"\"\n",
    "    citations = []\n",
    "    paper_ids = [p[\"paper_id\"] for p in papers]\n",
    "    paper_years = {p[\"paper_id\"]: p[\"year\"] for p in papers}\n",
    "    \n",
    "    for citing_paper in paper_ids:\n",
    "        citing_year = paper_years[citing_paper]\n",
    "        citable = [p for p in paper_ids if paper_years[p] <= citing_year and p != citing_paper]\n",
    "        \n",
    "        if citable:\n",
    "            num_citations = max(1, int(len(citable) * density * random.uniform(0.5, 1.5)))\n",
    "            num_citations = min(num_citations, len(citable), 10)\n",
    "            cited_papers = random.sample(citable, num_citations)\n",
    "            for cited in cited_papers:\n",
    "                citations.append({\"citing_paper\": citing_paper, \"cited_paper\": cited})\n",
    "    \n",
    "    # Edge cases: orphan and self-citation\n",
    "    citations.append({\"citing_paper\": \"paper_0010\", \"cited_paper\": \"paper_9999\"})\n",
    "    citations.append({\"citing_paper\": \"paper_0015\", \"cited_paper\": \"paper_0015\"})\n",
    "    \n",
    "    return citations\n",
    "\n",
    "\n",
    "def generate_author_affiliations():\n",
    "    \"\"\"Generate author-institution mapping data.\"\"\"\n",
    "    affiliations = {\"authors\": {}, \"institutions\": {}}\n",
    "    \n",
    "    for auth_id, auth_data in CANONICAL_AUTHORS.items():\n",
    "        affiliations[\"authors\"][auth_id] = {\n",
    "            \"canonical_name\": auth_data[\"canonical_name\"],\n",
    "            \"known_variations\": auth_data[\"variations\"][:2],\n",
    "            \"primary_institution\": auth_data[\"institution\"],\n",
    "        }\n",
    "    \n",
    "    for inst_id, inst_data in CANONICAL_INSTITUTIONS.items():\n",
    "        affiliations[\"institutions\"][inst_id] = {\n",
    "            \"canonical_name\": inst_data[\"canonical_name\"],\n",
    "            \"known_variations\": inst_data[\"variations\"][:2],\n",
    "            \"country\": inst_data[\"country\"]\n",
    "        }\n",
    "    \n",
    "    return affiliations\n",
    "\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Generating synthetic dataset...\")\n",
    "papers_list = generate_papers(100)\n",
    "citations_list = generate_citations(papers_list)\n",
    "affiliations_data_gen = generate_author_affiliations()\n",
    "\n",
    "# Convert to the formats expected by the benchmark\n",
    "papers_raw = papers_list\n",
    "citations_raw = pd.DataFrame(citations_list)\n",
    "affiliations_raw = affiliations_data_gen\n",
    "\n",
    "print(f\"\\n✓ Dataset generated successfully:\")\n",
    "print(f\"  - Papers: {len(papers_raw)} records\")\n",
    "print(f\"  - Citations: {len(citations_raw)} relationships\")\n",
    "print(f\"  - Affiliations: {len(affiliations_raw.get('authors', {}))} authors, {len(affiliations_raw.get('institutions', {}))} institutions\")\n",
    "print(f\"\\nEdge cases included:\")\n",
    "print(f\"  - Paper with missing abstract (paper_0005)\")\n",
    "print(f\"  - Paper with missing keywords (paper_0012)\")\n",
    "print(f\"  - Paper with missing institution (paper_0045)\")\n",
    "print(f\"  - Paper with duplicate author entry (paper_0067)\")\n",
    "print(f\"  - Orphan citation (paper_9999 doesn't exist)\")\n",
    "print(f\"  - Self-citation (paper_0015 cites itself)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Prompt\n",
    "\n",
    "The task specification for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_PROMPT = \"\"\"\n",
    "# Research Paper Entity Extraction and Citation Analysis Benchmark\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are a data scientist tasked with building an automated pipeline for analyzing research paper metadata. \n",
    "Your goal is to extract structured information from a collection of research papers, resolve entity ambiguities, \n",
    "construct a citation network, and produce a comprehensive analytical report.\n",
    "\n",
    "You must decide for yourself how to decompose the task, which intermediate computations to perform, and in what order.\n",
    "Do not simply follow a fixed step-by-step structure.\n",
    "\n",
    "## Context\n",
    "\n",
    "You have access to three data sources (already loaded in memory):\n",
    "\n",
    "### Input Data Structures\n",
    "\n",
    "**papers_raw** (list[dict]): List of ~100 paper records. Each paper dict has this schema:\n",
    "- \"paper_id\": str (e.g., \"paper_0001\")\n",
    "- \"title\": str\n",
    "- \"authors\": list[str] (e.g., [\"J. Smith\", \"Maria Garcia\"])\n",
    "- \"institution\": str or None (e.g., \"MIT\" or \"Stanford University\")\n",
    "- \"abstract\": str (may be empty string \"\")\n",
    "- \"keywords\": list[str] (e.g., [\"machine learning\", \"neural networks\"], may be empty [])\n",
    "- \"venue\": str (e.g., \"NeurIPS\", \"ICML\")\n",
    "- \"year\": int\n",
    "- \"publication_date\": str (ISO format \"YYYY-MM-DD\")\n",
    "\n",
    "**citations_raw** (pd.DataFrame): Citation relationships with columns:\n",
    "- citing_paper: str (paper_id of the paper doing the citing)\n",
    "- cited_paper: str (paper_id of the paper being cited)\n",
    "\n",
    "**affiliations_raw** (dict): Reference data for entity resolution. \n",
    "IMPORTANT: Structure is a dict-of-dicts keyed by ID, NOT a list:\n",
    "{\n",
    "    \"authors\": {\n",
    "        \"auth_001\": {\"canonical_name\": str, \"known_variations\": list[str], \"primary_institution\": str},\n",
    "        \"auth_002\": {...},\n",
    "        ...\n",
    "    },\n",
    "    \"institutions\": {\n",
    "        \"inst_001\": {\"canonical_name\": str, \"known_variations\": list[str], \"country\": str},\n",
    "        \"inst_002\": {...},\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "To iterate over authors: for auth_id, auth_info in affiliations_raw[\"authors\"].items()\n",
    "To iterate over institutions: for inst_id, inst_info in affiliations_raw[\"institutions\"].items()\n",
    "\n",
    "### Data Challenges (Intentional)\n",
    "\n",
    "The data contains edge cases you must handle:\n",
    "- Author name variations: Same person appears as \"John Smith\", \"J. Smith\", \"Smith, John\"\n",
    "- Institution name variations: Same institution appears as \"MIT\", \"Massachusetts Institute of Technology\"\n",
    "- Missing fields: Some papers have empty abstract (\"\") or empty keywords ([])\n",
    "- Orphan citations: Some citations reference paper_ids that don't exist in papers_raw\n",
    "- Self-citations: Some papers cite themselves\n",
    "\n",
    "## Required Output Variables\n",
    "\n",
    "You must produce these variables:\n",
    "\n",
    "### Core Data Variables\n",
    "- papers_df: pd.DataFrame with columns: paper_id, title, authors, institution, abstract, keywords, venue, year, publication_date\n",
    "- citations_df: pd.DataFrame with columns: citing_paper, cited_paper\n",
    "- affiliations_data: dict with 'authors' and 'institutions' keys\n",
    "\n",
    "### Entity Extraction Variables\n",
    "- extracted_authors: list[dict] with keys: name, paper_ids, name_variations\n",
    "- extracted_institutions: list[dict] with keys: name, paper_ids, name_variations\n",
    "- extracted_topics: dict[str, int] mapping topics to frequency counts\n",
    "- methods_from_abstracts: list[str] of research methods found\n",
    "\n",
    "### Entity Resolution Variables\n",
    "- author_resolution_map: dict[str, str] mapping variations to canonical names\n",
    "- institution_resolution_map: dict[str, str] mapping variations to canonical names\n",
    "- resolved_author_count: int\n",
    "- resolved_institution_count: int\n",
    "\n",
    "### Citation Network Variables\n",
    "- citation_graph: dict[str, list[str]] adjacency list\n",
    "- in_degree: dict[str, int] incoming citations per paper\n",
    "- out_degree: dict[str, int] outgoing citations per paper\n",
    "- pagerank_scores: dict[str, float] PageRank scores\n",
    "- top_cited_papers: list[str] top 10 most cited paper IDs\n",
    "- orphan_citations: list[dict] citations to non-existent papers\n",
    "- self_citations: list[str] papers that cite themselves\n",
    "\n",
    "### Validation Dictionary\n",
    "- validation_results: dict[str, bool] with keys:\n",
    "  - papers_loaded_ok, citations_loaded_ok, affiliations_loaded_ok\n",
    "  - no_duplicate_paper_ids, authors_extracted, institutions_extracted\n",
    "  - resolution_maps_valid, citation_graph_built, pagerank_computed\n",
    "  - orphans_identified, self_citations_identified, all_pagerank_finite\n",
    "\n",
    "### Summary Statistics\n",
    "- summary_stats: dict with keys:\n",
    "  - total_papers, total_citations, unique_authors_raw, unique_authors_resolved\n",
    "  - unique_institutions_raw, unique_institutions_resolved\n",
    "  - papers_with_missing_abstract, papers_with_missing_keywords\n",
    "  - orphan_citation_count, self_citation_count, avg_citations_per_paper\n",
    "  - most_common_venue, year_range\n",
    "\n",
    "### Final Report\n",
    "- final_report: dict with this EXACT structure:\n",
    "{\n",
    "    \"metadata\": {\n",
    "        \"task\": \"Research Paper Entity Extraction and Citation Analysis\",\n",
    "        \"papers_analyzed\": int,\n",
    "        \"execution_timestamp\": str  # ISO format datetime\n",
    "    },\n",
    "    \"entity_extraction\": {\n",
    "        \"authors\": {\n",
    "            \"total_unique\": int,  # Number of unique resolved authors\n",
    "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]  # Top 5 authors by paper count\n",
    "        },\n",
    "        \"institutions\": {\n",
    "            \"total_unique\": int,\n",
    "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]\n",
    "        },\n",
    "        \"topics\": {\n",
    "            \"total_unique\": int,\n",
    "            \"top_10_by_frequency\": [{\"topic\": str, \"count\": int}, ...]  # Top 10 topics\n",
    "        }\n",
    "    },\n",
    "    \"citation_analysis\": {\n",
    "        \"total_citations\": int,\n",
    "        \"top_10_cited_papers\": [{\"paper_id\": str, \"citation_count\": int, \"title\": str}, ...],\n",
    "        \"orphan_citations\": [{\"citing_paper\": str, \"cited_paper\": str}, ...],\n",
    "        \"self_citations\": [str, ...],  # List of paper_ids\n",
    "        \"network_statistics\": {\n",
    "            \"avg_in_degree\": float,\n",
    "            \"avg_out_degree\": float,\n",
    "            \"max_in_degree\": int,\n",
    "            \"max_out_degree\": int\n",
    "        }\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_abstracts\": int,\n",
    "        \"missing_keywords\": int,\n",
    "        \"missing_institutions\": int,\n",
    "        \"duplicate_author_entries\": int\n",
    "    },\n",
    "    \"validation_summary\": {\n",
    "        \"all_checks_passed\": bool,\n",
    "        \"failed_checks\": [str, ...]  # List of failed validation key names\n",
    "    }\n",
    "}\n",
    "\n",
    "## Constraints\n",
    "1. Do not hardcode specific paper IDs, author names, or institution names\n",
    "2. Entity resolution must use fuzzy matching or reference data\n",
    "3. PageRank must use damping factor 0.85\n",
    "4. Handle edge cases gracefully\n",
    "\n",
    "## Success Criteria\n",
    "1. All validation checks pass\n",
    "2. Entity resolution reduces author count\n",
    "3. Orphan citations are identified (at least one exists)\n",
    "4. Self-citations are identified (at least one exists)\n",
    "5. PageRank scores sum to approximately 1.0\n",
    "6. Final report follows exact schema\n",
    "7. All numeric values are finite\n",
    "\n",
    "Write complete Python code to solve this task. Store all results in the specified variable names.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Benchmark prompt loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_task(prompt, data_context):\n",
    "    \"\"\"Run the agent using google.colab.ai to generate code for the task.\"\"\"\n",
    "    \n",
    "    # Prepare context with data samples\n",
    "    context = f\"\"\"\n",
    "You have access to the following data (already loaded in Python):\n",
    "\n",
    "papers_raw: A list of {len(data_context['papers'])} paper dictionaries\n",
    "Sample: {json.dumps(data_context['papers'][0], indent=2)}\n",
    "\n",
    "citations_raw: A pandas DataFrame with {len(data_context['citations'])} rows\n",
    "Columns: {data_context['citations'].columns.tolist()}\n",
    "Sample:\n",
    "{data_context['citations'].head(3).to_string()}\n",
    "\n",
    "affiliations_raw: A dictionary with author and institution reference data\n",
    "Keys: {list(data_context['affiliations'].keys())}\n",
    "Sample author: {json.dumps(list(data_context['affiliations']['authors'].values())[0], indent=2)}\n",
    "Sample institution: {json.dumps(list(data_context['affiliations']['institutions'].values())[0], indent=2)}\n",
    "\n",
    "{prompt}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Sending task to agent...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Use google.colab.ai to generate response\n",
    "    # The ai.generate_text function uses the Colab Pro's native AI capabilities\n",
    "    response = ai.generate_text(\n",
    "        prompt=context,\n",
    "        model_name=MODEL_NAME,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Prepare data context\n",
    "data_context = {\n",
    "    'papers': papers_raw,\n",
    "    'citations': citations_raw,\n",
    "    'affiliations': affiliations_raw\n",
    "}\n",
    "\n",
    "# Run the agent\n",
    "agent_response = run_agent_task(BENCHMARK_PROMPT, data_context)\n",
    "print(\"Agent response received\")\n",
    "print(\"=\"*50)\n",
    "print(agent_response[:2000] + \"...\" if len(agent_response) > 2000 else agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Python code from agent response and execute it\n",
    "def extract_and_execute_code(response_text):\n",
    "    \"\"\"Extract Python code blocks from the response and execute them.\"\"\"\n",
    "    \n",
    "    # Find all code blocks\n",
    "    code_blocks = re.findall(r'```python\\n(.*?)```', response_text, re.DOTALL)\n",
    "    \n",
    "    if not code_blocks:\n",
    "        # Try without language specifier\n",
    "        code_blocks = re.findall(r'```\\n(.*?)```', response_text, re.DOTALL)\n",
    "    \n",
    "    if not code_blocks:\n",
    "        print(\"No code blocks found in response\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all code blocks\n",
    "    full_code = \"\\n\\n\".join(code_blocks)\n",
    "    \n",
    "    print(f\"Extracted {len(code_blocks)} code block(s)\")\n",
    "    print(\"Executing agent code...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Execute the code\n",
    "    exec_globals = {\n",
    "        'papers_raw': papers_raw,\n",
    "        'citations_raw': citations_raw,\n",
    "        'affiliations_raw': affiliations_raw,\n",
    "        'pd': pd,\n",
    "        'np': np,\n",
    "        'json': json,\n",
    "        're': re,\n",
    "        'nx': nx,\n",
    "        'defaultdict': defaultdict,\n",
    "        'Counter': Counter,\n",
    "        'datetime': datetime,\n",
    "        'Dict': Dict,\n",
    "        'List': List,\n",
    "        'Any': Any,\n",
    "        'Tuple': Tuple,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        exec(full_code, exec_globals)\n",
    "        print(\"Code executed successfully!\")\n",
    "        return exec_globals\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing code: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute the agent's code\n",
    "exec_result = extract_and_execute_code(agent_response)\n",
    "\n",
    "# If successful, extract variables to global scope\n",
    "if exec_result:\n",
    "    required_vars = [\n",
    "        'papers_df', 'citations_df', 'affiliations_data',\n",
    "        'extracted_authors', 'extracted_institutions', 'extracted_topics', 'methods_from_abstracts',\n",
    "        'author_resolution_map', 'institution_resolution_map', 'resolved_author_count', 'resolved_institution_count',\n",
    "        'citation_graph', 'in_degree', 'out_degree', 'pagerank_scores', 'top_cited_papers',\n",
    "        'orphan_citations', 'self_citations',\n",
    "        'validation_results', 'summary_stats', 'final_report'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nVariable extraction:\")\n",
    "    for var in required_vars:\n",
    "        if var in exec_result:\n",
    "            globals()[var] = exec_result[var]\n",
    "            print(f\"  ✓ {var}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var} (missing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Output\n",
    "\n",
    "Display the results produced by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the agent's outputs\n",
    "try:\n",
    "    print(\"=== VALIDATION RESULTS ===\")\n",
    "    print(json.dumps(validation_results, indent=2))\n",
    "    print(\"\\n=== FINAL REPORT ===\")\n",
    "    print(json.dumps(final_report, indent=2, default=str))\n",
    "except NameError as e:\n",
    "    print(f\"Variable not defined: {e}\")\n",
    "    print(\"Agent may not have completed the task successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Unit Tests\n",
    "\n",
    "Comprehensive tests to validate the agent's solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataLoading(unittest.TestCase):\n",
    "    \"\"\"Tests for data loading functionality.\"\"\"\n",
    "    \n",
    "    def test_papers_df_exists_and_not_empty(self):\n",
    "        self.assertIsInstance(papers_df, pd.DataFrame)\n",
    "        self.assertGreater(len(papers_df), 0)\n",
    "    \n",
    "    def test_papers_df_has_required_columns(self):\n",
    "        required = {'paper_id', 'title', 'authors', 'institution', \n",
    "                   'abstract', 'keywords', 'venue', 'year', 'publication_date'}\n",
    "        self.assertTrue(required.issubset(set(papers_df.columns)))\n",
    "    \n",
    "    def test_citations_df_exists_and_not_empty(self):\n",
    "        self.assertIsInstance(citations_df, pd.DataFrame)\n",
    "        self.assertGreater(len(citations_df), 0)\n",
    "    \n",
    "    def test_citations_df_has_required_columns(self):\n",
    "        required = {'citing_paper', 'cited_paper'}\n",
    "        self.assertTrue(required.issubset(set(citations_df.columns)))\n",
    "    \n",
    "    def test_affiliations_data_structure(self):\n",
    "        self.assertIsInstance(affiliations_data, dict)\n",
    "        self.assertIn('authors', affiliations_data)\n",
    "        self.assertIn('institutions', affiliations_data)\n",
    "    \n",
    "    def test_no_duplicate_paper_ids(self):\n",
    "        self.assertEqual(papers_df['paper_id'].nunique(), len(papers_df))\n",
    "\n",
    "\n",
    "class TestEntityExtraction(unittest.TestCase):\n",
    "    \"\"\"Tests for entity extraction functionality.\"\"\"\n",
    "    \n",
    "    def test_extracted_authors_not_empty(self):\n",
    "        self.assertGreater(len(extracted_authors), 0)\n",
    "    \n",
    "    def test_extracted_authors_structure(self):\n",
    "        for author in extracted_authors:\n",
    "            self.assertIn('name', author)\n",
    "            self.assertIn('paper_ids', author)\n",
    "            self.assertIn('name_variations', author)\n",
    "    \n",
    "    def test_extracted_institutions_not_empty(self):\n",
    "        self.assertGreater(len(extracted_institutions), 0)\n",
    "    \n",
    "    def test_extracted_topics_is_dict(self):\n",
    "        self.assertIsInstance(extracted_topics, dict)\n",
    "    \n",
    "    def test_methods_from_abstracts_is_list(self):\n",
    "        self.assertIsInstance(methods_from_abstracts, list)\n",
    "\n",
    "\n",
    "class TestEntityResolution(unittest.TestCase):\n",
    "    \"\"\"Tests for entity resolution functionality.\"\"\"\n",
    "    \n",
    "    def test_author_resolution_map_not_empty(self):\n",
    "        self.assertGreater(len(author_resolution_map), 0)\n",
    "    \n",
    "    def test_institution_resolution_map_not_empty(self):\n",
    "        self.assertGreater(len(institution_resolution_map), 0)\n",
    "    \n",
    "    def test_resolved_counts_are_positive(self):\n",
    "        self.assertGreater(resolved_author_count, 0)\n",
    "        self.assertGreater(resolved_institution_count, 0)\n",
    "\n",
    "\n",
    "class TestCitationNetwork(unittest.TestCase):\n",
    "    \"\"\"Tests for citation network functionality.\"\"\"\n",
    "    \n",
    "    def test_citation_graph_not_empty(self):\n",
    "        self.assertGreater(len(citation_graph), 0)\n",
    "    \n",
    "    def test_pagerank_scores_not_empty(self):\n",
    "        self.assertGreater(len(pagerank_scores), 0)\n",
    "    \n",
    "    def test_pagerank_scores_sum_to_one(self):\n",
    "        total = sum(pagerank_scores.values())\n",
    "        self.assertAlmostEqual(total, 1.0, delta=0.01)\n",
    "    \n",
    "    def test_pagerank_scores_are_finite(self):\n",
    "        for score in pagerank_scores.values():\n",
    "            self.assertTrue(np.isfinite(score))\n",
    "    \n",
    "    def test_orphan_citations_identified(self):\n",
    "        self.assertIsInstance(orphan_citations, list)\n",
    "        self.assertGreater(len(orphan_citations), 0)\n",
    "    \n",
    "    def test_self_citations_identified(self):\n",
    "        self.assertIsInstance(self_citations, list)\n",
    "        self.assertGreater(len(self_citations), 0)\n",
    "\n",
    "\n",
    "class TestValidationResults(unittest.TestCase):\n",
    "    \"\"\"Tests for validation results.\"\"\"\n",
    "    \n",
    "    def test_validation_results_is_dict(self):\n",
    "        self.assertIsInstance(validation_results, dict)\n",
    "    \n",
    "    def test_validation_results_has_required_keys(self):\n",
    "        required_keys = {\n",
    "            \"papers_loaded_ok\", \"citations_loaded_ok\", \"affiliations_loaded_ok\",\n",
    "            \"no_duplicate_paper_ids\", \"authors_extracted\", \"institutions_extracted\",\n",
    "            \"resolution_maps_valid\", \"citation_graph_built\", \"pagerank_computed\",\n",
    "            \"orphans_identified\", \"self_citations_identified\", \"all_pagerank_finite\"\n",
    "        }\n",
    "        self.assertTrue(required_keys.issubset(set(validation_results.keys())))\n",
    "    \n",
    "    def test_all_validations_pass(self):\n",
    "        failed = [k for k, v in validation_results.items() if not v]\n",
    "        self.assertEqual(len(failed), 0, f\"Failed validations: {failed}\")\n",
    "\n",
    "\n",
    "class TestSummaryStats(unittest.TestCase):\n",
    "    \"\"\"Tests for summary statistics.\"\"\"\n",
    "    \n",
    "    def test_summary_stats_is_dict(self):\n",
    "        self.assertIsInstance(summary_stats, dict)\n",
    "    \n",
    "    def test_summary_stats_has_required_keys(self):\n",
    "        required_keys = {\n",
    "            \"total_papers\", \"total_citations\", \"unique_authors_raw\",\n",
    "            \"unique_authors_resolved\", \"unique_institutions_raw\",\n",
    "            \"unique_institutions_resolved\", \"papers_with_missing_abstract\",\n",
    "            \"papers_with_missing_keywords\", \"orphan_citation_count\",\n",
    "            \"self_citation_count\", \"avg_citations_per_paper\",\n",
    "            \"most_common_venue\", \"year_range\"\n",
    "        }\n",
    "        self.assertTrue(required_keys.issubset(set(summary_stats.keys())))\n",
    "\n",
    "\n",
    "class TestFinalReport(unittest.TestCase):\n",
    "    \"\"\"Tests for final report structure.\"\"\"\n",
    "    \n",
    "    def test_final_report_is_dict(self):\n",
    "        self.assertIsInstance(final_report, dict)\n",
    "    \n",
    "    def test_final_report_has_metadata(self):\n",
    "        self.assertIn('metadata', final_report)\n",
    "    \n",
    "    def test_final_report_has_entity_extraction(self):\n",
    "        self.assertIn('entity_extraction', final_report)\n",
    "    \n",
    "    def test_final_report_has_citation_analysis(self):\n",
    "        self.assertIn('citation_analysis', final_report)\n",
    "    \n",
    "    def test_final_report_has_data_quality(self):\n",
    "        self.assertIn('data_quality', final_report)\n",
    "    \n",
    "    def test_final_report_has_validation_summary(self):\n",
    "        self.assertIn('validation_summary', final_report)\n",
    "    \n",
    "    def test_all_checks_passed(self):\n",
    "        self.assertTrue(final_report['validation_summary']['all_checks_passed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all unit tests\n",
    "def run_tests():\n",
    "    \"\"\"Run all unit tests and report results.\"\"\"\n",
    "    loader = unittest.TestLoader()\n",
    "    suite = unittest.TestSuite()\n",
    "    \n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDataLoading))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestEntityExtraction))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestEntityResolution))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestCitationNetwork))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestValidationResults))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestSummaryStats))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestFinalReport))\n",
    "    \n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    print(f\"Success: {result.wasSuccessful()}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Execute tests\n",
    "try:\n",
    "    test_result = run_tests()\n",
    "except Exception as e:\n",
    "    print(f\"Error running tests: {e}\")\n",
    "    print(\"Some required variables may not be defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nAgent Model: {MODEL_NAME}\")\n",
    "    print(f\"Papers Analyzed: {len(papers_df)}\")\n",
    "    print(f\"Citations Processed: {len(citations_df)}\")\n",
    "    print(f\"\\nEntity Resolution:\")\n",
    "    print(f\"  Authors: {summary_stats.get('unique_authors_raw', 'N/A')} raw -> {resolved_author_count} resolved\")\n",
    "    print(f\"  Institutions: {summary_stats.get('unique_institutions_raw', 'N/A')} raw -> {resolved_institution_count} resolved\")\n",
    "    print(f\"\\nCitation Network:\")\n",
    "    print(f\"  Orphan citations found: {len(orphan_citations)}\")\n",
    "    print(f\"  Self-citations found: {len(self_citations)}\")\n",
    "    print(f\"  PageRank sum: {sum(pagerank_scores.values()):.4f}\")\n",
    "    print(f\"\\nValidation Summary:\")\n",
    "    failed = [k for k, v in validation_results.items() if not v]\n",
    "    if failed:\n",
    "        print(f\"  FAILED checks: {failed}\")\n",
    "    else:\n",
    "        print(\"  ALL CHECKS PASSED ✓\")\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Tests run: {test_result.testsRun}\")\n",
    "    print(f\"  Failures: {len(test_result.failures)}\")\n",
    "    print(f\"  Errors: {len(test_result.errors)}\")\n",
    "    \n",
    "    if test_result.wasSuccessful() and not failed:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ BENCHMARK COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✗ BENCHMARK COMPLETED WITH ISSUES\")\n",
    "        print(\"=\"*60)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError generating summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
