{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Colab: Research Paper Entity Extraction Benchmark\n",
        "\n",
        "This notebook sets up **Gemini 3 Pro Preview** as an autonomous agent to solve the Research Paper Entity Extraction and Citation Analysis benchmark.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab Pro (for native Gemini access via `google.colab.ai`)\n",
        "\n",
        "**Model Used:**\n",
        "- `google/gemini-3-pro-preview` - Gemini 3 Pro Preview model\n",
        "\n",
        "**Implementation:**\n",
        "- Uses `google.colab.ai` module for native Colab Pro AI integration\n",
        "- No external API keys required - uses Colab Pro's built-in AI capabilities\n",
        "- Self-contained dataset generation (no file uploads needed)\n",
        "\n",
        "**Note:** This notebook runs end-to-end without manual intervention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q pandas networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available AI models in Colab Pro:\n"
          ]
        },
        {
          "ename": "TimeoutException",
          "evalue": "Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1270679202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# List available models in Colab Pro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available AI models in Colab Pro:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mavailable_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36mlist_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_proxy_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     response = _requests.get(\n\u001b[1;32m    132\u001b[0m         \u001b[0;34mf'{_get_model_proxy_host()}/models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36m_get_model_proxy_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mmodel_proxy_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_userdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PROXY_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_proxy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret MODEL_PROXY_API_KEY timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import re\n",
        "import networkx as nx\n",
        "import warnings\n",
        "import unittest\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# List available models in Colab Pro\n",
        "print(\"Available AI models in Colab Pro:\")\n",
        "available_models = ai.list_models()\n",
        "for model in available_models:\n",
        "    print(f\"  - {model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Configuration\n",
        "\n",
        "Select and configure Gemini-3-Pro from available Colab Pro models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent model initialized: gemini-3-pro\n"
          ]
        }
      ],
      "source": [
        "# Select the model for agentic tasks\n",
        "# Using Gemini 3 Pro Preview - the most advanced reasoning model available\n",
        "MODEL_NAME = \"google/gemini-3-pro-preview\"\n",
        "\n",
        "# Verify the model is available\n",
        "if MODEL_NAME in available_models:\n",
        "    print(f\"Model '{MODEL_NAME}' is available - SELECTED\")\n",
        "else:\n",
        "    print(f\"Warning: '{MODEL_NAME}' not found. Available models: {available_models}\")\n",
        "    # Fallback to other Pro/capable models\n",
        "    fallback_order = [\"google/gemini-2.5-pro\", \"google/gemini-2.0-flash\", \"google/gemini-2.5-flash\"]\n",
        "    for fallback in fallback_order:\n",
        "        if fallback in available_models:\n",
        "            MODEL_NAME = fallback\n",
        "            print(f\"Using fallback model: {MODEL_NAME}\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nAgent model selected: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Dataset\n",
        "\n",
        "Generate the synthetic benchmark dataset. This ensures the notebook is fully self-contained and reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'papers_metadata.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2805805738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAPERS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpapers_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'papers_metadata.json'"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATASET GENERATION - Self-contained synthetic data generation\n",
        "# ============================================================================\n",
        "\n",
        "import random\n",
        "import csv\n",
        "from datetime import timedelta\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Canonical authors with their name variations\n",
        "CANONICAL_AUTHORS = {\n",
        "    \"auth_001\": {\n",
        "        \"canonical_name\": \"John Smith\",\n",
        "        \"variations\": [\"J. Smith\", \"John A. Smith\", \"J. A. Smith\", \"Smith, John\"],\n",
        "        \"institution\": \"inst_001\"\n",
        "    },\n",
        "    \"auth_002\": {\n",
        "        \"canonical_name\": \"Maria Garcia\",\n",
        "        \"variations\": [\"M. Garcia\", \"Maria L. Garcia\", \"Garcia, Maria\", \"M. L. Garcia\"],\n",
        "        \"institution\": \"inst_002\"\n",
        "    },\n",
        "    \"auth_003\": {\n",
        "        \"canonical_name\": \"Wei Zhang\",\n",
        "        \"variations\": [\"W. Zhang\", \"Wei W. Zhang\", \"Zhang, Wei\", \"Zhang Wei\"],\n",
        "        \"institution\": \"inst_003\"\n",
        "    },\n",
        "    \"auth_004\": {\n",
        "        \"canonical_name\": \"Emily Johnson\",\n",
        "        \"variations\": [\"E. Johnson\", \"Emily R. Johnson\", \"Johnson, Emily\", \"E. R. Johnson\"],\n",
        "        \"institution\": \"inst_001\"\n",
        "    },\n",
        "    \"auth_005\": {\n",
        "        \"canonical_name\": \"Ahmed Hassan\",\n",
        "        \"variations\": [\"A. Hassan\", \"Ahmed M. Hassan\", \"Hassan, Ahmed\", \"A. M. Hassan\"],\n",
        "        \"institution\": \"inst_004\"\n",
        "    },\n",
        "    \"auth_006\": {\n",
        "        \"canonical_name\": \"Sarah Williams\",\n",
        "        \"variations\": [\"S. Williams\", \"Sarah K. Williams\", \"Williams, Sarah\", \"S. K. Williams\"],\n",
        "        \"institution\": \"inst_002\"\n",
        "    },\n",
        "    \"auth_007\": {\n",
        "        \"canonical_name\": \"Yuki Tanaka\",\n",
        "        \"variations\": [\"Y. Tanaka\", \"Yuki S. Tanaka\", \"Tanaka, Yuki\", \"Tanaka Yuki\"],\n",
        "        \"institution\": \"inst_005\"\n",
        "    },\n",
        "    \"auth_008\": {\n",
        "        \"canonical_name\": \"Michael Brown\",\n",
        "        \"variations\": [\"M. Brown\", \"Michael J. Brown\", \"Brown, Michael\", \"M. J. Brown\"],\n",
        "        \"institution\": \"inst_003\"\n",
        "    },\n",
        "    \"auth_009\": {\n",
        "        \"canonical_name\": \"Lisa Chen\",\n",
        "        \"variations\": [\"L. Chen\", \"Lisa Y. Chen\", \"Chen, Lisa\", \"Chen Lisa\"],\n",
        "        \"institution\": \"inst_004\"\n",
        "    },\n",
        "    \"auth_010\": {\n",
        "        \"canonical_name\": \"David Miller\",\n",
        "        \"variations\": [\"D. Miller\", \"David A. Miller\", \"Miller, David\", \"D. A. Miller\"],\n",
        "        \"institution\": \"inst_005\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Canonical institutions with their variations\n",
        "CANONICAL_INSTITUTIONS = {\n",
        "    \"inst_001\": {\n",
        "        \"canonical_name\": \"Massachusetts Institute of Technology\",\n",
        "        \"variations\": [\"MIT\", \"M.I.T.\", \"Massachusetts Inst. of Technology\", \"Mass. Institute of Technology\"],\n",
        "        \"country\": \"USA\"\n",
        "    },\n",
        "    \"inst_002\": {\n",
        "        \"canonical_name\": \"Stanford University\",\n",
        "        \"variations\": [\"Stanford\", \"Stanford Univ.\", \"Stanford U.\", \"Leland Stanford Junior University\"],\n",
        "        \"country\": \"USA\"\n",
        "    },\n",
        "    \"inst_003\": {\n",
        "        \"canonical_name\": \"Tsinghua University\",\n",
        "        \"variations\": [\"Tsinghua\", \"Tsinghua Univ.\", \"Qinghua University\", \"THU\"],\n",
        "        \"country\": \"China\"\n",
        "    },\n",
        "    \"inst_004\": {\n",
        "        \"canonical_name\": \"University of Oxford\",\n",
        "        \"variations\": [\"Oxford\", \"Oxford Univ.\", \"Oxford University\", \"Univ. of Oxford\"],\n",
        "        \"country\": \"UK\"\n",
        "    },\n",
        "    \"inst_005\": {\n",
        "        \"canonical_name\": \"University of Tokyo\",\n",
        "        \"variations\": [\"Tokyo Univ.\", \"UTokyo\", \"Tokyo University\", \"Univ. of Tokyo\"],\n",
        "        \"country\": \"Japan\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Research topics and methods\n",
        "RESEARCH_TOPICS = [\n",
        "    \"machine learning\", \"deep learning\", \"neural networks\", \"natural language processing\",\n",
        "    \"computer vision\", \"reinforcement learning\", \"transformer models\", \"attention mechanisms\",\n",
        "    \"graph neural networks\", \"federated learning\", \"transfer learning\", \"meta-learning\",\n",
        "    \"generative models\", \"adversarial learning\", \"explainable AI\", \"optimization\",\n",
        "    \"representation learning\", \"self-supervised learning\", \"multi-task learning\", \"few-shot learning\"\n",
        "]\n",
        "\n",
        "RESEARCH_METHODS = [\n",
        "    \"gradient descent\", \"backpropagation\", \"stochastic optimization\", \"cross-validation\",\n",
        "    \"ablation study\", \"hyperparameter tuning\", \"ensemble methods\", \"regularization\",\n",
        "    \"dropout\", \"batch normalization\", \"attention mechanism\", \"skip connections\",\n",
        "    \"data augmentation\", \"pre-training\", \"fine-tuning\", \"knowledge distillation\"\n",
        "]\n",
        "\n",
        "VENUES = [\n",
        "    \"NeurIPS\", \"ICML\", \"ICLR\", \"AAAI\", \"CVPR\", \"ACL\", \"EMNLP\", \"NAACL\",\n",
        "    \"ECCV\", \"ICCV\", \"KDD\", \"WWW\", \"SIGIR\", \"IJCAI\", \"UAI\", \"AISTATS\"\n",
        "]\n",
        "\n",
        "ABSTRACT_TEMPLATES = [\n",
        "    \"We propose {method}, a novel approach to {topic} that achieves state-of-the-art results on {benchmark}. \"\n",
        "    \"Our method leverages {technique} to address the challenge of {challenge}. \"\n",
        "    \"Experiments demonstrate {improvement}% improvement over previous baselines.\",\n",
        "    \n",
        "    \"This paper introduces {method} for {topic}. Unlike prior work that relies on {old_approach}, \"\n",
        "    \"we utilize {technique} to capture {aspect}. Our approach shows significant improvements in {metric}.\",\n",
        "    \n",
        "    \"Recent advances in {topic} have shown promising results using {technique}. \"\n",
        "    \"We address limitations by proposing {method}, which combines {component1} with {component2}. \"\n",
        "    \"Comprehensive experiments demonstrate the effectiveness of our approach.\",\n",
        "]\n",
        "\n",
        "TEMPLATE_FILLS = {\n",
        "    \"method\": [\"DeepNet\", \"TransNet\", \"GraphFormer\", \"AttnNet\", \"MultiScale\", \"HierNet\", \"AdaptNet\"],\n",
        "    \"benchmark\": [\"ImageNet\", \"COCO\", \"GLUE\", \"SQuAD\", \"WMT\", \"Citeseer\", \"PubMed\"],\n",
        "    \"challenge\": [\"scalability\", \"generalization\", \"data efficiency\", \"computational cost\"],\n",
        "    \"technique\": [\"self-attention\", \"graph convolution\", \"contrastive learning\", \"knowledge distillation\"],\n",
        "    \"improvement\": [\"15\", \"23\", \"8\", \"31\", \"12\", \"19\", \"27\"],\n",
        "    \"old_approach\": [\"hand-crafted features\", \"fixed architectures\", \"single-scale processing\"],\n",
        "    \"aspect\": [\"semantic relationships\", \"hierarchical structure\", \"temporal dynamics\"],\n",
        "    \"metric\": [\"accuracy\", \"F1 score\", \"BLEU score\", \"perplexity\"],\n",
        "    \"component1\": [\"local attention\", \"global context\", \"residual connections\"],\n",
        "    \"component2\": [\"positional encoding\", \"gating mechanisms\", \"skip connections\"],\n",
        "}\n",
        "\n",
        "\n",
        "def generate_abstract(topics, methods):\n",
        "    \"\"\"Generate a synthetic abstract.\"\"\"\n",
        "    template = random.choice(ABSTRACT_TEMPLATES)\n",
        "    fills = {key: random.choice(values) for key, values in TEMPLATE_FILLS.items()}\n",
        "    fills[\"topic\"] = random.choice(topics)\n",
        "    abstract = template.format(**fills)\n",
        "    if random.random() > 0.5:\n",
        "        abstract += f\" We employ {random.choice(methods)} in our implementation.\"\n",
        "    return abstract\n",
        "\n",
        "\n",
        "def generate_papers(num_papers=100):\n",
        "    \"\"\"Generate synthetic paper metadata.\"\"\"\n",
        "    papers = []\n",
        "    author_ids = list(CANONICAL_AUTHORS.keys())\n",
        "    base_date = datetime(2020, 1, 1)\n",
        "    \n",
        "    for i in range(num_papers):\n",
        "        paper_id = f\"paper_{i:04d}\"\n",
        "        num_authors = random.randint(1, 4)\n",
        "        selected_author_ids = random.sample(author_ids, num_authors)\n",
        "        \n",
        "        # Use name variations for authors\n",
        "        authors = []\n",
        "        for aid in selected_author_ids:\n",
        "            auth = CANONICAL_AUTHORS[aid]\n",
        "            if random.random() > 0.4:\n",
        "                authors.append(random.choice(auth[\"variations\"]))\n",
        "            else:\n",
        "                authors.append(auth[\"canonical_name\"])\n",
        "        \n",
        "        # Get institution with variations\n",
        "        primary_inst_id = CANONICAL_AUTHORS[selected_author_ids[0]][\"institution\"]\n",
        "        inst = CANONICAL_INSTITUTIONS[primary_inst_id]\n",
        "        institution = random.choice(inst[\"variations\"]) if random.random() > 0.5 else inst[\"canonical_name\"]\n",
        "        \n",
        "        paper_topics = random.sample(RESEARCH_TOPICS, random.randint(2, 4))\n",
        "        paper_methods = random.sample(RESEARCH_METHODS, random.randint(1, 3))\n",
        "        abstract = generate_abstract(paper_topics, paper_methods)\n",
        "        \n",
        "        method_name = random.choice(TEMPLATE_FILLS[\"method\"])\n",
        "        main_topic = paper_topics[0].title()\n",
        "        title_templates = [\n",
        "            f\"{method_name}: A Novel Approach to {main_topic}\",\n",
        "            f\"Improving {main_topic} with {method_name}\",\n",
        "            f\"{method_name} for Efficient {main_topic}\",\n",
        "        ]\n",
        "        title = random.choice(title_templates)\n",
        "        \n",
        "        venue = random.choice(VENUES)\n",
        "        pub_date = base_date + timedelta(days=random.randint(0, 1500))\n",
        "        \n",
        "        paper = {\n",
        "            \"paper_id\": paper_id,\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"institution\": institution,\n",
        "            \"abstract\": abstract,\n",
        "            \"keywords\": paper_topics,\n",
        "            \"venue\": venue,\n",
        "            \"year\": pub_date.year,\n",
        "            \"publication_date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
        "        }\n",
        "        \n",
        "        # Edge cases\n",
        "        if i == 5: paper[\"abstract\"] = \"\"\n",
        "        if i == 12: paper[\"keywords\"] = []\n",
        "        if i == 45: paper[\"institution\"] = None\n",
        "        if i == 67:\n",
        "            dup_author = selected_author_ids[0]\n",
        "            paper[\"authors\"].append(CANONICAL_AUTHORS[dup_author][\"variations\"][0])\n",
        "        \n",
        "        papers.append(paper)\n",
        "    \n",
        "    return papers\n",
        "\n",
        "\n",
        "def generate_citations(papers, density=0.05):\n",
        "    \"\"\"Generate citation relationships.\"\"\"\n",
        "    citations = []\n",
        "    paper_ids = [p[\"paper_id\"] for p in papers]\n",
        "    paper_years = {p[\"paper_id\"]: p[\"year\"] for p in papers}\n",
        "    \n",
        "    for citing_paper in paper_ids:\n",
        "        citing_year = paper_years[citing_paper]\n",
        "        citable = [p for p in paper_ids if paper_years[p] <= citing_year and p != citing_paper]\n",
        "        \n",
        "        if citable:\n",
        "            num_citations = max(1, int(len(citable) * density * random.uniform(0.5, 1.5)))\n",
        "            num_citations = min(num_citations, len(citable), 10)\n",
        "            cited_papers = random.sample(citable, num_citations)\n",
        "            for cited in cited_papers:\n",
        "                citations.append({\"citing_paper\": citing_paper, \"cited_paper\": cited})\n",
        "    \n",
        "    # Edge cases: orphan and self-citation\n",
        "    citations.append({\"citing_paper\": \"paper_0010\", \"cited_paper\": \"paper_9999\"})\n",
        "    citations.append({\"citing_paper\": \"paper_0015\", \"cited_paper\": \"paper_0015\"})\n",
        "    \n",
        "    return citations\n",
        "\n",
        "\n",
        "def generate_author_affiliations():\n",
        "    \"\"\"Generate author-institution mapping data.\"\"\"\n",
        "    affiliations = {\"authors\": {}, \"institutions\": {}}\n",
        "    \n",
        "    for auth_id, auth_data in CANONICAL_AUTHORS.items():\n",
        "        affiliations[\"authors\"][auth_id] = {\n",
        "            \"canonical_name\": auth_data[\"canonical_name\"],\n",
        "            \"known_variations\": auth_data[\"variations\"][:2],\n",
        "            \"primary_institution\": auth_data[\"institution\"],\n",
        "        }\n",
        "    \n",
        "    for inst_id, inst_data in CANONICAL_INSTITUTIONS.items():\n",
        "        affiliations[\"institutions\"][inst_id] = {\n",
        "            \"canonical_name\": inst_data[\"canonical_name\"],\n",
        "            \"known_variations\": inst_data[\"variations\"][:2],\n",
        "            \"country\": inst_data[\"country\"]\n",
        "        }\n",
        "    \n",
        "    return affiliations\n",
        "\n",
        "\n",
        "# Generate the dataset\n",
        "print(\"Generating synthetic dataset...\")\n",
        "papers_list = generate_papers(100)\n",
        "citations_list = generate_citations(papers_list)\n",
        "affiliations_data_gen = generate_author_affiliations()\n",
        "\n",
        "# Convert to the formats expected by the benchmark\n",
        "papers_raw = papers_list\n",
        "citations_raw = pd.DataFrame(citations_list)\n",
        "affiliations_raw = affiliations_data_gen\n",
        "\n",
        "print(f\"\\nâœ“ Dataset generated successfully:\")\n",
        "print(f\"  - Papers: {len(papers_raw)} records\")\n",
        "print(f\"  - Citations: {len(citations_raw)} relationships\")\n",
        "print(f\"  - Affiliations: {len(affiliations_raw.get('authors', {}))} authors, {len(affiliations_raw.get('institutions', {}))} institutions\")\n",
        "print(f\"\\nEdge cases included:\")\n",
        "print(f\"  - Paper with missing abstract (paper_0005)\")\n",
        "print(f\"  - Paper with missing keywords (paper_0012)\")\n",
        "print(f\"  - Paper with missing institution (paper_0045)\")\n",
        "print(f\"  - Paper with duplicate author entry (paper_0067)\")\n",
        "print(f\"  - Orphan citation (paper_9999 doesn't exist)\")\n",
        "print(f\"  - Self-citation (paper_0015 cites itself)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Prompt\n",
        "\n",
        "The task specification for the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BENCHMARK_PROMPT = \"\"\"\n",
        "# Research Paper Entity Extraction and Citation Analysis Benchmark\n",
        "\n",
        "## Scenario\n",
        "\n",
        "You are a data scientist tasked with building an automated pipeline for analyzing research paper metadata. \n",
        "Your goal is to extract structured information from a collection of research papers, resolve entity ambiguities, \n",
        "construct a citation network, and produce a comprehensive analytical report.\n",
        "\n",
        "You must decide for yourself how to decompose the task, which intermediate computations to perform, and in what order.\n",
        "Do not simply follow a fixed step-by-step structure.\n",
        "\n",
        "## Context\n",
        "\n",
        "You have access to three data sources (already loaded in memory):\n",
        "\n",
        "### Input Data Structures\n",
        "\n",
        "**papers_raw** (list[dict]): List of ~100 paper records. Each paper dict has this schema:\n",
        "- \"paper_id\": str (e.g., \"paper_0001\")\n",
        "- \"title\": str\n",
        "- \"authors\": list[str] (e.g., [\"J. Smith\", \"Maria Garcia\"])\n",
        "- \"institution\": str or None (e.g., \"MIT\" or \"Stanford University\")\n",
        "- \"abstract\": str (may be empty string \"\")\n",
        "- \"keywords\": list[str] (e.g., [\"machine learning\", \"neural networks\"], may be empty [])\n",
        "- \"venue\": str (e.g., \"NeurIPS\", \"ICML\")\n",
        "- \"year\": int\n",
        "- \"publication_date\": str (ISO format \"YYYY-MM-DD\")\n",
        "\n",
        "**citations_raw** (pd.DataFrame): Citation relationships with columns:\n",
        "- citing_paper: str (paper_id of the paper doing the citing)\n",
        "- cited_paper: str (paper_id of the paper being cited)\n",
        "\n",
        "**affiliations_raw** (dict): Reference data for entity resolution. \n",
        "IMPORTANT: Structure is a dict-of-dicts keyed by ID, NOT a list:\n",
        "{\n",
        "    \"authors\": {\n",
        "        \"auth_001\": {\"canonical_name\": str, \"known_variations\": list[str], \"primary_institution\": str},\n",
        "        \"auth_002\": {...},\n",
        "        ...\n",
        "    },\n",
        "    \"institutions\": {\n",
        "        \"inst_001\": {\"canonical_name\": str, \"known_variations\": list[str], \"country\": str},\n",
        "        \"inst_002\": {...},\n",
        "        ...\n",
        "    }\n",
        "}\n",
        "\n",
        "To iterate over authors: for auth_id, auth_info in affiliations_raw[\"authors\"].items()\n",
        "To iterate over institutions: for inst_id, inst_info in affiliations_raw[\"institutions\"].items()\n",
        "\n",
        "### Data Challenges (Intentional)\n",
        "\n",
        "The data contains edge cases you must handle:\n",
        "- Author name variations: Same person appears as \"John Smith\", \"J. Smith\", \"Smith, John\"\n",
        "- Institution name variations: Same institution appears as \"MIT\", \"Massachusetts Institute of Technology\"\n",
        "- Missing fields: Some papers have empty abstract (\"\") or empty keywords ([])\n",
        "- Orphan citations: Some citations reference paper_ids that don't exist in papers_raw\n",
        "- Self-citations: Some papers cite themselves\n",
        "\n",
        "## Required Output Variables\n",
        "\n",
        "You must produce these variables:\n",
        "\n",
        "### Core Data Variables\n",
        "- papers_df: pd.DataFrame with columns: paper_id, title, authors, institution, abstract, keywords, venue, year, publication_date\n",
        "- citations_df: pd.DataFrame with columns: citing_paper, cited_paper\n",
        "- affiliations_data: dict with 'authors' and 'institutions' keys\n",
        "\n",
        "### Entity Extraction Variables\n",
        "- extracted_authors: list[dict] with keys: name, paper_ids, name_variations\n",
        "- extracted_institutions: list[dict] with keys: name, paper_ids, name_variations\n",
        "- extracted_topics: dict[str, int] mapping topics to frequency counts\n",
        "- methods_from_abstracts: list[str] of research methods found\n",
        "\n",
        "### Entity Resolution Variables\n",
        "- author_resolution_map: dict[str, str] mapping variations to canonical names\n",
        "- institution_resolution_map: dict[str, str] mapping variations to canonical names\n",
        "- resolved_author_count: int\n",
        "- resolved_institution_count: int\n",
        "\n",
        "### Citation Network Variables\n",
        "- citation_graph: dict[str, list[str]] adjacency list\n",
        "- in_degree: dict[str, int] incoming citations per paper\n",
        "- out_degree: dict[str, int] outgoing citations per paper\n",
        "- pagerank_scores: dict[str, float] PageRank scores\n",
        "- top_cited_papers: list[str] top 10 most cited paper IDs\n",
        "- orphan_citations: list[dict] citations to non-existent papers\n",
        "- self_citations: list[str] papers that cite themselves\n",
        "\n",
        "### Validation Dictionary\n",
        "- validation_results: dict[str, bool] with keys:\n",
        "  - papers_loaded_ok, citations_loaded_ok, affiliations_loaded_ok\n",
        "  - no_duplicate_paper_ids, authors_extracted, institutions_extracted\n",
        "  - resolution_maps_valid, citation_graph_built, pagerank_computed\n",
        "  - orphans_identified, self_citations_identified, all_pagerank_finite\n",
        "\n",
        "### Summary Statistics\n",
        "- summary_stats: dict with keys:\n",
        "  - total_papers, total_citations, unique_authors_raw, unique_authors_resolved\n",
        "  - unique_institutions_raw, unique_institutions_resolved\n",
        "  - papers_with_missing_abstract, papers_with_missing_keywords\n",
        "  - orphan_citation_count, self_citation_count, avg_citations_per_paper\n",
        "  - most_common_venue, year_range\n",
        "\n",
        "### Final Report\n",
        "- final_report: dict with this EXACT structure:\n",
        "{\n",
        "    \"metadata\": {\n",
        "        \"task\": \"Research Paper Entity Extraction and Citation Analysis\",\n",
        "        \"papers_analyzed\": int,\n",
        "        \"execution_timestamp\": str  # ISO format datetime\n",
        "    },\n",
        "    \"entity_extraction\": {\n",
        "        \"authors\": {\n",
        "            \"total_unique\": int,  # Number of unique resolved authors\n",
        "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]  # Top 5 authors by paper count\n",
        "        },\n",
        "        \"institutions\": {\n",
        "            \"total_unique\": int,\n",
        "            \"top_5_by_paper_count\": [{\"name\": str, \"paper_count\": int}, ...]\n",
        "        },\n",
        "        \"topics\": {\n",
        "            \"total_unique\": int,\n",
        "            \"top_10_by_frequency\": [{\"topic\": str, \"count\": int}, ...]  # Top 10 topics\n",
        "        }\n",
        "    },\n",
        "    \"citation_analysis\": {\n",
        "        \"total_citations\": int,\n",
        "        \"top_10_cited_papers\": [{\"paper_id\": str, \"citation_count\": int, \"title\": str}, ...],\n",
        "        \"orphan_citations\": [{\"citing_paper\": str, \"cited_paper\": str}, ...],\n",
        "        \"self_citations\": [str, ...],  # List of paper_ids\n",
        "        \"network_statistics\": {\n",
        "            \"avg_in_degree\": float,\n",
        "            \"avg_out_degree\": float,\n",
        "            \"max_in_degree\": int,\n",
        "            \"max_out_degree\": int\n",
        "        }\n",
        "    },\n",
        "    \"data_quality\": {\n",
        "        \"missing_abstracts\": int,\n",
        "        \"missing_keywords\": int,\n",
        "        \"missing_institutions\": int,\n",
        "        \"duplicate_author_entries\": int\n",
        "    },\n",
        "    \"validation_summary\": {\n",
        "        \"all_checks_passed\": bool,\n",
        "        \"failed_checks\": [str, ...]  # List of failed validation key names\n",
        "    }\n",
        "}\n",
        "\n",
        "## Constraints\n",
        "1. Do not hardcode specific paper IDs, author names, or institution names\n",
        "2. Entity resolution must use fuzzy matching or reference data\n",
        "3. PageRank must use damping factor 0.85\n",
        "4. Handle edge cases gracefully\n",
        "\n",
        "## Success Criteria\n",
        "1. All validation checks pass\n",
        "2. Entity resolution reduces author count\n",
        "3. Orphan citations are identified (at least one exists)\n",
        "4. Self-citations are identified (at least one exists)\n",
        "5. PageRank scores sum to approximately 1.0\n",
        "6. Final report follows exact schema\n",
        "7. All numeric values are finite\n",
        "\n",
        "Write complete Python code to solve this task. Store all results in the specified variable names.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Benchmark prompt loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent_task(prompt, data_context):\n",
        "    \"\"\"Run the agent using google.colab.ai to generate code for the task.\"\"\"\n",
        "    \n",
        "    # Prepare context with data samples\n",
        "    context = f\"\"\"\n",
        "You have access to the following data (already loaded in Python):\n",
        "\n",
        "papers_raw: A list of {len(data_context['papers'])} paper dictionaries\n",
        "Sample: {json.dumps(data_context['papers'][0], indent=2)}\n",
        "\n",
        "citations_raw: A pandas DataFrame with {len(data_context['citations'])} rows\n",
        "Columns: {data_context['citations'].columns.tolist()}\n",
        "Sample:\n",
        "{data_context['citations'].head(3).to_string()}\n",
        "\n",
        "affiliations_raw: A dictionary with author and institution reference data\n",
        "Keys: {list(data_context['affiliations'].keys())}\n",
        "Sample author: {json.dumps(list(data_context['affiliations']['authors'].values())[0], indent=2)}\n",
        "Sample institution: {json.dumps(list(data_context['affiliations']['institutions'].values())[0], indent=2)}\n",
        "\n",
        "{prompt}\n",
        "\"\"\"\n",
        "    \n",
        "    print(\"Sending task to agent...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Use google.colab.ai to generate response\n",
        "    # The ai.generate_text function uses the Colab Pro's native AI capabilities\n",
        "    response = ai.generate_text(\n",
        "        prompt=context,\n",
        "        model_name=MODEL_NAME,\n",
        "    )\n",
        "    \n",
        "    return response\n",
        "\n",
        "\n",
        "# Prepare data context\n",
        "data_context = {\n",
        "    'papers': papers_raw,\n",
        "    'citations': citations_raw,\n",
        "    'affiliations': affiliations_raw\n",
        "}\n",
        "\n",
        "# Run the agent\n",
        "agent_response = run_agent_task(BENCHMARK_PROMPT, data_context)\n",
        "print(\"Agent response received\")\n",
        "print(\"=\"*50)\n",
        "print(agent_response[:2000] + \"...\" if len(agent_response) > 2000 else agent_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
