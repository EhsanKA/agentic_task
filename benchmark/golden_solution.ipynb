{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Solution: Research Paper Entity Extraction & Citation Analysis\n",
        "\n",
        "Reference implementation. Runs end-to-end without manual intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q pandas networkx python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q git+https://github.com/EhsanKA/agentic_task.git\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.data.loader import setup_data\n",
        "from benchmark.pipeline.runner import run_pipeline\n",
        "\n",
        "papers_raw, citations_raw, affiliations_raw, DATA_DIR = setup_data()\n",
        "results = run_pipeline(papers_raw, citations_raw, affiliations_raw)\n",
        "\n",
        "for k, v in results.items():\n",
        "    globals()[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(json.dumps(validation_results, indent=2))\n",
        "print(f\"\\nOrphans: {len(orphan_citations)}, Self-cites: {len(self_citations)}\")\n",
        "print(f\"Temporal anomalies: {len(temporal_anomalies)}, Rings: {len(citation_ring_papers)}\")\n",
        "print(f\"Typos: {len(typo_corrections)}, Affiliation conflicts: {len(affiliation_conflicts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(json.dumps(final_report, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.evaluation.tests import set_context, run_all_tests\n",
        "\n",
        "set_context(results)\n",
        "test_result = run_all_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "if test_result.wasSuccessful():\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "else:\n",
        "    print(f\"FAILED: {len(test_result.failures)} failures, {len(test_result.errors)} errors\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
