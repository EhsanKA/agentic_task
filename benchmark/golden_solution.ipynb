{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Solution: Research Paper Entity Extraction & Citation Analysis\n",
        "\n",
        "Reference implementation. Runs end-to-end without manual intervention.\n",
        "\n",
        "**Deliverables demonstrated:**\n",
        "- Data loaded from files (environment interaction)\n",
        "- Entity extraction with resolution and disambiguation\n",
        "- Citation network analysis with anomaly detection\n",
        "- Artifact generation (final_report.json saved to disk)\n",
        "- All unit tests pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, sys, os, shutil\n",
        "\n",
        "REPO_URL = \"https://github.com/EhsanKA/agentic_task.git\"\n",
        "REPO_DIR = \"/content/agentic_task\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", REPO_DIR], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation & Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from benchmark.data.loader import setup_data\n",
        "\n",
        "papers_raw, citations_raw, affiliations_raw, DATA_DIR = setup_data()\n",
        "print(f\"Data directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.pipeline.runner import run_pipeline\n",
        "\n",
        "results = run_pipeline(papers_raw, citations_raw, affiliations_raw, data_dir=DATA_DIR)\n",
        "\n",
        "for k, v in results.items():\n",
        "    globals()[k] = v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(json.dumps(validation_results, indent=2))\n",
        "print(f\"\\nOrphans: {len(orphan_citations)}, Self-cites: {len(self_citations)}\")\n",
        "print(f\"Temporal anomalies: {len(temporal_anomalies)}, Rings: {len(citation_ring_papers)}\")\n",
        "print(f\"Typos: {len(typo_corrections)}, Affiliation conflicts: {len(affiliation_conflicts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(json.dumps(final_report, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark.evaluation.tests import set_context, run_all_tests\n",
        "\n",
        "results[\"_data_dir\"] = DATA_DIR\n",
        "set_context(results)\n",
        "test_result = run_all_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"=\" * 60)\n",
        "print(f\"Artifact saved: {os.path.exists(os.path.join(DATA_DIR, 'final_report.json'))}\")\n",
        "if test_result.wasSuccessful():\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "else:\n",
        "    print(f\"FAILED: {len(test_result.failures)} failures, {len(test_result.errors)} errors\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
