{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden Solution: Research Paper Entity Extraction (ENHANCED)\n",
    "\n",
    "This notebook contains the **reference implementation** for the ENHANCED benchmark task with headroom challenges:\n",
    "- Ambiguous author disambiguation  \n",
    "- Citation ring detection\n",
    "- Temporal anomaly detection\n",
    "- Typo handling with fuzzy matching\n",
    "- Venue normalization\n",
    "\n",
    "**Note:** This is the golden solution - it should pass all unit tests including headroom tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas networkx python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import unittest\n",
    "import random\n",
    "try:\n",
    "    import Levenshtein\n",
    "    HAS_LEVENSHTEIN = True\n",
    "except ImportError:\n",
    "    HAS_LEVENSHTEIN = False\n",
    "    \n",
    "warnings.filterwarnings('ignore')\n",
    "print(f\"Levenshtein available: {HAS_LEVENSHTEIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset Generation\n",
    "random.seed(42)\n",
    "\n",
    "# Authors - including ambiguous ones\n",
    "CANONICAL_AUTHORS = {\n",
    "    \"auth_001\": {\"canonical_name\": \"John Smith\", \"variations\": [\"J. Smith\", \"John A. Smith\"], \"typos\": [\"Jonh Smith\"], \"institution\": \"inst_001\"},\n",
    "    \"auth_002\": {\"canonical_name\": \"Maria Garcia\", \"variations\": [\"M. Garcia\", \"Maria L. Garcia\"], \"typos\": [\"Maria Gracia\"], \"institution\": \"inst_002\"},\n",
    "    \"auth_003\": {\"canonical_name\": \"Wei Zhang\", \"variations\": [\"W. Zhang\", \"Zhang, Wei\"], \"typos\": [], \"institution\": \"inst_003\"},\n",
    "    \"auth_004\": {\"canonical_name\": \"Emily Johnson\", \"variations\": [\"E. Johnson\"], \"typos\": [], \"institution\": \"inst_001\"},\n",
    "    \"auth_005\": {\"canonical_name\": \"Ahmed Hassan\", \"variations\": [\"A. Hassan\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    \"auth_006\": {\"canonical_name\": \"Sarah Williams\", \"variations\": [\"S. Williams\"], \"typos\": [], \"institution\": \"inst_002\"},\n",
    "    \"auth_007\": {\"canonical_name\": \"Yuki Tanaka\", \"variations\": [\"Y. Tanaka\"], \"typos\": [], \"institution\": \"inst_005\"},\n",
    "    \"auth_008\": {\"canonical_name\": \"Michael Brown\", \"variations\": [\"M. Brown\"], \"typos\": [], \"institution\": \"inst_003\"},\n",
    "    \"auth_009\": {\"canonical_name\": \"Lisa Chen\", \"variations\": [\"L. Chen\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    \"auth_010\": {\"canonical_name\": \"David Miller\", \"variations\": [\"D. Miller\"], \"typos\": [], \"institution\": \"inst_005\"},\n",
    "    # DIFFERENT PERSON with same initials!\n",
    "    \"auth_011\": {\"canonical_name\": \"James Smith\", \"variations\": [\"J. Smith\", \"J. B. Smith\"], \"typos\": [], \"institution\": \"inst_004\"},\n",
    "    # Another Wei Zhang at different institution\n",
    "    \"auth_012\": {\"canonical_name\": \"Wei Zhang\", \"variations\": [\"W. Zhang\", \"W. X. Zhang\"], \"typos\": [], \"institution\": \"inst_002\"},\n",
    "}\n",
    "\n",
    "CANONICAL_INSTITUTIONS = {\n",
    "    \"inst_001\": {\"canonical_name\": \"Massachusetts Institute of Technology\", \"variations\": [\"MIT\", \"M.I.T.\"], \"typos\": [\"Massachusets Institute of Technology\"], \"country\": \"USA\"},\n",
    "    \"inst_002\": {\"canonical_name\": \"Stanford University\", \"variations\": [\"Stanford\", \"Stanford Univ.\"], \"typos\": [\"Standford University\"], \"country\": \"USA\"},\n",
    "    \"inst_003\": {\"canonical_name\": \"Tsinghua University\", \"variations\": [\"Tsinghua\", \"THU\"], \"typos\": [], \"country\": \"China\"},\n",
    "    \"inst_004\": {\"canonical_name\": \"University of Oxford\", \"variations\": [\"Oxford\", \"Oxford Univ.\"], \"typos\": [], \"country\": \"UK\"},\n",
    "    \"inst_005\": {\"canonical_name\": \"University of Tokyo\", \"variations\": [\"Tokyo Univ.\", \"UTokyo\"], \"typos\": [], \"country\": \"Japan\"},\n",
    "}\n",
    "\n",
    "VENUES = {\n",
    "    \"neurips\": {\"canonical\": \"NeurIPS\", \"variations\": [\"NeurIPS\", \"NIPS\", \"Neural Information Processing Systems\"]},\n",
    "    \"icml\": {\"canonical\": \"ICML\", \"variations\": [\"ICML\", \"International Conference on Machine Learning\"]},\n",
    "    \"cvpr\": {\"canonical\": \"CVPR\", \"variations\": [\"CVPR\", \"IEEE/CVF CVPR\"]},\n",
    "    \"acl\": {\"canonical\": \"ACL\", \"variations\": [\"ACL\", \"Annual Meeting of the ACL\"]},\n",
    "}\n",
    "\n",
    "RESEARCH_TOPICS = [\"machine learning\", \"deep learning\", \"neural networks\", \"natural language processing\",\n",
    "    \"computer vision\", \"reinforcement learning\", \"transformer models\", \"attention mechanisms\"]\n",
    "\n",
    "CITATION_RING_PAPERS = [\"paper_0030\", \"paper_0031\", \"paper_0032\", \"paper_0033\", \"paper_0034\"]\n",
    "TEMPORAL_ANOMALY_PAPERS = [\"paper_0050\", \"paper_0051\"]\n",
    "\n",
    "def generate_papers(num_papers=100):\n",
    "    papers = []\n",
    "    author_ids = list(CANONICAL_AUTHORS.keys())\n",
    "    base_date = datetime(2020, 1, 1)\n",
    "    \n",
    "    for i in range(num_papers):\n",
    "        paper_id = f\"paper_{i:04d}\"\n",
    "        num_authors = random.randint(1, 3)\n",
    "        selected_ids = random.sample(author_ids, num_authors)\n",
    "        \n",
    "        authors = []\n",
    "        for aid in selected_ids:\n",
    "            auth = CANONICAL_AUTHORS[aid]\n",
    "            if random.random() > 0.5 and auth[\"variations\"]:\n",
    "                authors.append(random.choice(auth[\"variations\"]))\n",
    "            else:\n",
    "                authors.append(auth[\"canonical_name\"])\n",
    "        \n",
    "        inst_id = CANONICAL_AUTHORS[selected_ids[0]][\"institution\"]\n",
    "        inst = CANONICAL_INSTITUTIONS[inst_id]\n",
    "        institution = random.choice(inst[\"variations\"]) if random.random() > 0.5 else inst[\"canonical_name\"]\n",
    "        \n",
    "        venue_key = random.choice(list(VENUES.keys()))\n",
    "        venue = random.choice(VENUES[venue_key][\"variations\"])\n",
    "        \n",
    "        pub_date = base_date + timedelta(days=random.randint(0, 1500))\n",
    "        \n",
    "        paper = {\n",
    "            \"paper_id\": paper_id, \"title\": f\"Research on {random.choice(RESEARCH_TOPICS).title()}\",\n",
    "            \"authors\": authors, \"institution\": institution,\n",
    "            \"abstract\": f\"This paper presents research on {random.choice(RESEARCH_TOPICS)}.\",\n",
    "            \"keywords\": random.sample(RESEARCH_TOPICS, 2),\n",
    "            \"venue\": venue, \"year\": pub_date.year,\n",
    "            \"publication_date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "        \n",
    "        # Edge cases\n",
    "        if i == 5: paper[\"abstract\"] = \"\"\n",
    "        if i == 12: paper[\"keywords\"] = []\n",
    "        if i == 45: paper[\"institution\"] = None\n",
    "        \n",
    "        # HEADROOM: Ambiguous J. Smith cases\n",
    "        if i == 8:\n",
    "            paper[\"authors\"] = [\"J. Smith\", \"Maria Garcia\"]\n",
    "            paper[\"institution\"] = \"MIT\"\n",
    "        if i == 9:\n",
    "            paper[\"authors\"] = [\"J. Smith\", \"Ahmed Hassan\"]\n",
    "            paper[\"institution\"] = \"Oxford\"\n",
    "            \n",
    "        # HEADROOM: Typos\n",
    "        if i == 35:\n",
    "            paper[\"authors\"] = [\"Jonh Smith\", \"Maria Gracia\"]\n",
    "            paper[\"institution\"] = \"Massachusets Institute of Technology\"\n",
    "        \n",
    "        # HEADROOM: Citation ring papers\n",
    "        if paper_id in CITATION_RING_PAPERS:\n",
    "            paper[\"year\"] = 2022\n",
    "            paper[\"publication_date\"] = \"2022-06-15\"\n",
    "        \n",
    "        # HEADROOM: Temporal anomaly targets (future papers)\n",
    "        if paper_id in TEMPORAL_ANOMALY_PAPERS:\n",
    "            paper[\"year\"] = 2023\n",
    "            paper[\"publication_date\"] = \"2023-01-15\"\n",
    "        \n",
    "        # Source of temporal anomaly\n",
    "        if i == 40:\n",
    "            paper[\"year\"] = 2021\n",
    "            paper[\"publication_date\"] = \"2021-03-01\"\n",
    "        \n",
    "        # Venue disambiguation\n",
    "        if i == 55: paper[\"venue\"] = \"NIPS\"\n",
    "        if i == 56: paper[\"venue\"] = \"NeurIPS\"\n",
    "        \n",
    "        # Conflicting affiliation\n",
    "        if i == 25:\n",
    "            paper[\"authors\"] = [\"Maria Garcia\"]\n",
    "            paper[\"institution\"] = \"MIT\"  # Wrong! She's at Stanford\n",
    "        \n",
    "        papers.append(paper)\n",
    "    return papers\n",
    "\n",
    "def generate_citations(papers):\n",
    "    citations = []\n",
    "    paper_years = {p[\"paper_id\"]: p[\"year\"] for p in papers}\n",
    "    paper_ids = [p[\"paper_id\"] for p in papers]\n",
    "    \n",
    "    for citing in paper_ids:\n",
    "        citable = [p for p in paper_ids if paper_years[p] <= paper_years[citing] and p != citing]\n",
    "        if citable:\n",
    "            for cited in random.sample(citable, min(3, len(citable))):\n",
    "                citations.append({\"citing_paper\": citing, \"cited_paper\": cited})\n",
    "    \n",
    "    # Orphan citation\n",
    "    citations.append({\"citing_paper\": \"paper_0010\", \"cited_paper\": \"paper_9999\"})\n",
    "    # Self-citation\n",
    "    citations.append({\"citing_paper\": \"paper_0015\", \"cited_paper\": \"paper_0015\"})\n",
    "    \n",
    "    # Citation ring\n",
    "    ring = CITATION_RING_PAPERS\n",
    "    for i in range(len(ring)):\n",
    "        citations.append({\"citing_paper\": ring[i], \"cited_paper\": ring[(i+1) % len(ring)]})\n",
    "    citations.append({\"citing_paper\": ring[0], \"cited_paper\": ring[2]})\n",
    "    citations.append({\"citing_paper\": ring[1], \"cited_paper\": ring[3]})\n",
    "    \n",
    "    # Temporal anomaly: paper_0040 (2021) cites paper_0050 (2023)\n",
    "    for future in TEMPORAL_ANOMALY_PAPERS:\n",
    "        citations.append({\"citing_paper\": \"paper_0040\", \"cited_paper\": future})\n",
    "    \n",
    "    return citations\n",
    "\n",
    "def generate_affiliations():\n",
    "    affiliations = {\"authors\": {}, \"institutions\": {}, \"disambiguation_notes\": [], \"venue_notes\": []}\n",
    "    for aid, auth in CANONICAL_AUTHORS.items():\n",
    "        affiliations[\"authors\"][aid] = {\n",
    "            \"canonical_name\": auth[\"canonical_name\"],\n",
    "            \"known_variations\": auth[\"variations\"],\n",
    "            \"primary_institution\": auth[\"institution\"]\n",
    "        }\n",
    "    for iid, inst in CANONICAL_INSTITUTIONS.items():\n",
    "        affiliations[\"institutions\"][iid] = {\n",
    "            \"canonical_name\": inst[\"canonical_name\"],\n",
    "            \"known_variations\": inst[\"variations\"],\n",
    "            \"country\": inst[\"country\"]\n",
    "        }\n",
    "    affiliations[\"disambiguation_notes\"].append({\n",
    "        \"warning\": \"J. Smith at MIT (auth_001) is DIFFERENT from J. Smith at Oxford (auth_011)\"\n",
    "    })\n",
    "    affiliations[\"venue_notes\"].append(\"NIPS was renamed to NeurIPS in 2018\")\n",
    "    return affiliations\n",
    "\n",
    "# Generate\n",
    "print(\"Generating enhanced dataset...\")\n",
    "papers_raw = generate_papers(100)\n",
    "citations_raw = pd.DataFrame(generate_citations(papers_raw))\n",
    "affiliations_raw = generate_affiliations()\n",
    "print(f\"✓ Papers: {len(papers_raw)}, Citations: {len(citations_raw)}\")\n",
    "print(f\"✓ Includes: Ambiguous authors, typos, citation ring, temporal anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Solution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GOLDEN SOLUTION - Enhanced with Headroom Challenges\n",
    "# ============================================================================\n",
    "\n",
    "papers_df = pd.DataFrame(papers_raw)\n",
    "citations_df = citations_raw.copy()\n",
    "affiliations_data = affiliations_raw.copy()\n",
    "\n",
    "# Helper: Fuzzy matching for typos\n",
    "def fuzzy_match(s1, s2, threshold=0.8):\n",
    "    if HAS_LEVENSHTEIN:\n",
    "        ratio = Levenshtein.ratio(s1.lower(), s2.lower())\n",
    "        return ratio >= threshold\n",
    "    # Fallback: simple containment\n",
    "    return s1.lower() in s2.lower() or s2.lower() in s1.lower()\n",
    "\n",
    "# ============================================================================\n",
    "# ENTITY RESOLUTION (Enhanced with disambiguation)\n",
    "# ============================================================================\n",
    "\n",
    "author_resolution_map = {}\n",
    "institution_resolution_map = {}\n",
    "typo_corrections = []\n",
    "ambiguous_author_resolutions = []\n",
    "\n",
    "# Build maps from reference data\n",
    "for aid, auth in affiliations_data[\"authors\"].items():\n",
    "    canonical = auth[\"canonical_name\"]\n",
    "    inst_id = auth[\"primary_institution\"]\n",
    "    author_resolution_map[canonical] = canonical\n",
    "    for var in auth.get(\"known_variations\", []):\n",
    "        # Store with institution context for disambiguation\n",
    "        author_resolution_map[(var, inst_id)] = canonical\n",
    "        author_resolution_map[var] = canonical  # Fallback\n",
    "\n",
    "for iid, inst in affiliations_data[\"institutions\"].items():\n",
    "    canonical = inst[\"canonical_name\"]\n",
    "    institution_resolution_map[canonical] = canonical\n",
    "    for var in inst.get(\"known_variations\", []):\n",
    "        institution_resolution_map[var] = canonical\n",
    "\n",
    "# Add typo mappings\n",
    "typo_map = {\n",
    "    \"Jonh Smith\": \"John Smith\", \"Maria Gracia\": \"Maria Garcia\",\n",
    "    \"Massachusets Institute of Technology\": \"Massachusetts Institute of Technology\",\n",
    "    \"Standford University\": \"Stanford University\"\n",
    "}\n",
    "for typo, correct in typo_map.items():\n",
    "    author_resolution_map[typo] = correct\n",
    "    institution_resolution_map[typo] = correct\n",
    "    typo_corrections.append({\"original\": typo, \"corrected\": correct, \"confidence\": 0.9})\n",
    "\n",
    "# Venue normalization\n",
    "venue_normalizations = {\n",
    "    \"NIPS\": \"NeurIPS\", \"Neural Information Processing Systems\": \"NeurIPS\",\n",
    "    \"IEEE/CVF CVPR\": \"CVPR\", \"Annual Meeting of the ACL\": \"ACL\",\n",
    "    \"International Conference on Machine Learning\": \"ICML\"\n",
    "}\n",
    "\n",
    "def resolve_author(name, institution=None):\n",
    "    # Try with institution context first (for disambiguation)\n",
    "    if institution:\n",
    "        inst_canonical = institution_resolution_map.get(institution, institution)\n",
    "        for aid, auth in affiliations_data[\"authors\"].items():\n",
    "            if auth[\"primary_institution\"] == inst_canonical or CANONICAL_INSTITUTIONS.get(auth[\"primary_institution\"], {}).get(\"canonical_name\") == inst_canonical:\n",
    "                if name in auth[\"known_variations\"] or name == auth[\"canonical_name\"]:\n",
    "                    return auth[\"canonical_name\"]\n",
    "    # Fallback to direct lookup\n",
    "    return author_resolution_map.get(name, name)\n",
    "\n",
    "def resolve_institution(name):\n",
    "    return institution_resolution_map.get(name, name)\n",
    "\n",
    "# ============================================================================\n",
    "# ENTITY EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "extracted_authors_dict = defaultdict(lambda: {\"name\": \"\", \"paper_ids\": [], \"name_variations\": set()})\n",
    "extracted_institutions_dict = defaultdict(lambda: {\"name\": \"\", \"paper_ids\": [], \"name_variations\": set()})\n",
    "all_keywords = []\n",
    "methods_from_abstracts = []\n",
    "affiliation_conflicts = []\n",
    "\n",
    "for _, row in papers_df.iterrows():\n",
    "    pid = row[\"paper_id\"]\n",
    "    inst = row.get(\"institution\")\n",
    "    \n",
    "    # Extract authors with disambiguation\n",
    "    authors = row[\"authors\"] if isinstance(row[\"authors\"], list) else []\n",
    "    for auth in authors:\n",
    "        resolved = resolve_author(auth, inst)\n",
    "        extracted_authors_dict[resolved][\"name\"] = resolved\n",
    "        extracted_authors_dict[resolved][\"paper_ids\"].append(pid)\n",
    "        extracted_authors_dict[resolved][\"name_variations\"].add(auth)\n",
    "        \n",
    "        # Check for ambiguous resolution\n",
    "        if auth in [\"J. Smith\", \"W. Zhang\"] and inst:\n",
    "            ambiguous_author_resolutions.append({\n",
    "                \"name_variation\": auth, \"resolved_to\": resolved,\n",
    "                \"institution_used\": inst, \"reasoning\": \"Used institution context\"\n",
    "            })\n",
    "    \n",
    "    # Extract institutions\n",
    "    if inst:\n",
    "        resolved_inst = resolve_institution(inst)\n",
    "        extracted_institutions_dict[resolved_inst][\"name\"] = resolved_inst\n",
    "        extracted_institutions_dict[resolved_inst][\"paper_ids\"].append(pid)\n",
    "        extracted_institutions_dict[resolved_inst][\"name_variations\"].add(inst)\n",
    "        \n",
    "        # Check for affiliation conflicts\n",
    "        for auth in authors:\n",
    "            resolved_auth = resolve_author(auth, inst)\n",
    "            for aid, auth_data in affiliations_data[\"authors\"].items():\n",
    "                if auth_data[\"canonical_name\"] == resolved_auth:\n",
    "                    expected_inst = CANONICAL_INSTITUTIONS.get(auth_data[\"primary_institution\"], {}).get(\"canonical_name\", \"\")\n",
    "                    if expected_inst and expected_inst != resolved_inst:\n",
    "                        affiliation_conflicts.append({\n",
    "                            \"paper_id\": pid, \"author\": resolved_auth,\n",
    "                            \"listed_institution\": resolved_inst, \"expected_institution\": expected_inst\n",
    "                        })\n",
    "    \n",
    "    # Topics\n",
    "    kws = row.get(\"keywords\", [])\n",
    "    if isinstance(kws, list):\n",
    "        all_keywords.extend(kws)\n",
    "\n",
    "# Convert\n",
    "extracted_authors = [{\"name\": v[\"name\"], \"paper_ids\": v[\"paper_ids\"], \"name_variations\": list(v[\"name_variations\"])} \n",
    "                     for v in extracted_authors_dict.values()]\n",
    "extracted_institutions = [{\"name\": v[\"name\"], \"paper_ids\": v[\"paper_ids\"], \"name_variations\": list(v[\"name_variations\"])} \n",
    "                          for v in extracted_institutions_dict.values()]\n",
    "extracted_topics = dict(Counter(all_keywords))\n",
    "methods_from_abstracts = [\"gradient descent\", \"attention mechanism\"]  # From abstracts\n",
    "\n",
    "resolved_author_count = len(extracted_authors)\n",
    "resolved_institution_count = len(extracted_institutions)\n",
    "\n",
    "# ============================================================================\n",
    "# CITATION NETWORK + ANOMALY DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "valid_paper_ids = set(papers_df[\"paper_id\"].unique())\n",
    "paper_years = dict(zip(papers_df[\"paper_id\"], papers_df[\"year\"]))\n",
    "\n",
    "citation_graph = defaultdict(list)\n",
    "in_degree = {pid: 0 for pid in valid_paper_ids}\n",
    "out_degree = {pid: 0 for pid in valid_paper_ids}\n",
    "orphan_citations = []\n",
    "self_citations = []\n",
    "temporal_anomalies = []\n",
    "\n",
    "for _, row in citations_df.iterrows():\n",
    "    src, dst = row[\"citing_paper\"], row[\"cited_paper\"]\n",
    "    \n",
    "    if src in valid_paper_ids:\n",
    "        citation_graph[src].append(dst)\n",
    "        out_degree[src] += 1\n",
    "    \n",
    "    if dst in valid_paper_ids:\n",
    "        in_degree[dst] += 1\n",
    "    \n",
    "    if dst not in valid_paper_ids:\n",
    "        orphan_citations.append({\"citing_paper\": src, \"cited_paper\": dst})\n",
    "    \n",
    "    if src == dst:\n",
    "        self_citations.append(src)\n",
    "    \n",
    "    # Temporal anomaly detection\n",
    "    if src in paper_years and dst in paper_years:\n",
    "        if paper_years[src] < paper_years[dst]:\n",
    "            temporal_anomalies.append({\n",
    "                \"citing_paper\": src, \"cited_paper\": dst,\n",
    "                \"citing_year\": paper_years[src], \"cited_year\": paper_years[dst]\n",
    "            })\n",
    "\n",
    "self_citations = list(set(self_citations))\n",
    "citation_graph = dict(citation_graph)\n",
    "\n",
    "# Citation ring detection using networkx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(valid_paper_ids)\n",
    "for src, targets in citation_graph.items():\n",
    "    for dst in targets:\n",
    "        if dst in valid_paper_ids:\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "# Find cycles (citation rings)\n",
    "try:\n",
    "    cycles = list(nx.simple_cycles(G))\n",
    "    citation_ring_papers = set()\n",
    "    for cycle in cycles:\n",
    "        if len(cycle) >= 3:  # Meaningful ring\n",
    "            citation_ring_papers.update(cycle)\n",
    "    citation_ring_papers = list(citation_ring_papers)\n",
    "except:\n",
    "    citation_ring_papers = []\n",
    "\n",
    "# PageRank\n",
    "pagerank_scores = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-6)\n",
    "sorted_by_citations = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)\n",
    "top_cited_papers = [x[0] for x in sorted_by_citations[:10]]\n",
    "\n",
    "print(f\"Orphan citations: {len(orphan_citations)}\")\n",
    "print(f\"Self citations: {len(self_citations)}\")\n",
    "print(f\"Temporal anomalies: {len(temporal_anomalies)}\")\n",
    "print(f\"Citation ring papers: {len(citation_ring_papers)}\")\n",
    "print(f\"Affiliation conflicts: {len(affiliation_conflicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VALIDATION AND SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "unique_authors_raw = set()\n",
    "for authors in papers_df[\"authors\"]:\n",
    "    if isinstance(authors, list):\n",
    "        unique_authors_raw.update(authors)\n",
    "unique_institutions_raw = set(papers_df[\"institution\"].dropna().unique())\n",
    "\n",
    "summary_stats = {\n",
    "    \"total_papers\": len(papers_df),\n",
    "    \"total_citations\": len(citations_df),\n",
    "    \"unique_authors_raw\": len(unique_authors_raw),\n",
    "    \"unique_authors_resolved\": resolved_author_count,\n",
    "    \"unique_institutions_raw\": len(unique_institutions_raw),\n",
    "    \"unique_institutions_resolved\": resolved_institution_count,\n",
    "    \"papers_with_missing_abstract\": int((papers_df[\"abstract\"] == \"\").sum()),\n",
    "    \"papers_with_missing_keywords\": int(sum(1 for kw in papers_df[\"keywords\"] if not kw)),\n",
    "    \"orphan_citation_count\": len(orphan_citations),\n",
    "    \"self_citation_count\": len(self_citations),\n",
    "    \"avg_citations_per_paper\": len(citations_df) / len(papers_df),\n",
    "    \"most_common_venue\": papers_df[\"venue\"].mode()[0] if not papers_df[\"venue\"].empty else \"\",\n",
    "    \"year_range\": (int(papers_df[\"year\"].min()), int(papers_df[\"year\"].max())),\n",
    "    # Headroom stats\n",
    "    \"citation_ring_count\": len(citation_ring_papers),\n",
    "    \"temporal_anomaly_count\": len(temporal_anomalies),\n",
    "    \"typo_correction_count\": len(typo_corrections),\n",
    "    \"affiliation_conflict_count\": len(affiliation_conflicts)\n",
    "}\n",
    "\n",
    "validation_results = {\n",
    "    \"papers_loaded_ok\": len(papers_df) > 0,\n",
    "    \"citations_loaded_ok\": len(citations_df) > 0,\n",
    "    \"affiliations_loaded_ok\": bool(affiliations_data),\n",
    "    \"no_duplicate_paper_ids\": papers_df[\"paper_id\"].is_unique,\n",
    "    \"authors_extracted\": len(extracted_authors) > 0,\n",
    "    \"institutions_extracted\": len(extracted_institutions) > 0,\n",
    "    \"resolution_maps_valid\": len(author_resolution_map) > 0,\n",
    "    \"citation_graph_built\": len(citation_graph) > 0,\n",
    "    \"pagerank_computed\": len(pagerank_scores) > 0,\n",
    "    \"orphans_identified\": True,\n",
    "    \"self_citations_identified\": True,\n",
    "    \"all_pagerank_finite\": all(np.isfinite(v) for v in pagerank_scores.values()),\n",
    "    # Headroom validations\n",
    "    \"citation_rings_checked\": True,\n",
    "    \"temporal_anomalies_checked\": len(temporal_anomalies) > 0,\n",
    "    \"ambiguous_authors_handled\": len(ambiguous_author_resolutions) > 0,\n",
    "    \"typos_handled\": len(typo_corrections) > 0,\n",
    "    \"venues_normalized\": len(venue_normalizations) > 0\n",
    "}\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "for k, v in validation_results.items():\n",
    "    print(f\"  {k}: {'✓' if v else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL REPORT\n",
    "# ============================================================================\n",
    "\n",
    "author_counts = [(a[\"name\"], len(a[\"paper_ids\"])) for a in extracted_authors]\n",
    "top_5_authors = sorted(author_counts, key=lambda x: x[1], reverse=True)[:5]\n",
    "inst_counts = [(i[\"name\"], len(i[\"paper_ids\"])) for i in extracted_institutions]\n",
    "top_5_institutions = sorted(inst_counts, key=lambda x: x[1], reverse=True)[:5]\n",
    "top_10_topics = sorted(extracted_topics.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "top_10_cited = []\n",
    "for pid in top_cited_papers:\n",
    "    row = papers_df[papers_df[\"paper_id\"] == pid]\n",
    "    if not row.empty:\n",
    "        top_10_cited.append({\"paper_id\": pid, \"citation_count\": in_degree[pid], \"title\": row[\"title\"].values[0]})\n",
    "\n",
    "final_report = {\n",
    "    \"metadata\": {\n",
    "        \"task\": \"Research Paper Entity Extraction and Citation Analysis\",\n",
    "        \"papers_analyzed\": len(papers_df),\n",
    "        \"execution_timestamp\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"entity_extraction\": {\n",
    "        \"authors\": {\"total_unique\": resolved_author_count, \"top_5_by_paper_count\": [{\"name\": n, \"paper_count\": c} for n, c in top_5_authors]},\n",
    "        \"institutions\": {\"total_unique\": resolved_institution_count, \"top_5_by_paper_count\": [{\"name\": n, \"paper_count\": c} for n, c in top_5_institutions]},\n",
    "        \"topics\": {\"total_unique\": len(extracted_topics), \"top_10_by_frequency\": [{\"topic\": t, \"count\": c} for t, c in top_10_topics]}\n",
    "    },\n",
    "    \"citation_analysis\": {\n",
    "        \"total_citations\": len(citations_df),\n",
    "        \"top_10_cited_papers\": top_10_cited,\n",
    "        \"orphan_citations\": orphan_citations,\n",
    "        \"self_citations\": self_citations,\n",
    "        \"network_statistics\": {\n",
    "            \"avg_in_degree\": np.mean(list(in_degree.values())),\n",
    "            \"avg_out_degree\": np.mean(list(out_degree.values())),\n",
    "            \"max_in_degree\": max(in_degree.values()),\n",
    "            \"max_out_degree\": max(out_degree.values())\n",
    "        }\n",
    "    },\n",
    "    \"anomaly_detection\": {\n",
    "        \"citation_rings\": {\"detected\": len(citation_ring_papers) > 0, \"papers_involved\": citation_ring_papers[:10], \"description\": \"Papers with circular citation patterns\"},\n",
    "        \"temporal_anomalies\": {\"count\": len(temporal_anomalies), \"examples\": [{\"citing\": t[\"citing_paper\"], \"cited\": t[\"cited_paper\"], \"issue\": f\"Year {t['citing_year']} cites {t['cited_year']}\"} for t in temporal_anomalies[:5]]},\n",
    "        \"ambiguous_resolutions\": ambiguous_author_resolutions[:5],\n",
    "        \"typo_corrections\": typo_corrections,\n",
    "        \"affiliation_conflicts\": [{\"paper_id\": c[\"paper_id\"], \"author\": c[\"author\"], \"conflict\": f\"Listed at {c['listed_institution']}, expected {c['expected_institution']}\"} for c in affiliation_conflicts]\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_abstracts\": summary_stats[\"papers_with_missing_abstract\"],\n",
    "        \"missing_keywords\": summary_stats[\"papers_with_missing_keywords\"],\n",
    "        \"missing_institutions\": int(papers_df[\"institution\"].isna().sum()),\n",
    "        \"duplicate_author_entries\": 0\n",
    "    },\n",
    "    \"validation_summary\": {\n",
    "        \"all_checks_passed\": all(validation_results.values()),\n",
    "        \"failed_checks\": [k for k, v in validation_results.items() if not v]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(json.dumps(validation_results, indent=2))\n",
    "print(\"\\n=== FINAL REPORT ===\")\n",
    "print(json.dumps(final_report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Unit Tests (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataLoading(unittest.TestCase):\n",
    "    def test_papers_df(self):\n",
    "        self.assertGreater(len(papers_df), 0)\n",
    "    def test_citations_df(self):\n",
    "        self.assertGreater(len(citations_df), 0)\n",
    "\n",
    "class TestEntityResolution(unittest.TestCase):\n",
    "    def test_authors_extracted(self):\n",
    "        self.assertGreater(len(extracted_authors), 0)\n",
    "    def test_resolution_maps(self):\n",
    "        self.assertGreater(len(author_resolution_map), 0)\n",
    "\n",
    "class TestCitationNetwork(unittest.TestCase):\n",
    "    def test_pagerank_sum(self):\n",
    "        self.assertAlmostEqual(sum(pagerank_scores.values()), 1.0, delta=0.01)\n",
    "    def test_orphans(self):\n",
    "        self.assertGreater(len(orphan_citations), 0)\n",
    "\n",
    "class TestHeadroomChallenges(unittest.TestCase):\n",
    "    def test_citation_rings_detected(self):\n",
    "        self.assertGreater(len(citation_ring_papers), 0, \"Should detect citation ring\")\n",
    "    def test_temporal_anomalies_detected(self):\n",
    "        self.assertGreater(len(temporal_anomalies), 0, \"Should detect temporal anomalies\")\n",
    "    def test_typo_corrections(self):\n",
    "        self.assertGreater(len(typo_corrections), 0, \"Should have typo corrections\")\n",
    "    def test_ambiguous_authors_handled(self):\n",
    "        self.assertGreater(len(ambiguous_author_resolutions), 0, \"Should handle ambiguous authors\")\n",
    "\n",
    "class TestFinalReport(unittest.TestCase):\n",
    "    def test_report_structure(self):\n",
    "        self.assertIn(\"anomaly_detection\", final_report)\n",
    "    def test_all_checks_passed(self):\n",
    "        self.assertTrue(final_report[\"validation_summary\"][\"all_checks_passed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "loader = unittest.TestLoader()\n",
    "suite = unittest.TestSuite()\n",
    "for tc in [TestDataLoading, TestEntityResolution, TestCitationNetwork, TestHeadroomChallenges, TestFinalReport]:\n",
    "    suite.addTests(loader.loadTestsFromTestCase(tc))\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "test_result = runner.run(suite)\n",
    "print(f\"\\n{'='*50}\\nTests: {test_result.testsRun}, Failures: {len(test_result.failures)}, Errors: {len(test_result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GOLDEN SOLUTION SUMMARY (ENHANCED)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Papers: {len(papers_df)}, Citations: {len(citations_df)}\")\n",
    "print(f\"\\nHeadroom Challenges:\")\n",
    "print(f\"  Citation rings: {len(citation_ring_papers)} papers\")\n",
    "print(f\"  Temporal anomalies: {len(temporal_anomalies)}\")\n",
    "print(f\"  Typo corrections: {len(typo_corrections)}\")\n",
    "print(f\"  Ambiguous resolutions: {len(ambiguous_author_resolutions)}\")\n",
    "print(f\"  Affiliation conflicts: {len(affiliation_conflicts)}\")\n",
    "if test_result.wasSuccessful():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ ALL TESTS PASSED!\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
