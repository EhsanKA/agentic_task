{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Solution: Research Paper Entity Extraction and Citation Analysis\n",
        "\n",
        "This notebook contains the reference implementation for the benchmark task.\n",
        "It demonstrates that the task is solvable and provides the expected outputs.\n",
        "\n",
        "**Note:** This is the golden solution - it should pass all unit tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if not already installed)\n",
        "%pip install -q pandas networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import re\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "\n",
        "Load the three data files and validate their structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab, upload files or mount Google Drive\n",
        "# Here we assume files are in the current directory or uploaded\n",
        "\n",
        "# Define file paths (adjust as needed for your environment)\n",
        "PAPERS_FILE = \"papers_metadata.json\"\n",
        "CITATIONS_FILE = \"citations.csv\"\n",
        "AFFILIATIONS_FILE = \"author_affiliations.json\"\n",
        "\n",
        "# Load papers metadata\n",
        "with open(PAPERS_FILE, 'r') as f:\n",
        "    papers_raw = json.load(f)\n",
        "papers_df = pd.DataFrame(papers_raw)\n",
        "\n",
        "# Load citations\n",
        "citations_df = pd.read_csv(CITATIONS_FILE)\n",
        "\n",
        "# Load author affiliations reference data\n",
        "with open(AFFILIATIONS_FILE, 'r') as f:\n",
        "    affiliations_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(papers_df)} papers\")\n",
        "print(f\"Loaded {len(citations_df)} citations\")\n",
        "print(f\"Affiliations data keys: {list(affiliations_data.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect data structure\n",
        "print(\"Papers columns:\", papers_df.columns.tolist())\n",
        "print(\"\\nSample paper:\")\n",
        "print(papers_df.iloc[0].to_dict())\n",
        "print(\"\\nCitations columns:\", citations_df.columns.tolist())\n",
        "print(\"\\nSample citations:\")\n",
        "print(citations_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entity Extraction\n",
        "\n",
        "Extract authors, institutions, topics, and methods from the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_all_authors(papers_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Extract all author entities from papers.\"\"\"\n",
        "    author_to_papers = defaultdict(list)\n",
        "    \n",
        "    for _, row in papers_df.iterrows():\n",
        "        paper_id = row['paper_id']\n",
        "        authors = row['authors']\n",
        "        if isinstance(authors, list):\n",
        "            for author in authors:\n",
        "                if author and isinstance(author, str):\n",
        "                    author_to_papers[author.strip()].append(paper_id)\n",
        "    \n",
        "    extracted_authors = []\n",
        "    for name, paper_ids in author_to_papers.items():\n",
        "        extracted_authors.append({\n",
        "            \"name\": name,\n",
        "            \"paper_ids\": list(set(paper_ids)),\n",
        "            \"name_variations\": [name]  # Will be expanded during resolution\n",
        "        })\n",
        "    \n",
        "    return extracted_authors\n",
        "\n",
        "\n",
        "def extract_all_institutions(papers_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Extract all institution entities from papers.\"\"\"\n",
        "    inst_to_papers = defaultdict(list)\n",
        "    \n",
        "    for _, row in papers_df.iterrows():\n",
        "        paper_id = row['paper_id']\n",
        "        institution = row.get('institution')\n",
        "        if institution and isinstance(institution, str) and institution.strip():\n",
        "            inst_to_papers[institution.strip()].append(paper_id)\n",
        "    \n",
        "    extracted_institutions = []\n",
        "    for name, paper_ids in inst_to_papers.items():\n",
        "        extracted_institutions.append({\n",
        "            \"name\": name,\n",
        "            \"paper_ids\": list(set(paper_ids)),\n",
        "            \"name_variations\": [name]\n",
        "        })\n",
        "    \n",
        "    return extracted_institutions\n",
        "\n",
        "\n",
        "def extract_topics(papers_df: pd.DataFrame) -> Dict[str, int]:\n",
        "    \"\"\"Extract topic/keyword frequencies.\"\"\"\n",
        "    topic_counts = Counter()\n",
        "    \n",
        "    for _, row in papers_df.iterrows():\n",
        "        keywords = row.get('keywords', [])\n",
        "        if isinstance(keywords, list):\n",
        "            for kw in keywords:\n",
        "                if kw and isinstance(kw, str):\n",
        "                    topic_counts[kw.strip().lower()] += 1\n",
        "    \n",
        "    return dict(topic_counts)\n",
        "\n",
        "\n",
        "def extract_methods_from_abstracts(papers_df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"Extract research methods mentioned in abstracts.\"\"\"\n",
        "    # Common research method patterns to look for\n",
        "    method_patterns = [\n",
        "        r'gradient descent', r'backpropagation', r'stochastic optimization',\n",
        "        r'cross-validation', r'ablation stud(?:y|ies)', r'hyperparameter tuning',\n",
        "        r'ensemble method', r'regularization', r'dropout', r'batch normalization',\n",
        "        r'attention mechanism', r'skip connection', r'data augmentation',\n",
        "        r'pre-training', r'fine-tuning', r'knowledge distillation',\n",
        "        r'self-attention', r'graph convolution', r'contrastive learning',\n",
        "        r'adversarial training', r'curriculum learning', r'multi-head attention'\n",
        "    ]\n",
        "    \n",
        "    found_methods = set()\n",
        "    \n",
        "    for _, row in papers_df.iterrows():\n",
        "        abstract = row.get('abstract', '')\n",
        "        if abstract and isinstance(abstract, str):\n",
        "            abstract_lower = abstract.lower()\n",
        "            for pattern in method_patterns:\n",
        "                if re.search(pattern, abstract_lower):\n",
        "                    match = re.search(pattern, abstract_lower)\n",
        "                    if match:\n",
        "                        found_methods.add(match.group())\n",
        "    \n",
        "    return list(found_methods)\n",
        "\n",
        "\n",
        "# Execute extraction\n",
        "extracted_authors = extract_all_authors(papers_df)\n",
        "extracted_institutions = extract_all_institutions(papers_df)\n",
        "extracted_topics = extract_topics(papers_df)\n",
        "methods_from_abstracts = extract_methods_from_abstracts(papers_df)\n",
        "\n",
        "print(f\"Extracted {len(extracted_authors)} unique author names\")\n",
        "print(f\"Extracted {len(extracted_institutions)} unique institution names\")\n",
        "print(f\"Extracted {len(extracted_topics)} unique topics\")\n",
        "print(f\"Found {len(methods_from_abstracts)} methods in abstracts\")\n",
        "print(f\"\\nMethods found: {methods_from_abstracts[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entity Resolution\n",
        "\n",
        "Resolve author and institution name variations to canonical forms using fuzzy matching and reference data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_name(name: str) -> str:\n",
        "    \"\"\"Normalize a name for comparison.\"\"\"\n",
        "    name = re.sub(r'[.,]', '', name.lower())\n",
        "    name = ' '.join(name.split())\n",
        "    return name\n",
        "\n",
        "\n",
        "def get_name_parts(name: str) -> Tuple[str, str]:\n",
        "    \"\"\"Extract first name initial and last name from various formats.\"\"\"\n",
        "    name = normalize_name(name)\n",
        "    \n",
        "    # Handle \"Last, First\" format\n",
        "    if ',' in name:\n",
        "        parts = name.split(',')\n",
        "        last = parts[0].strip()\n",
        "        first = parts[1].strip() if len(parts) > 1 else ''\n",
        "    else:\n",
        "        parts = name.split()\n",
        "        if len(parts) >= 2:\n",
        "            first = parts[0]\n",
        "            last = parts[-1]\n",
        "        elif len(parts) == 1:\n",
        "            first = ''\n",
        "            last = parts[0]\n",
        "        else:\n",
        "            first = ''\n",
        "            last = ''\n",
        "    \n",
        "    first_initial = first[0] if first else ''\n",
        "    return first_initial, last\n",
        "\n",
        "\n",
        "def build_author_resolution_map(extracted_authors: List[Dict], \n",
        "                                 affiliations_data: Dict) -> Dict[str, str]:\n",
        "    \"\"\"Build a mapping from author name variations to canonical forms.\"\"\"\n",
        "    resolution_map = {}\n",
        "    \n",
        "    canonical_by_parts = {}\n",
        "    known_variations = {}\n",
        "    \n",
        "    for auth_id, auth_info in affiliations_data.get('authors', {}).items():\n",
        "        canonical = auth_info['canonical_name']\n",
        "        first_init, last = get_name_parts(canonical)\n",
        "        canonical_by_parts[(first_init, last)] = canonical\n",
        "        \n",
        "        for var in auth_info.get('known_variations', []):\n",
        "            known_variations[normalize_name(var)] = canonical\n",
        "        known_variations[normalize_name(canonical)] = canonical\n",
        "    \n",
        "    for author_entry in extracted_authors:\n",
        "        name = author_entry['name']\n",
        "        normalized = normalize_name(name)\n",
        "        \n",
        "        if normalized in known_variations:\n",
        "            resolution_map[name] = known_variations[normalized]\n",
        "            continue\n",
        "        \n",
        "        first_init, last = get_name_parts(name)\n",
        "        if (first_init, last) in canonical_by_parts:\n",
        "            resolution_map[name] = canonical_by_parts[(first_init, last)]\n",
        "            continue\n",
        "        \n",
        "        resolution_map[name] = name\n",
        "    \n",
        "    return resolution_map\n",
        "\n",
        "\n",
        "def build_institution_resolution_map(extracted_institutions: List[Dict],\n",
        "                                      affiliations_data: Dict) -> Dict[str, str]:\n",
        "    \"\"\"Build a mapping from institution name variations to canonical forms.\"\"\"\n",
        "    resolution_map = {}\n",
        "    known_variations = {}\n",
        "    \n",
        "    for inst_id, inst_info in affiliations_data.get('institutions', {}).items():\n",
        "        canonical = inst_info['canonical_name']\n",
        "        \n",
        "        for var in inst_info.get('known_variations', []):\n",
        "            known_variations[normalize_name(var)] = canonical\n",
        "        known_variations[normalize_name(canonical)] = canonical\n",
        "    \n",
        "    for inst_entry in extracted_institutions:\n",
        "        name = inst_entry['name']\n",
        "        normalized = normalize_name(name)\n",
        "        \n",
        "        if normalized in known_variations:\n",
        "            resolution_map[name] = known_variations[normalized]\n",
        "            continue\n",
        "        \n",
        "        matched = False\n",
        "        for var_norm, canonical in known_variations.items():\n",
        "            if var_norm in normalized or normalized in var_norm:\n",
        "                resolution_map[name] = canonical\n",
        "                matched = True\n",
        "                break\n",
        "        \n",
        "        if not matched:\n",
        "            resolution_map[name] = name\n",
        "    \n",
        "    return resolution_map\n",
        "\n",
        "\n",
        "# Execute resolution\n",
        "author_resolution_map = build_author_resolution_map(extracted_authors, affiliations_data)\n",
        "institution_resolution_map = build_institution_resolution_map(extracted_institutions, affiliations_data)\n",
        "\n",
        "# Count unique resolved entities\n",
        "resolved_author_count = len(set(author_resolution_map.values()))\n",
        "resolved_institution_count = len(set(institution_resolution_map.values()))\n",
        "\n",
        "print(f\"Author resolution: {len(extracted_authors)} names -> {resolved_author_count} unique authors\")\n",
        "print(f\"Institution resolution: {len(extracted_institutions)} names -> {resolved_institution_count} unique institutions\")\n",
        "print(f\"\\nSample author resolutions:\")\n",
        "for name, canonical in list(author_resolution_map.items())[:5]:\n",
        "    if name != canonical:\n",
        "        print(f\"  '{name}' -> '{canonical}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Citation Network Analysis\n",
        "\n",
        "Build the citation graph and compute network metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_citation_graph(citations_df: pd.DataFrame, \n",
        "                         valid_paper_ids: set) -> Dict[str, List[str]]:\n",
        "    \"\"\"Build adjacency list representation of citation graph.\"\"\"\n",
        "    graph = defaultdict(list)\n",
        "    \n",
        "    for _, row in citations_df.iterrows():\n",
        "        citing = row['citing_paper']\n",
        "        cited = row['cited_paper']\n",
        "        graph[citing].append(cited)\n",
        "    \n",
        "    for paper_id in valid_paper_ids:\n",
        "        if paper_id not in graph:\n",
        "            graph[paper_id] = []\n",
        "    \n",
        "    return dict(graph)\n",
        "\n",
        "\n",
        "def compute_degrees(citation_graph: Dict[str, List[str]], \n",
        "                    valid_paper_ids: set) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"Compute in-degree and out-degree for each paper.\"\"\"\n",
        "    in_degree = defaultdict(int)\n",
        "    out_degree = {}\n",
        "    \n",
        "    for paper_id in valid_paper_ids:\n",
        "        in_degree[paper_id] = 0\n",
        "    \n",
        "    for citing, cited_list in citation_graph.items():\n",
        "        out_degree[citing] = len(cited_list)\n",
        "        for cited in cited_list:\n",
        "            if cited in valid_paper_ids:\n",
        "                in_degree[cited] += 1\n",
        "    \n",
        "    for paper_id in valid_paper_ids:\n",
        "        if paper_id not in out_degree:\n",
        "            out_degree[paper_id] = 0\n",
        "    \n",
        "    return dict(in_degree), out_degree\n",
        "\n",
        "\n",
        "def compute_pagerank(citation_graph: Dict[str, List[str]], \n",
        "                     valid_paper_ids: set,\n",
        "                     damping: float = 0.85) -> Dict[str, float]:\n",
        "    \"\"\"Compute PageRank scores using networkx.\"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    G.add_nodes_from(valid_paper_ids)\n",
        "    \n",
        "    for citing, cited_list in citation_graph.items():\n",
        "        if citing in valid_paper_ids:\n",
        "            for cited in cited_list:\n",
        "                if cited in valid_paper_ids:\n",
        "                    G.add_edge(citing, cited)\n",
        "    \n",
        "    pagerank = nx.pagerank(G, alpha=damping)\n",
        "    return pagerank\n",
        "\n",
        "\n",
        "def find_orphan_citations(citations_df: pd.DataFrame,\n",
        "                          valid_paper_ids: set) -> List[Dict]:\n",
        "    \"\"\"Find citations that reference non-existent papers.\"\"\"\n",
        "    orphans = []\n",
        "    \n",
        "    for _, row in citations_df.iterrows():\n",
        "        cited = row['cited_paper']\n",
        "        if cited not in valid_paper_ids:\n",
        "            orphans.append({\n",
        "                \"citing_paper\": row['citing_paper'],\n",
        "                \"cited_paper\": cited\n",
        "            })\n",
        "    \n",
        "    return orphans\n",
        "\n",
        "\n",
        "def find_self_citations(citations_df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"Find papers that cite themselves.\"\"\"\n",
        "    self_citing = []\n",
        "    \n",
        "    for _, row in citations_df.iterrows():\n",
        "        if row['citing_paper'] == row['cited_paper']:\n",
        "            self_citing.append(row['citing_paper'])\n",
        "    \n",
        "    return list(set(self_citing))\n",
        "\n",
        "\n",
        "# Execute citation analysis\n",
        "valid_paper_ids = set(papers_df['paper_id'].tolist())\n",
        "\n",
        "citation_graph = build_citation_graph(citations_df, valid_paper_ids)\n",
        "in_degree, out_degree = compute_degrees(citation_graph, valid_paper_ids)\n",
        "pagerank_scores = compute_pagerank(citation_graph, valid_paper_ids)\n",
        "\n",
        "# Get top cited papers\n",
        "top_cited_papers = sorted(in_degree.keys(), key=lambda x: in_degree[x], reverse=True)[:10]\n",
        "\n",
        "# Find anomalies\n",
        "orphan_citations = find_orphan_citations(citations_df, valid_paper_ids)\n",
        "self_citations = find_self_citations(citations_df)\n",
        "\n",
        "print(f\"Citation graph: {len(citation_graph)} nodes\")\n",
        "print(f\"PageRank scores computed: {len(pagerank_scores)} papers\")\n",
        "print(f\"PageRank sum: {sum(pagerank_scores.values()):.4f}\")\n",
        "print(f\"\\nTop 5 cited papers:\")\n",
        "for paper_id in top_cited_papers[:5]:\n",
        "    print(f\"  {paper_id}: {in_degree[paper_id]} citations\")\n",
        "print(f\"\\nOrphan citations found: {len(orphan_citations)}\")\n",
        "print(f\"Self-citations found: {len(self_citations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Checks\n",
        "\n",
        "Perform all required validation checks and store results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_all_validations() -> Dict[str, bool]:\n",
        "    \"\"\"Run all validation checks and return results.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Data loading checks\n",
        "    expected_paper_cols = {'paper_id', 'title', 'authors', 'institution', \n",
        "                           'abstract', 'keywords', 'venue', 'year', 'publication_date'}\n",
        "    results[\"papers_loaded_ok\"] = (\n",
        "        len(papers_df) > 0 and \n",
        "        expected_paper_cols.issubset(set(papers_df.columns))\n",
        "    )\n",
        "    \n",
        "    expected_citation_cols = {'citing_paper', 'cited_paper'}\n",
        "    results[\"citations_loaded_ok\"] = (\n",
        "        len(citations_df) > 0 and\n",
        "        expected_citation_cols.issubset(set(citations_df.columns))\n",
        "    )\n",
        "    \n",
        "    results[\"affiliations_loaded_ok\"] = (\n",
        "        isinstance(affiliations_data, dict) and\n",
        "        'authors' in affiliations_data and\n",
        "        'institutions' in affiliations_data\n",
        "    )\n",
        "    \n",
        "    results[\"no_duplicate_paper_ids\"] = (\n",
        "        papers_df['paper_id'].nunique() == len(papers_df)\n",
        "    )\n",
        "    \n",
        "    results[\"authors_extracted\"] = len(extracted_authors) > 0\n",
        "    results[\"institutions_extracted\"] = len(extracted_institutions) > 0\n",
        "    \n",
        "    results[\"resolution_maps_valid\"] = (\n",
        "        len(author_resolution_map) > 0 and\n",
        "        len(institution_resolution_map) > 0 and\n",
        "        isinstance(author_resolution_map, dict) and\n",
        "        isinstance(institution_resolution_map, dict)\n",
        "    )\n",
        "    \n",
        "    results[\"citation_graph_built\"] = (\n",
        "        len(citation_graph) > 0 and\n",
        "        isinstance(citation_graph, dict)\n",
        "    )\n",
        "    \n",
        "    results[\"pagerank_computed\"] = (\n",
        "        len(pagerank_scores) > 0 and\n",
        "        isinstance(pagerank_scores, dict) and\n",
        "        all(isinstance(v, float) for v in pagerank_scores.values())\n",
        "    )\n",
        "    \n",
        "    results[\"orphans_identified\"] = isinstance(orphan_citations, list)\n",
        "    results[\"self_citations_identified\"] = isinstance(self_citations, list)\n",
        "    \n",
        "    results[\"all_pagerank_finite\"] = all(\n",
        "        np.isfinite(v) for v in pagerank_scores.values()\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "validation_results = run_all_validations()\n",
        "\n",
        "print(\"=== Validation Results ===\")\n",
        "for check, passed in validation_results.items():\n",
        "    status = \"PASS\" if passed else \"FAIL\"\n",
        "    print(f\"  [{status}] {check}: {passed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n",
        "\n",
        "Compute all required summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_summary_statistics() -> Dict[str, Any]:\n",
        "    \"\"\"Compute all required summary statistics.\"\"\"\n",
        "    stats = {}\n",
        "    \n",
        "    stats[\"total_papers\"] = len(papers_df)\n",
        "    stats[\"total_citations\"] = len(citations_df)\n",
        "    \n",
        "    stats[\"unique_authors_raw\"] = len(extracted_authors)\n",
        "    stats[\"unique_authors_resolved\"] = resolved_author_count\n",
        "    \n",
        "    stats[\"unique_institutions_raw\"] = len(extracted_institutions)\n",
        "    stats[\"unique_institutions_resolved\"] = resolved_institution_count\n",
        "    \n",
        "    stats[\"papers_with_missing_abstract\"] = papers_df['abstract'].apply(\n",
        "        lambda x: not x or (isinstance(x, str) and len(x.strip()) == 0)\n",
        "    ).sum()\n",
        "    \n",
        "    stats[\"papers_with_missing_keywords\"] = papers_df['keywords'].apply(\n",
        "        lambda x: not x or (isinstance(x, list) and len(x) == 0)\n",
        "    ).sum()\n",
        "    \n",
        "    stats[\"orphan_citation_count\"] = len(orphan_citations)\n",
        "    stats[\"self_citation_count\"] = len(self_citations)\n",
        "    \n",
        "    stats[\"avg_citations_per_paper\"] = round(\n",
        "        sum(out_degree.values()) / len(out_degree) if out_degree else 0, 2\n",
        "    )\n",
        "    \n",
        "    venue_counts = papers_df['venue'].value_counts()\n",
        "    stats[\"most_common_venue\"] = venue_counts.index[0] if len(venue_counts) > 0 else \"\"\n",
        "    \n",
        "    stats[\"year_range\"] = (int(papers_df['year'].min()), int(papers_df['year'].max()))\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "summary_stats = compute_summary_statistics()\n",
        "\n",
        "print(\"=== Summary Statistics ===\")\n",
        "for key, value in summary_stats.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Report Generation\n",
        "\n",
        "Compile all results into the required final report structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_final_report() -> Dict:\n",
        "    \"\"\"Generate the complete final report.\"\"\"\n",
        "    \n",
        "    # Compute author paper counts (using resolved names)\n",
        "    author_paper_counts = defaultdict(set)\n",
        "    for author_entry in extracted_authors:\n",
        "        canonical = author_resolution_map.get(author_entry['name'], author_entry['name'])\n",
        "        for paper_id in author_entry['paper_ids']:\n",
        "            author_paper_counts[canonical].add(paper_id)\n",
        "    \n",
        "    top_authors = sorted(\n",
        "        author_paper_counts.items(), \n",
        "        key=lambda x: len(x[1]), \n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Compute institution paper counts\n",
        "    inst_paper_counts = defaultdict(set)\n",
        "    for inst_entry in extracted_institutions:\n",
        "        canonical = institution_resolution_map.get(inst_entry['name'], inst_entry['name'])\n",
        "        for paper_id in inst_entry['paper_ids']:\n",
        "            inst_paper_counts[canonical].add(paper_id)\n",
        "    \n",
        "    top_institutions = sorted(\n",
        "        inst_paper_counts.items(),\n",
        "        key=lambda x: len(x[1]),\n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Top topics\n",
        "    top_topics = sorted(\n",
        "        extracted_topics.items(),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )[:10]\n",
        "    \n",
        "    # Top cited papers with titles\n",
        "    paper_titles = dict(zip(papers_df['paper_id'], papers_df['title']))\n",
        "    top_cited_with_info = [\n",
        "        {\n",
        "            \"paper_id\": pid,\n",
        "            \"citation_count\": in_degree[pid],\n",
        "            \"title\": paper_titles.get(pid, \"Unknown\")\n",
        "        }\n",
        "        for pid in top_cited_papers\n",
        "    ]\n",
        "    \n",
        "    # Network statistics\n",
        "    in_degrees = list(in_degree.values())\n",
        "    out_degrees = list(out_degree.values())\n",
        "    \n",
        "    # Check for duplicate authors in papers\n",
        "    duplicate_author_count = 0\n",
        "    for _, row in papers_df.iterrows():\n",
        "        authors = row.get('authors', [])\n",
        "        if isinstance(authors, list):\n",
        "            resolved = [author_resolution_map.get(a, a) for a in authors if a]\n",
        "            if len(resolved) != len(set(resolved)):\n",
        "                duplicate_author_count += 1\n",
        "    \n",
        "    # Count missing institutions\n",
        "    missing_inst_count = papers_df['institution'].apply(\n",
        "        lambda x: not x or (isinstance(x, str) and len(x.strip()) == 0) or pd.isna(x)\n",
        "    ).sum()\n",
        "    \n",
        "    failed_checks = [k for k, v in validation_results.items() if not v]\n",
        "    \n",
        "    report = {\n",
        "        \"metadata\": {\n",
        "            \"task\": \"Research Paper Entity Extraction and Citation Analysis\",\n",
        "            \"papers_analyzed\": len(papers_df),\n",
        "            \"execution_timestamp\": datetime.now().isoformat()\n",
        "        },\n",
        "        \"entity_extraction\": {\n",
        "            \"authors\": {\n",
        "                \"total_unique\": resolved_author_count,\n",
        "                \"top_5_by_paper_count\": [\n",
        "                    {\"name\": name, \"paper_count\": len(papers)}\n",
        "                    for name, papers in top_authors\n",
        "                ]\n",
        "            },\n",
        "            \"institutions\": {\n",
        "                \"total_unique\": resolved_institution_count,\n",
        "                \"top_5_by_paper_count\": [\n",
        "                    {\"name\": name, \"paper_count\": len(papers)}\n",
        "                    for name, papers in top_institutions\n",
        "                ]\n",
        "            },\n",
        "            \"topics\": {\n",
        "                \"total_unique\": len(extracted_topics),\n",
        "                \"top_10_by_frequency\": [\n",
        "                    {\"topic\": topic, \"count\": count}\n",
        "                    for topic, count in top_topics\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        \"citation_analysis\": {\n",
        "            \"total_citations\": len(citations_df),\n",
        "            \"top_10_cited_papers\": top_cited_with_info,\n",
        "            \"orphan_citations\": orphan_citations,\n",
        "            \"self_citations\": self_citations,\n",
        "            \"network_statistics\": {\n",
        "                \"avg_in_degree\": round(sum(in_degrees) / len(in_degrees), 2) if in_degrees else 0,\n",
        "                \"avg_out_degree\": round(sum(out_degrees) / len(out_degrees), 2) if out_degrees else 0,\n",
        "                \"max_in_degree\": max(in_degrees) if in_degrees else 0,\n",
        "                \"max_out_degree\": max(out_degrees) if out_degrees else 0\n",
        "            }\n",
        "        },\n",
        "        \"data_quality\": {\n",
        "            \"missing_abstracts\": int(summary_stats[\"papers_with_missing_abstract\"]),\n",
        "            \"missing_keywords\": int(summary_stats[\"papers_with_missing_keywords\"]),\n",
        "            \"missing_institutions\": int(missing_inst_count),\n",
        "            \"duplicate_author_entries\": duplicate_author_count\n",
        "        },\n",
        "        \"validation_summary\": {\n",
        "            \"all_checks_passed\": len(failed_checks) == 0,\n",
        "            \"failed_checks\": failed_checks\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "final_report = generate_final_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== VALIDATION RESULTS ===\")\n",
        "print(json.dumps(validation_results, indent=2))\n",
        "print(\"\\n=== FINAL REPORT ===\")\n",
        "print(json.dumps(final_report, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Unit Tests\n",
        "\n",
        "The following section contains comprehensive unit tests to validate the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestDataLoading(unittest.TestCase):\n",
        "    \"\"\"Tests for data loading functionality.\"\"\"\n",
        "    \n",
        "    def test_papers_df_exists_and_not_empty(self):\n",
        "        self.assertIsInstance(papers_df, pd.DataFrame)\n",
        "        self.assertGreater(len(papers_df), 0)\n",
        "    \n",
        "    def test_papers_df_has_required_columns(self):\n",
        "        required = {'paper_id', 'title', 'authors', 'institution', \n",
        "                   'abstract', 'keywords', 'venue', 'year', 'publication_date'}\n",
        "        self.assertTrue(required.issubset(set(papers_df.columns)))\n",
        "    \n",
        "    def test_citations_df_exists_and_not_empty(self):\n",
        "        self.assertIsInstance(citations_df, pd.DataFrame)\n",
        "        self.assertGreater(len(citations_df), 0)\n",
        "    \n",
        "    def test_citations_df_has_required_columns(self):\n",
        "        required = {'citing_paper', 'cited_paper'}\n",
        "        self.assertTrue(required.issubset(set(citations_df.columns)))\n",
        "    \n",
        "    def test_affiliations_data_structure(self):\n",
        "        self.assertIsInstance(affiliations_data, dict)\n",
        "        self.assertIn('authors', affiliations_data)\n",
        "        self.assertIn('institutions', affiliations_data)\n",
        "    \n",
        "    def test_no_duplicate_paper_ids(self):\n",
        "        self.assertEqual(papers_df['paper_id'].nunique(), len(papers_df))\n",
        "\n",
        "\n",
        "class TestEntityExtraction(unittest.TestCase):\n",
        "    \"\"\"Tests for entity extraction functionality.\"\"\"\n",
        "    \n",
        "    def test_extracted_authors_not_empty(self):\n",
        "        self.assertGreater(len(extracted_authors), 0)\n",
        "    \n",
        "    def test_extracted_authors_structure(self):\n",
        "        for author in extracted_authors:\n",
        "            self.assertIn('name', author)\n",
        "            self.assertIn('paper_ids', author)\n",
        "            self.assertIn('name_variations', author)\n",
        "            self.assertIsInstance(author['name'], str)\n",
        "            self.assertIsInstance(author['paper_ids'], list)\n",
        "    \n",
        "    def test_extracted_institutions_not_empty(self):\n",
        "        self.assertGreater(len(extracted_institutions), 0)\n",
        "    \n",
        "    def test_extracted_institutions_structure(self):\n",
        "        for inst in extracted_institutions:\n",
        "            self.assertIn('name', inst)\n",
        "            self.assertIn('paper_ids', inst)\n",
        "            self.assertIn('name_variations', inst)\n",
        "    \n",
        "    def test_extracted_topics_is_dict(self):\n",
        "        self.assertIsInstance(extracted_topics, dict)\n",
        "        for key, value in extracted_topics.items():\n",
        "            self.assertIsInstance(key, str)\n",
        "            self.assertIsInstance(value, int)\n",
        "    \n",
        "    def test_methods_from_abstracts_is_list(self):\n",
        "        self.assertIsInstance(methods_from_abstracts, list)\n",
        "        for method in methods_from_abstracts:\n",
        "            self.assertIsInstance(method, str)\n",
        "\n",
        "\n",
        "class TestEntityResolution(unittest.TestCase):\n",
        "    \"\"\"Tests for entity resolution functionality.\"\"\"\n",
        "    \n",
        "    def test_author_resolution_map_not_empty(self):\n",
        "        self.assertGreater(len(author_resolution_map), 0)\n",
        "    \n",
        "    def test_author_resolution_map_structure(self):\n",
        "        for key, value in author_resolution_map.items():\n",
        "            self.assertIsInstance(key, str)\n",
        "            self.assertIsInstance(value, str)\n",
        "    \n",
        "    def test_institution_resolution_map_not_empty(self):\n",
        "        self.assertGreater(len(institution_resolution_map), 0)\n",
        "    \n",
        "    def test_resolution_reduces_author_count(self):\n",
        "        raw_count = len(extracted_authors)\n",
        "        self.assertLessEqual(resolved_author_count, raw_count)\n",
        "    \n",
        "    def test_resolved_counts_are_positive(self):\n",
        "        self.assertGreater(resolved_author_count, 0)\n",
        "        self.assertGreater(resolved_institution_count, 0)\n",
        "\n",
        "\n",
        "class TestCitationNetwork(unittest.TestCase):\n",
        "    \"\"\"Tests for citation network functionality.\"\"\"\n",
        "    \n",
        "    def test_citation_graph_not_empty(self):\n",
        "        self.assertGreater(len(citation_graph), 0)\n",
        "    \n",
        "    def test_citation_graph_structure(self):\n",
        "        for key, value in citation_graph.items():\n",
        "            self.assertIsInstance(key, str)\n",
        "            self.assertIsInstance(value, list)\n",
        "    \n",
        "    def test_in_degree_covers_all_papers(self):\n",
        "        self.assertEqual(len(in_degree), len(papers_df))\n",
        "    \n",
        "    def test_out_degree_covers_all_papers(self):\n",
        "        self.assertEqual(len(out_degree), len(papers_df))\n",
        "    \n",
        "    def test_pagerank_scores_not_empty(self):\n",
        "        self.assertGreater(len(pagerank_scores), 0)\n",
        "    \n",
        "    def test_pagerank_scores_are_floats(self):\n",
        "        for paper_id, score in pagerank_scores.items():\n",
        "            self.assertIsInstance(score, float)\n",
        "    \n",
        "    def test_pagerank_scores_sum_to_one(self):\n",
        "        total = sum(pagerank_scores.values())\n",
        "        self.assertAlmostEqual(total, 1.0, delta=0.01)\n",
        "    \n",
        "    def test_pagerank_scores_are_finite(self):\n",
        "        for score in pagerank_scores.values():\n",
        "            self.assertTrue(np.isfinite(score))\n",
        "    \n",
        "    def test_top_cited_papers_length(self):\n",
        "        self.assertLessEqual(len(top_cited_papers), 10)\n",
        "        self.assertGreater(len(top_cited_papers), 0)\n",
        "    \n",
        "    def test_orphan_citations_identified(self):\n",
        "        self.assertIsInstance(orphan_citations, list)\n",
        "        self.assertGreater(len(orphan_citations), 0)\n",
        "    \n",
        "    def test_orphan_citations_structure(self):\n",
        "        for orphan in orphan_citations:\n",
        "            self.assertIn('citing_paper', orphan)\n",
        "            self.assertIn('cited_paper', orphan)\n",
        "    \n",
        "    def test_self_citations_identified(self):\n",
        "        self.assertIsInstance(self_citations, list)\n",
        "        self.assertGreater(len(self_citations), 0)\n",
        "\n",
        "\n",
        "class TestValidationResults(unittest.TestCase):\n",
        "    \"\"\"Tests for validation results.\"\"\"\n",
        "    \n",
        "    def test_validation_results_is_dict(self):\n",
        "        self.assertIsInstance(validation_results, dict)\n",
        "    \n",
        "    def test_validation_results_has_required_keys(self):\n",
        "        required_keys = {\n",
        "            \"papers_loaded_ok\", \"citations_loaded_ok\", \"affiliations_loaded_ok\",\n",
        "            \"no_duplicate_paper_ids\", \"authors_extracted\", \"institutions_extracted\",\n",
        "            \"resolution_maps_valid\", \"citation_graph_built\", \"pagerank_computed\",\n",
        "            \"orphans_identified\", \"self_citations_identified\", \"all_pagerank_finite\"\n",
        "        }\n",
        "        self.assertTrue(required_keys.issubset(set(validation_results.keys())))\n",
        "    \n",
        "    def test_all_validations_pass(self):\n",
        "        failed = [k for k, v in validation_results.items() if not v]\n",
        "        self.assertEqual(len(failed), 0, f\"Failed validations: {failed}\")\n",
        "\n",
        "\n",
        "class TestSummaryStats(unittest.TestCase):\n",
        "    \"\"\"Tests for summary statistics.\"\"\"\n",
        "    \n",
        "    def test_summary_stats_is_dict(self):\n",
        "        self.assertIsInstance(summary_stats, dict)\n",
        "    \n",
        "    def test_summary_stats_has_required_keys(self):\n",
        "        required_keys = {\n",
        "            \"total_papers\", \"total_citations\", \"unique_authors_raw\",\n",
        "            \"unique_authors_resolved\", \"unique_institutions_raw\",\n",
        "            \"unique_institutions_resolved\", \"papers_with_missing_abstract\",\n",
        "            \"papers_with_missing_keywords\", \"orphan_citation_count\",\n",
        "            \"self_citation_count\", \"avg_citations_per_paper\",\n",
        "            \"most_common_venue\", \"year_range\"\n",
        "        }\n",
        "        self.assertTrue(required_keys.issubset(set(summary_stats.keys())))\n",
        "    \n",
        "    def test_total_papers_matches_dataframe(self):\n",
        "        self.assertEqual(summary_stats[\"total_papers\"], len(papers_df))\n",
        "    \n",
        "    def test_year_range_is_valid(self):\n",
        "        year_range = summary_stats[\"year_range\"]\n",
        "        self.assertIsInstance(year_range, tuple)\n",
        "        self.assertEqual(len(year_range), 2)\n",
        "        self.assertLessEqual(year_range[0], year_range[1])\n",
        "\n",
        "\n",
        "class TestFinalReport(unittest.TestCase):\n",
        "    \"\"\"Tests for final report structure.\"\"\"\n",
        "    \n",
        "    def test_final_report_is_dict(self):\n",
        "        self.assertIsInstance(final_report, dict)\n",
        "    \n",
        "    def test_final_report_has_metadata(self):\n",
        "        self.assertIn('metadata', final_report)\n",
        "        self.assertIn('task', final_report['metadata'])\n",
        "        self.assertIn('papers_analyzed', final_report['metadata'])\n",
        "        self.assertIn('execution_timestamp', final_report['metadata'])\n",
        "    \n",
        "    def test_final_report_has_entity_extraction(self):\n",
        "        self.assertIn('entity_extraction', final_report)\n",
        "        ee = final_report['entity_extraction']\n",
        "        self.assertIn('authors', ee)\n",
        "        self.assertIn('institutions', ee)\n",
        "        self.assertIn('topics', ee)\n",
        "    \n",
        "    def test_final_report_has_citation_analysis(self):\n",
        "        self.assertIn('citation_analysis', final_report)\n",
        "        ca = final_report['citation_analysis']\n",
        "        self.assertIn('total_citations', ca)\n",
        "        self.assertIn('top_10_cited_papers', ca)\n",
        "        self.assertIn('orphan_citations', ca)\n",
        "        self.assertIn('self_citations', ca)\n",
        "        self.assertIn('network_statistics', ca)\n",
        "    \n",
        "    def test_final_report_has_data_quality(self):\n",
        "        self.assertIn('data_quality', final_report)\n",
        "        dq = final_report['data_quality']\n",
        "        self.assertIn('missing_abstracts', dq)\n",
        "        self.assertIn('missing_keywords', dq)\n",
        "        self.assertIn('missing_institutions', dq)\n",
        "        self.assertIn('duplicate_author_entries', dq)\n",
        "    \n",
        "    def test_final_report_has_validation_summary(self):\n",
        "        self.assertIn('validation_summary', final_report)\n",
        "        vs = final_report['validation_summary']\n",
        "        self.assertIn('all_checks_passed', vs)\n",
        "        self.assertIn('failed_checks', vs)\n",
        "    \n",
        "    def test_top_authors_structure(self):\n",
        "        top_authors = final_report['entity_extraction']['authors']['top_5_by_paper_count']\n",
        "        self.assertLessEqual(len(top_authors), 5)\n",
        "        for author in top_authors:\n",
        "            self.assertIn('name', author)\n",
        "            self.assertIn('paper_count', author)\n",
        "    \n",
        "    def test_top_cited_papers_structure(self):\n",
        "        top_papers = final_report['citation_analysis']['top_10_cited_papers']\n",
        "        self.assertLessEqual(len(top_papers), 10)\n",
        "        for paper in top_papers:\n",
        "            self.assertIn('paper_id', paper)\n",
        "            self.assertIn('citation_count', paper)\n",
        "            self.assertIn('title', paper)\n",
        "    \n",
        "    def test_network_statistics_structure(self):\n",
        "        ns = final_report['citation_analysis']['network_statistics']\n",
        "        self.assertIn('avg_in_degree', ns)\n",
        "        self.assertIn('avg_out_degree', ns)\n",
        "        self.assertIn('max_in_degree', ns)\n",
        "        self.assertIn('max_out_degree', ns)\n",
        "    \n",
        "    def test_all_checks_passed(self):\n",
        "        self.assertTrue(final_report['validation_summary']['all_checks_passed'])\n",
        "        self.assertEqual(len(final_report['validation_summary']['failed_checks']), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all unit tests\n",
        "def run_tests():\n",
        "    \"\"\"Run all unit tests and report results.\"\"\"\n",
        "    loader = unittest.TestLoader()\n",
        "    suite = unittest.TestSuite()\n",
        "    \n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestDataLoading))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestEntityExtraction))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestEntityResolution))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestCitationNetwork))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestValidationResults))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestSummaryStats))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestFinalReport))\n",
        "    \n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    result = runner.run(suite)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Tests run: {result.testsRun}\")\n",
        "    print(f\"Failures: {len(result.failures)}\")\n",
        "    print(f\"Errors: {len(result.errors)}\")\n",
        "    print(f\"Success: {result.wasSuccessful()}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "test_result = run_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Verification\n",
        "\n",
        "Final check that all success criteria are met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_success_criteria():\n",
        "    \"\"\"Verify all success criteria from the benchmark prompt.\"\"\"\n",
        "    criteria = {}\n",
        "    \n",
        "    # 1. All validation checks pass\n",
        "    criteria[\"all_validation_checks_pass\"] = all(validation_results.values())\n",
        "    \n",
        "    # 2. Entity resolution reduces author count\n",
        "    criteria[\"resolution_reduces_authors\"] = resolved_author_count < len(extracted_authors)\n",
        "    \n",
        "    # 3. Orphan citations identified\n",
        "    criteria[\"orphan_citations_found\"] = len(orphan_citations) >= 1\n",
        "    \n",
        "    # 4. Self-citations identified\n",
        "    criteria[\"self_citations_found\"] = len(self_citations) >= 1\n",
        "    \n",
        "    # 5. PageRank sums to ~1.0\n",
        "    pr_sum = sum(pagerank_scores.values())\n",
        "    criteria[\"pagerank_sums_to_one\"] = abs(pr_sum - 1.0) < 0.01\n",
        "    \n",
        "    # 6. Final report has correct schema\n",
        "    required_keys = {'metadata', 'entity_extraction', 'citation_analysis', \n",
        "                     'data_quality', 'validation_summary'}\n",
        "    criteria[\"final_report_schema_valid\"] = required_keys.issubset(set(final_report.keys()))\n",
        "    \n",
        "    # 7. All numeric values are finite\n",
        "    criteria[\"all_numerics_finite\"] = all(np.isfinite(v) for v in pagerank_scores.values())\n",
        "    \n",
        "    print(\"=== SUCCESS CRITERIA VERIFICATION ===\")\n",
        "    all_passed = True\n",
        "    for criterion, passed in criteria.items():\n",
        "        status = \"PASS\" if passed else \"FAIL\"\n",
        "        print(f\"  [{status}] {criterion}: {passed}\")\n",
        "        if not passed:\n",
        "            all_passed = False\n",
        "    \n",
        "    print(f\"\\n{'='*40}\")\n",
        "    if all_passed:\n",
        "        print(\"ALL SUCCESS CRITERIA MET!\")\n",
        "    else:\n",
        "        print(\"SOME CRITERIA FAILED\")\n",
        "    \n",
        "    return all_passed\n",
        "\n",
        "success = verify_success_criteria()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
